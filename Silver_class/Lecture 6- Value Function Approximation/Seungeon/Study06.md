# RL-study-2019

Value function approximation
Silverì˜ ê°•ì˜ëŠ” 1~5 ê°•ê³¼ 6~10ê°•ìœ¼ë¡œ ë‚˜ë‰œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.

### 1. Large-Scale Reinforcement Learning

  Reinforcement learning can be used to solve large problems, e.g.
    Backgammon  : 10^20 states
    Computer Go : 10^170 states
    Helicopter  : continous state space

#### Value Function Approximation

  So far we have represented value function by a lookup table
    Every state s has an entry V(s)
    Or overy state-action pair s,a has an engry Q(s,a)

  Problem with large MDPs:
    There are too many states and/or actions to store in memory
    It is too slow to learn the value of each state individually

  Solution for large MDPs:
    Estimate value function with function approximation
         v_hat(s,w) ~~ v_pi(s)
      or q_hat(s,a,w) ~~ q_pi(s,a)

    Generalise from seen states to unseen states
    Update parameter w using MC or TD learning

  ë²ˆì—­
    ê·¼ì‚¬ í•¨ìˆ˜ë¥¼ í†µí•´ì„œ, ì‹¤ì œ valueì™€ ì‹¤ì œ që¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ
    ê·¸ëŸ¬ê¸° ìœ„í•´ì„œ ê·¼ì‚¬ í•¨ìˆ˜ì˜ parameterì¸ wë¥¼ í•™ìŠµ ì‹œì¼œì•¼í•˜ê³  ê·¸ë•Œ MC or TD learningì„ ì´ìš©í•˜ëŠ” ê²ƒ.
    ì´ë ‡ê²Œ í•™ìŠµì„ ì‹œí‚¤ë©´, ë³´ì§€ ëª» í•œ stateë„ ë³¸ stateë“¤ë¡œ ì¼ë°˜í™” í•  ìˆ˜ ìˆë‹¤.

  Which Function Approximation?
    There are many function approximations, e.g.
      Linear combinations of features
      Neural network
      Decision tree
      Nearest neighbour
      Fourier / wavelet bases
      ...

  ì´ì¤‘ì— ë¯¸ë¶„ì„ í†µí•´ í•™ìŠµì´ ê°€ëŠ¥í•œ, Lineaer combinations of features, Neural network ì´ ë‘ê°€ì§€ë¥¼ ë³´í†µ ì‚¬ìš©í•˜ê³¤ í•œë‹¤.

  ë˜í•œ, non-stationaryí•˜ê³  non-iidí•œ ë°ì´í„°ì— ì ì ˆí•œ training methodë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

#### Incremental Methods
  Incremental (ì¡°ê¸ˆì”© ë°”ê¿”ë‚˜ê°€ëŠ”) ë°©ë²•ë“¤ì— ëŒ€í•´ ë°°ìš¸ ê²ƒì´ë‹¤.

  Gradient descent
    Let J(w) be a differentiable function of parameter vector w
    Define the gradient of J(w) to be
      Delta_w J(w) = (w1ì—ëŒ€í•œ Jì˜ í¸ë¯¸ë¶„, w2ì— ëŒ€í•œ Jì˜ í¸ë¯¸ë¶„, ...)

    To find a local minimum of J(w)
    Adjust w in direction of -ve gradient
      Delta w = -(1/2) * alpha * Delta_w J(w)
      where alpha is a step-size parameter

#### Value Function Approximation By Stochastic Gradient Descent

  Goal : find parameter vector w minimising mean-squared error between approximate value function v_hat(s,w) and true value function v_pi(s)
    J(w) = E_pi[(v_pi(s) - v_hat(s,w))^2]

  Gradient descent finds a local minimum
    Delta w = -(1/2) * alpha * Delta_w J(w)
            = alpha * E_pi[(v_pi(s) - v_hat(s,w)) * Delta_w v_hat(s,w)]

  Stochastic gradient descent samples the gradient
    Delta w = alpha * (v_pi(s) - v_hat(s,w)) * Delta_w v_hat(s,w)
    ê¸°ëŒ“ê°’ì„ ì§ì ‘ êµ¬í•˜ì§€ ì•Šê³ , Samplingì„ í†µí•´ estimation í•˜ëŠ” ê²ƒ

  Expected update is equal to full gradient update

#### Feature Vectors

  Represente state by a feature vector
    x(S) = (x1(S), x2(S), ... , xn(S))

  For example:
    Distance of robot from landmarks
    Trends in the stock market
    Piece and pawn configurations in chess

  ì–´ë–¤ ê°’ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” íŠ¹ì„±ì´ë¼ê³  ìƒê°í•˜ë©´ ë¨

#### Linear Value Function Approximation

  Represent value function by a linear combination of features
    v_hat(s,w) = x(S)^Tw = Sigma{j=1->n} xj(S)*wj

    Valueë¼ëŠ” ê²ƒì´ weightë“¤ê³¼ featureë“¤ì˜ ë‚´ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤! ë¼ëŠ” ê²ƒì´
    Linear value function approximationì˜ ì›ë¦¬ ê·¸ëƒ¥ Linear regressionì´ë‹¤!!

  Objective function is quadratic in parameters w
    J(w) = E_pi[(v_pi(S) - x(S)^Tw)^2]

    ëª©ì  í•¨ìˆ˜ëŠ” True valueì™€ Estimation value ì‚¬ì´ì˜ MSE!

  Stochastic gradient descent converges on global optimum
  Update rule is particularly simple
    Delta_w v_hat(S,w) = x(S)
    => ì™œëƒí•˜ë©´, ë§¨ ì²˜ìŒ ì‹ì—ì„œ wì— ëŒ€í•´ ë¯¸ë¶„í•˜ë©´ ë‚˜ì˜´
    Delta w = alpha * (v_pi(S) - v_hat(S,w)) * x(S)
    => ì™œëƒí•˜ë©´, stochastic descentì—ì„œ ì´ë ‡ê²Œ ì‚¬ìš©í•  ê±°ì„ x(s)ê°€ ê³±í•´ì§€ëŠ”ê±´

    Update = step-size * prediction error * feature value

#### Table Lookup Features

  Table lookup is a special case of linear value function approximation
  Using table lookup features
    x^table(S) = (I(S=s1), ... , I(S=sn))

  Parameter vector w gives value of each individual state
    v_hat(S,w) = (I(S=s1), ..., I(S=sn)) * (w1, ... , wn)

#### Incremental Prediction Algorithm

  Have assumed true value function v_pi(s) given by superviser
  But in RL there is no supervisor, only rewards
  In practice, we substitute a target for v_pi(s)
    For MC, the target is the return G_t
      âˆ†w = ğ›¼ * (G_t - v_hat(St,w)) * âˆ‡w v_hat(St,w)

    For TD(0), the target is the TD target R(t+1) + ğ›¾ * v_hat(S(t+1), w)
      âˆ†w = ğ›¼ * (R(t+1) + ğ›¾ * v_hat(S(t+1), w) - v_hat(St,w)) * âˆ‡w v_hat(St,w)

    For TD(ğœ†), the garget is the ğœ†-return G^ğœ†_t
      âˆ†w = ğ›¼ * (G^ğœ†_t - v_hat(St,w)) * âˆ‡w v_hat(St,w)

  ì§€ê¸ˆê¹Œì§€ëŠ” true value function v_pi(s)ê°€ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì •í–ˆì§€ë§Œ, RLì—ì„œëŠ” ì´ true value functionì´ ì£¼ì–´ì§€ì§€ ì•Šê³ , rewardë§Œì´ ì£¼ì–´ì§„ë‹¤. ê·¸ë˜ì„œ ì‹¤ì œì ìœ¼ë¡œ, RLì—ì„œëŠ” v_pi(s)ì˜ true valueë¥¼ ìœ„ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ê°€ì •í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

  MCì—ì„œëŠ” return G_të¥¼ ì‚¬ìš©í•˜ê³ , TD(0)ì—ì„œëŠ” TD errorë¥¼ ì‚¬ìš©í•˜ê³ , TD(ğœ†)ì—ì„œëŠ” ğœ†-returnì„ ì‚¬ìš©í•œë‹¤ê³  ì§€ê¸ˆê¹Œì§€ ë°°ì› ì§€ìš”?^^

#### Monte-Carlo with Value Function Approximation

  Return Gt is an unbiased, noisy sample of true value v_pi(St)
    => Return GtëŠ” v_pi(St)ì˜ unbiased estimatorì¸ë° noisyí•˜ë‹¤.

  Can therefore apply supervised learning to "training data":
    <S1, G1>, <S2, G2>, ... , <ST,GT>

  For example, using linear Monte-Carlo policy evaluation
    âˆ†w = ğ›¼ * (Gt - v_hat(St,w)) * âˆ‡w v_hat(St,w)
       = ğ›¼ * (Gt - v_hat(St,w)) * x(St)

  Monte-Carlo evaluation converges to a local optimum
  Even when using non-linear value function approximation

#### TD Learning with Value Function Approximation

  TD-target R(t+1)+ ğ›¾*v_hat(S(t+1),w) is a biased sample of true value vğœ‹(St)

  Can still apply supervised learning to "training data":
    <S1, R2 + ğ›¾*v_hat(S2,w)>, <S2, R3 + ğ›¾*v_hat(S(3),w)>, ... <S(T-1), RT>

  For example, using linear TD(0)
    âˆ†w = ğ›¼ * (R + ğ›¾*v_hat(S',w) - v_hat(St,w)) * âˆ‡w v_hat(St,w)
       = ğ›¼ * ğ›¿ * x(S)

    => ì˜êµ­ì¸ì˜ ì§ˆë¬¸ R + ğ›¾*v_hat(S',w)ë„ wë¡œ ë¯¸ë¶„ì´ ë˜ëŠ”ë° âˆ†wë¥¼ ì €ë ‡ê²Œ ì¨ë„ ë˜ëŠ”ê²ë‹ˆê¹Œ? ì•ˆë˜ì§€ ì•ŠìŠµë‹ˆê¹Œ?? ê³±ì˜ ë¯¸ë¶„ìœ¼ë¡œ í•´ì•¼ì§€ ì•Šë‚˜ìš”

  Linear TD(0) converges (close) to global optimum

#### TD(ğœ†) with Value Function Approximation

  The ğœ†-return G^ğœ†_t is also a biased sample of true value v_pi(s)
  Can again apply supervised learning to "training data":
    <S1, G^ğœ†_1>, <S2, G^ğœ†2>, ... , <S(T-1), G^ğœ†(T-1)>

  Forward view linear TD(ğœ†)
    âˆ†w = ğ›¼ * (G^ğœ†_t - v_hat(St,w)) * âˆ‡w v_hat(St,w)
       = ğ›¼ * (G^ğœ†_t - v_hat(St,w)) * x(St)

  Backward view linear TD(ğœ†)
    ğ›¿t = R_(t+1) + ğ›¾ * v_hat(S',w) - v_hat(St,w)
    Et = ğ›¾ * ğœ† * E(t-1) + x(S_t)
    âˆ†w = ğ›¼ * ğ›¿t * Et

  Foward view and backward view linear TD(ğœ†) are equivalent

#### Control with Value Function Approximation

  ê¸°ì¡´ì˜ Dynamic programmingê³¼ ë‹¤ë¥´ê²Œ, ëª¨ë¥´ëŠ” modelì„ í’€ê±°ë‚˜ í˜¹ì€ ì•„ëŠ” modelì´ì—¬ë„ ë„ˆë¬´ í´ ê²½ìš°ì— control ë¬¸ì œë¥¼ Value function approximationì„ í†µí•´  í’€ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ë‹¤ìŒê³¼ ê°™ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

  Policy evaluation  : Approximate policy evaluation q^hat(., ., w) â‰ˆ q_pi
  Policy improvement : e-greedy policy improvement

#### Study of lambda : Should We Bootstrap?

  Mountain Car, Random walk, Puddle world, Cart and Pole ë“±ì˜ ë¬¸ì œì—ì„œ 0~1 ì‚¬ì´ ì–´ë”˜ê°€ì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§€ëŠ” lambdaê°€ ì¡´ì¬í•œë‹¤. returnë³´ë‹¤, TD(0)ë³´ë‹¤ ì¢‹ë‹¤ëŠ” ê²ƒì€ ê²½í—˜ì ìœ¼ë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤.

#### Convergence of Prediction Algorithms

  On-Policy     Algorithm     Table Lookup      Linear      Non-Linear
                    MC              O             O             O
                  TD(0)             O             O             X
                TD(lambda)          O             O             X

  Off-Policy    Algorithm     Table Lookup      Linear      Non-Linear
                    MC              O             O             O
                  TD(0)             O             X             X
                TD(lambda)          O             X             X

  => On-Policyë¬¸ì œì—ì„œ Non-linear ë¬¸ì œì˜ ê²½ìš° TD(0)ì™€ TD(lambda) predictionì— ëŒ€í•´ ìˆ˜ë ´ì´ ë³´ì¥ ë˜ì§€ ì•ŠëŠ”ë‹¤. (Global optimumì— ê°€ê¹ê²Œ ê°€ì§€ë§Œ Global optimumìœ¼ë¡œ ê°€ì§€ ëª» í•œë‹¤.)

  => Off-Policyë¬¸ì œì—ì„œëŠ” Linearì™€ Non-linear ë¬¸ì œì˜ ê²½ìš° TD(0)ì™€ TD(lambda) predictionì— ëŒ€í•´ ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  í•œë‹¤.

#### Gradient Temporal-Difference Learning

  TD does not follow the gradient of any objective function
  This is why TD can diverge when off-policy or using non-linear function approximation
  Gradient TD follows true gradient of projected Bellman error

  => TDê°€ off-policy í˜¹ì€ non-linear function approximation ë¬¸ì œì—ì„œ ë°œì‚°í•˜ëŠ” ì´ìœ ëŠ” TDê°€ ì–´ë–¤ objective functionì˜ ê²½ì‚¬ë¥¼ ë”°ë¼ê°€ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.

  => Gradient TDê°€ Bellman errorì˜ true gradientë¥¼ ì«“ê¸° ë•Œë¬¸ì— ìˆ˜ë ´ì„±ì´ ì¢‹ë‹¤ê³  ì‹¤ë²„ êµìˆ˜ë‹˜ì´ ì£¼ì¥ì¤‘

  (Prediction)

  On-Policy     Algorithm     Table Lookup      Linear      Non-Linear
                    MC              O             O             O
                  TD(0)             O             O             X
                Gradient TD         O             O             O

  Off-Policy    Algorithm     Table Lookup      Linear      Non-Linear
                    MC              O             O             O
                  TD(0)             O             X             X
                Gradient TD         O             O             O

  (Control)
                Algorithm         Table Lookup      Linear      Non-Linear
            Monte-Carlo Control         O            (O)            X
                  Sarsa                 O            (O)            X
                Q-learning              O             X             X
            Gradient Q-learning         O             O             X

  (O)ëŠ” near-optimal value function ê·¼ì²˜ì—ì„œ ì™”ë‹¤ë¦¬ ê°”ë‹¤ë¦¬ í•¨ì„ ì˜ë¯¸
  => Gradient Q-learningì´ controlì¼ ë•Œì—ë„ ìˆ˜ë ´ì„±ì´ ì¢‹ë‹¤ê³  í•¨ã…

### Batch Methods

#### Batch Reinforcement Learning

  Gradient descent is simple and appealing
  But it is not sample efficient
  Batch methods seek to find the best fitting value function
  Given the agent's experience ("training data")

  => Sampleì´ í•œë²ˆ ì“°ê³  ë²„ë ¤ì§€ëŠ” ë¹„íš¨ìœ¨ì ì¸ ê¸°ì¡´ì˜ ë°©ë²•ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë‚˜ì˜¨ ë°©ë²•

#### Least Squares Prediction

  Given value function approximation v_hat(s,w) â‰ˆ v_pi(s)
  And experience D consisting of <state, value> pairs
    D = {<s1, v1>, <s2,v2>, ... , <sT,vT>}

  Which parameteres w give the best fitting value function v_hat(s,w)?
  Least squares algorithms find parameter vector w minimising sum-squared error between v_hat(st,w) and target values v_t,
    LS(w) = Sigma {t=1->T} (v^ğœ‹_t - v_hat(st,w))^2
          = E_D[(v^ğœ‹ - v_hat(s,w))^2]

  Experience Dì˜ v_ğœ‹ë“¤ë¡œ v_hatì„ í‘œí˜„í•˜ê³  ì‹¶ì€ ê²ƒ..?? ëŒ€ì¶© Dë¥¼ í†µí•´ì„œ v_hatì„ êµ¬í•˜ê³  ì‹¶ë‹¤ ì´ëŸ° ëŠë‚Œìœ¼ë¡œ ìƒê°í•´ë³´ìêµ¬~

#### Stochastic Gradient Descent with Experience Replay

  Given experience consisting of <state, value> pairs
    D = {<s1, v^ğœ‹1>, <s2, v^ğœ‹2>, ... , <ST, v^ğœ‹T>}

  Repeat:
    1. Sample state, value from experience
      <s,v^ğœ‹> ~ D

    2. Apply stochastic gradient descent update
      âˆ†w = alpha * (v^ğœ‹ - v_hat(s,w)) * Delta_w v_hat(s,w)

  Converges to least squares solution
    w^ğœ‹ = {w} argmin LS(w)

  => Experience replayëŠ”, Off-policy RLì—ì„œ ë§ì´ ì‚¬ìš©ë˜ê³ ëŠ” í•œë‹¤.
  => Experience replayë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´, LS(w)ë¥¼ ê°€ì¥ ì‘ê²Œë§Œë“œëŠ” w vectorê°€ w^ğœ‹ê°€ ëœë‹¤!

#### Experience Replay in Deep Q-Networks (DQN)

  DQN uses experience replay and fixed Q-targets
    take action a_t according to e-greedy policy
    Store transition (st, at, r(t+1), s(t+1)) in replay memory D
    Sample random mini-batch of transition (s,a,r,s') from D
    Compute Q-learning targets w.r.t. old, fixed parameteres w-
    Optimise MSE between Q-network and Q-learning targets
      Li(wi) = E_Di [(r + ğ›¾ * maxQ(s',a' ; wi-) - Q(s,a ; wi))^2]

    Using variant of stochastic gradient descent

  DQN in Atari
    End-to-end learning of values Q(s,a) from pixel s
    Input state s is stack of raw pixels from last 4 frames
    Output is Q(s,a) for 18 joystick/button positions
    Reward is change in score for that step

  Network architecture and hyperparameters fixed across all games

  => RLì—ì„œì˜ ì˜ë¯¸ DQNì´ Off-policyì´ê³ , TDì´ê³ , Neural networkì—¬ì„œ ìˆ˜ë ´ì´ ì˜ ì•ˆë˜ëŠ” ê°•í™”í•™ìŠµ ë°©ë²•ì¸ë°, Replay memoryì˜ ì´ìš©ê³¼ Fixed-Q target schemeì„ ì´ìš©í•´ì„œ, í•œê³„ë¥¼ ëŒíŒŒí–ˆë‹¤.

#### Linear Least Squares Prediction

  Experience replay finds least squares solution
  But it may take many iterations
  Using linear value function approximation v_hat(s,w) = x(s)^Tw
  We can solve the least squares solution directly

  At minimum of LS(w), the expected update must be zero
    E_d[Delta w] = 0
    ì¼ë ¨ì˜ ê³¼ì •ì„ ê±°ì¹˜ë©´~~
    w ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ. ì´ê²Œ ë°”ë¡œ Least squares method ì˜€ì§€?

  For N features, direct solution time is O(N^3)
  Incremental solution time is O(N^2) using Shermann - Morrison

  We do not know true values v^ğœ‹t
  In practice, our "training data" must use noisy or biased samples of v^ğœ‹t
    LSMC Least Squares Monte-Carlo uses return
      v^ğœ‹(t) â‰ˆ G(t)

    LSTD Least Squares Temporal-Difference uses TD target
      v^ğœ‹(t) â‰ˆ R(t+1) + gamma * v_hat(S(t+1), w)

    LSTD(lambda) Least Squares TD(lambda) uses lambda-return
      v^ğœ‹(t) â‰ˆ G^ğœ†(t)

  In each case solve directly for fixed point of MC / TD / TD(ğœ†)









d
