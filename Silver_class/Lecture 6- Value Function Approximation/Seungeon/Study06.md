# RL-study-2019

Value function approximation
Silverì˜ ê°•ì˜ëŠ” 1~5 ê°•ê³¼ 6~10ê°•ìœ¼ë¡œ ë‚˜ë‰œë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.

### 1. Large-Scale Reinforcement Learning

  Reinforcement learning can be used to solve large problems, e.g.
    Backgammon  : 10^20 states
    Computer Go : 10^170 states
    Helicopter  : continous state space

#### Value Function Approximation

  So far we have represented value function by a lookup table
    Every state s has an entry V(s)
    Or overy state-action pair s,a has an engry Q(s,a)

  Problem with large MDPs:
    There are too many states and/or actions to store in memory
    It is too slow to learn the value of each state individually

  Solution for large MDPs:
    Estimate value function with function approximation
         v_hat(s,w) ~~ v_pi(s)
      or q_hat(s,a,w) ~~ q_pi(s,a)

    Generalise from seen states to unseen states
    Update parameter w using MC or TD learning

  ë²ˆì—­
    ê·¼ì‚¬ í•¨ìˆ˜ë¥¼ í†µí•´ì„œ, ì‹¤ì œ valueì™€ ì‹¤ì œ që¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ
    ê·¸ëŸ¬ê¸° ìœ„í•´ì„œ ê·¼ì‚¬ í•¨ìˆ˜ì˜ parameterì¸ wë¥¼ í•™ìŠµ ì‹œì¼œì•¼í•˜ê³  ê·¸ë•Œ MC or TD learningì„ ì´ìš©í•˜ëŠ” ê²ƒ.
    ì´ë ‡ê²Œ í•™ìŠµì„ ì‹œí‚¤ë©´, ë³´ì§€ ëª» í•œ stateë„ ë³¸ stateë“¤ë¡œ ì¼ë°˜í™” í•  ìˆ˜ ìˆë‹¤.

  Which Function Approximation?
    There are many function approximations, e.g.
      Linear combinations of features
      Neural network
      Decision tree
      Nearest neighbour
      Fourier / wavelet bases
      ...

  ì´ì¤‘ì— ë¯¸ë¶„ì„ í†µí•´ í•™ìŠµì´ ê°€ëŠ¥í•œ, Lineaer combinations of features, Neural network ì´ ë‘ê°€ì§€ë¥¼ ë³´í†µ ì‚¬ìš©í•˜ê³¤ í•œë‹¤.

  ë˜í•œ, non-stationaryí•˜ê³  non-iidí•œ ë°ì´í„°ì— ì ì ˆí•œ training methodë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

#### Incremental Methods
  Incremental (ì¡°ê¸ˆì”© ë°”ê¿”ë‚˜ê°€ëŠ”) ë°©ë²•ë“¤ì— ëŒ€í•´ ë°°ìš¸ ê²ƒì´ë‹¤.

  Gradient descent
    Let J(w) be a differentiable function of parameter vector w
    Define the gradient of J(w) to be
      Delta_w J(w) = (w1ì—ëŒ€í•œ Jì˜ í¸ë¯¸ë¶„, w2ì— ëŒ€í•œ Jì˜ í¸ë¯¸ë¶„, ...)

    To find a local minimum of J(w)
    Adjust w in direction of -ve gradient
      Delta w = -(1/2) * alpha * Delta_w J(w)
      where alpha is a step-size parameter

#### Value Function Approximation By Stochastic Gradient Descent

  Goal : find parameter vector w minimising mean-squared error between approximate value function v_hat(s,w) and true value function v_pi(s)
    J(w) = E_pi[(v_pi(s) - v_hat(s,w))^2]

  Gradient descent finds a local minimum
    Delta w = -(1/2) * alpha * Delta_w J(w)
            = alpha * E_pi[(v_pi(s) - v_hat(s,w)) * Delta_w v_hat(s,w)]

  Stochastic gradient descent samples the gradient
    Delta w = alpha * (v_pi(s) - v_hat(s,w)) * Delta_w v_hat(s,w)
    ê¸°ëŒ“ê°’ì„ ì§ì ‘ êµ¬í•˜ì§€ ì•Šê³ , Samplingì„ í†µí•´ estimation í•˜ëŠ” ê²ƒ

  Expected update is equal to full gradient update

#### Feature Vectors

  Represente state by a feature vector
    x(S) = (x1(S), x2(S), ... , xn(S))

  For example:
    Distance of robot from landmarks
    Trends in the stock market
    Piece and pawn configurations in chess

  ì–´ë–¤ ê°’ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” íŠ¹ì„±ì´ë¼ê³  ìƒê°í•˜ë©´ ë¨

#### Linear Value Function Approximation

  Represent value function by a linear combination of features
    v_hat(s,w) = x(S)^Tw = Sigma{j=1->n} xj(S)*wj

    Valueë¼ëŠ” ê²ƒì´ weightë“¤ê³¼ featureë“¤ì˜ ë‚´ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤! ë¼ëŠ” ê²ƒì´
    Linear value function approximationì˜ ì›ë¦¬ ê·¸ëƒ¥ Linear regressionì´ë‹¤!!

  Objective function is quadratic in parameters w
    J(w) = E_pi[(v_pi(S) - x(S)^Tw)^2]

    ëª©ì  í•¨ìˆ˜ëŠ” True valueì™€ Estimation value ì‚¬ì´ì˜ MSE!

  Stochastic gradient descent converges on global optimum
  Update rule is particularly simple
    Delta_w v_hat(S,w) = x(S)
    => ì™œëƒí•˜ë©´, ë§¨ ì²˜ìŒ ì‹ì—ì„œ wì— ëŒ€í•´ ë¯¸ë¶„í•˜ë©´ ë‚˜ì˜´
    Delta w = alpha * (v_pi(S) - v_hat(S,w)) * x(S)
    => ì™œëƒí•˜ë©´, stochastic descentì—ì„œ ì´ë ‡ê²Œ ì‚¬ìš©í•  ê±°ì„ x(s)ê°€ ê³±í•´ì§€ëŠ”ê±´

    Update = step-size * prediction error * feature value

#### Table Lookup Features

  Table lookup is a special case of linear value function approximation
  Using table lookup features
    x^table(S) = (I(S=s1), ... , I(S=sn))

  Parameter vector w gives value of each individual state
    v_hat(S,w) = (I(S=s1), ..., I(S=sn)) * (w1, ... , wn)

#### Incremental Prediction Algorithm

  Have assumed true value function v_pi(s) given by superviser
  But in RL there is no supervisor, only rewards
  In practice, we substitute a target for v_pi(s)
    For MC, the target is the return G_t
      âˆ†w = ğ›¼ * (G_t - v_hat(St,w)) * âˆ‡w v_hat(St,w)

    For TD(0), the target is the TD target R(t+1) + ğ›¾ * v_hat(S(t+1), w)
      âˆ†w = ğ›¼ * (R(t+1) + + ğ›¾ * v_hat(S(t+1), w) - v_hat(St,w)) * âˆ‡w v_hat(St,w)

    For TD(ğœ†), the garget is the ğœ†-return G^ğœ†_t
      âˆ†w = ğ›¼ * (G^ğœ†_t - v_hat(St,w)) * âˆ‡w v_hat(St,w)

















d
