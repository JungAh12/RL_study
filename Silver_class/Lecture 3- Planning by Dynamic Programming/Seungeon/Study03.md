# RL-study-2019

Planning by Dynamic Programming

1. What is Dynamic programming

  ë³µì¡í•œ ë¬¸ì œë¥¼ subproblemìœ¼ë¡œ ì˜ê²Œ ìª¼ê°œì„œ í‘¸ëŠ” ê²ƒ

  DPë¡œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì´ ì§€ë‹Œ íŠ¹ì„± ë‘ ê°€ì§€

    Optimal substructure => ì‘ì€ ë¬¸ì œë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

      - Principle of optimality applies
      - Optimal solution can be decomposed into subproblmes

    Overlapping subproblems => subproblemì— ëŒ€í•œ ì†”ë£¨ì…˜ë“¤ì„ ì €ì¥í•´ ë†¨ë‹¤ê°€ ì“¸ ìˆ˜ ìˆì–´ì•¼í•œë‹¤.

      - Subproblems recur many times
      - Solutions can be cached and reused

    Markov Decision ProcessëŠ” ìœ„ ë‘ê°€ì§€ íŠ¹ì„±ì„ ë§Œì¡±í•œë‹¤.

      - Bellman equation gives recursive decomposition
      - Value function stores and reuses solutions

  Dynamic programmingì€ MDPì˜ full knowledgeì„ ì•Œê³  ìˆì–´ì•¼ í•œë‹¤.

  DPëŠ” planningì— ì“°ì¸ë‹¤. (Modelì„ ì•Œ ë•Œ í‘¸ëŠ” ë¬¸ì œ)

    For prediction : (value functionì„ ë§ì¶”ëŠ” ë¬¸ì œ)
      Input  : MDP
         or  : MRP
      Output : value function v_ğœ‹

    Or for control : (optimal policyë¥¼ ë§ì¶”ëŠ” ë¬¸ì œ)
      Input  : MDP
      Output : optimal value function v_*
         and : optimal policy ğœ‹_*

2. Dynamic programmingì€ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ì“°ì¸ë‹¤ê³  í•œë‹¤.

  - String algorithms (sequence alignment)
  - Graph algorithm (shortest path algorithms)
  - Graphical models (Viterbi algorithm)
  - Bioinformatics (lattice models)

3. Policy Evaluation

  Problem  : policy ğœ‹ë¥¼ evaluationí•˜ëŠ” ê²ƒ
  Solution : iterative application of Bellman expectation backup

  v_1 -> v_2 -> ... -> v_ğœ‹  ==> v_1ìœ¼ë¡œ ë¶€í„° v_ğœ‹ë¥¼ ê³„ì‚°í•´ë‚´ëŠ”ê²ƒ

  Using synchronous backups
    - At each iteration k+1
    - For all states s
    - Update v_(k+1)(s) from v_k(s')
    - Where s' is a successor state of s

  ==> ëª¨ë“  stateì— ëŒ€í•´ì„œ, valueë¥¼ update í•´ ê°€ëŠ” ê²ƒ





















ã…‡
