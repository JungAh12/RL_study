# RL-study-2019

Planning by Dynamic Programming

### 1. What is Dynamic programming

  ë³µì¡í•œ ë¬¸ì œë¥¼ subproblemìœ¼ë¡œ ì˜ê²Œ ìª¼ê°œì„œ í‘¸ëŠ” ê²ƒ

  DPë¡œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì´ ì§€ë‹Œ íŠ¹ì„± ë‘ ê°€ì§€

    Optimal substructure => ì‘ì€ ë¬¸ì œë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

      - Principle of optimality applies
      - Optimal solution can be decomposed into subproblmes

    Overlapping subproblems => subproblemì— ëŒ€í•œ ì†”ë£¨ì…˜ë“¤ì„ ì €ì¥í•´ ë†¨ë‹¤ê°€ ì“¸ ìˆ˜ ìˆì–´ì•¼í•œë‹¤.

      - Subproblems recur many times
      - Solutions can be cached and reused

    Markov Decision ProcessëŠ” ìœ„ ë‘ê°€ì§€ íŠ¹ì„±ì„ ë§Œì¡±í•œë‹¤.

      - Bellman equation gives recursive decomposition
      - Value function stores and reuses solutions

  Dynamic programmingì€ MDPì˜ full knowledgeì„ ì•Œê³  ìˆì–´ì•¼ í•œë‹¤.

  DPëŠ” planningì— ì“°ì¸ë‹¤. (Modelì„ ì•Œ ë•Œ í‘¸ëŠ” ë¬¸ì œ)

    For prediction : (value functionì„ ë§ì¶”ëŠ” ë¬¸ì œ)
      Input  : MDP
         or  : MRP
      Output : value function v_ğœ‹

    Or for control : (optimal policyë¥¼ ë§ì¶”ëŠ” ë¬¸ì œ)
      Input  : MDP
      Output : optimal value function v_*
         and : optimal policy ğœ‹_*

### 2. Dynamic programmingì€ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ì“°ì¸ë‹¤ê³  í•œë‹¤.

  - String algorithms (sequence alignment)
  - Graph algorithm (shortest path algorithms)
  - Graphical models (Viterbi algorithm)
  - Bioinformatics (lattice models)

### 3. Policy Evaluation

  Problem  : policy ğœ‹ë¥¼ evaluationí•˜ëŠ” ê²ƒ
  Solution : iterative application of Bellman expectation backup

  v_1 -> v_2 -> ... -> v_ğœ‹  ==> v_1ìœ¼ë¡œ ë¶€í„° v_ğœ‹ë¥¼ ê³„ì‚°í•´ë‚´ëŠ”ê²ƒ

  Using synchronous backups
    - At each iteration k+1
    - For all states s
    - Update v_(k+1)(s) from v_k(s')
    - Where s' is a successor state of s

  ==> ëª¨ë“  stateì— ëŒ€í•´ì„œ, valueë¥¼ update í•´ ê°€ëŠ” ê²ƒ

  Bellman expectation equationì„ í†µí•´ì„œ evaluationí•´ê°€ëŠ” ê³¼ì •

  v^(k+1) = R^ğœ‹ + ğ›¾ * P^ğœ‹ * v^k

  ì´ë ‡ê²Œ ì—…ë°ì´íŠ¸ë¥¼ í•´ì£¼ë©´ Rì€ ì •í™•í•˜ê¸° ë•Œë¬¸ì— ë°˜ë³µí•˜ë‹¤ë³´ë©´ ì‹¤ì œê°’ì— ìˆ˜ë ´í•˜ê²Œëœë‹¤.

  ****ì˜ˆì‹œ****
    Pridiction ë¬¸ì œì¸ê±° ì•Œ ìˆ˜ ìˆê² ì£ ? ê° stateì˜ valueë¥¼ êµ¬í•˜ëŠ” ê±°ë‹ˆê¹Œ

    actionì´ (1/4)ë¡œ ìƒí•˜ì¢Œìš°ì´ê³  r = -1 on all transitions í•œë²ˆ ì›€ì§ì¼ë•Œë§ˆë‹¤ ë³´ìƒì´ -1

  - Undiscounted episodic MDP
  - Nonterminal states 1, ... , 14
  - One terminal state (shown twice as shaded squares)
  - Actions leading out of the grid leave state unchanged
  - Reward is -1 until the terminal state is reached
  - Agent follows uniform random policy

### 4. How to Improve a Policy

  - Given a policy ğœ‹

    - Evaluate the policy ğœ‹

      v_ğœ‹(s) = E[R_(t+1) + ğ›¾ * R_(t+2) + ... | S_t = s]

    - Improve the policy by acting greedily with respect to v_ğœ‹

      ğœ‹' = greedy(v_ğœ‹)

  - In small gridworld improved policy was optimal, ğœ‹' = ğœ‹*
  - In general, need more iterations of improvement / evaluation
  - But this process of policy iteration always converges to ğœ‹*












ã…‡
