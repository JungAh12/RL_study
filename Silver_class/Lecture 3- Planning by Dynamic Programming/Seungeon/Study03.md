# RL-study-2019

Planning by Dynamic Programming

### 1. What is Dynamic programming

  ë³µì¡í•œ ë¬¸ì œë¥¼ subproblemìœ¼ë¡œ ì˜ê²Œ ìª¼ê°œì„œ í‘¸ëŠ” ê²ƒ

  DPë¡œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì´ ì§€ë‹Œ íŠ¹ì„± ë‘ ê°€ì§€

    Optimal substructure => ì‘ì€ ë¬¸ì œë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

      - Principle of optimality applies
      - Optimal solution can be decomposed into subproblmes

    Overlapping subproblems => subproblemì— ëŒ€í•œ ì†”ë£¨ì…˜ë“¤ì„ ì €ì¥í•´ ë†¨ë‹¤ê°€ ì“¸ ìˆ˜ ìˆì–´ì•¼í•œë‹¤.

      - Subproblems recur many times
      - Solutions can be cached and reused

    Markov Decision ProcessëŠ” ìœ„ ë‘ê°€ì§€ íŠ¹ì„±ì„ ë§Œì¡±í•œë‹¤.

      - Bellman equation gives recursive decomposition
      - Value function stores and reuses solutions

  Dynamic programmingì€ MDPì˜ full knowledgeì„ ì•Œê³  ìˆì–´ì•¼ í•œë‹¤.

  DPëŠ” planningì— ì“°ì¸ë‹¤. (Modelì„ ì•Œ ë•Œ í‘¸ëŠ” ë¬¸ì œ)

    For prediction : (value functionì„ ë§ì¶”ëŠ” ë¬¸ì œ)
      Input  : MDP
         or  : MRP
      Output : value function v_ğœ‹

    Or for control : (optimal policyë¥¼ ë§ì¶”ëŠ” ë¬¸ì œ)
      Input  : MDP
      Output : optimal value function v_*
         and : optimal policy ğœ‹_*

### 2. Dynamic programmingì€ ì—¬ëŸ¬ ë¶„ì•¼ì—ì„œ ì“°ì¸ë‹¤ê³  í•œë‹¤.

  - String algorithms (sequence alignment)
  - Graph algorithm (shortest path algorithms)
  - Graphical models (Viterbi algorithm)
  - Bioinformatics (lattice models)

### 3. Policy Evaluation

  Problem  : policy ğœ‹ë¥¼ evaluationí•˜ëŠ” ê²ƒ
  Solution : iterative application of Bellman expectation backup

  v_1 -> v_2 -> ... -> v_ğœ‹  ==> v_1ìœ¼ë¡œ ë¶€í„° v_ğœ‹ë¥¼ ê³„ì‚°í•´ë‚´ëŠ”ê²ƒ

  Using synchronous backups
    - At each iteration k+1
    - For all states s
    - Update v_(k+1)(s) from v_k(s')
    - Where s' is a successor state of s

  ==> ëª¨ë“  stateì— ëŒ€í•´ì„œ, valueë¥¼ update í•´ ê°€ëŠ” ê²ƒ

  Bellman expectation equationì„ í†µí•´ì„œ evaluationí•´ê°€ëŠ” ê³¼ì •

  v^(k+1) = R^ğœ‹ + ğ›¾ * P^ğœ‹ * v^k

  ì´ë ‡ê²Œ ì—…ë°ì´íŠ¸ë¥¼ í•´ì£¼ë©´ Rì€ ì •í™•í•˜ê¸° ë•Œë¬¸ì— ë°˜ë³µí•˜ë‹¤ë³´ë©´ ì‹¤ì œê°’ì— ìˆ˜ë ´í•˜ê²Œëœë‹¤.

  ****ì˜ˆì‹œ****
    Pridiction ë¬¸ì œì¸ê±° ì•Œ ìˆ˜ ìˆê² ì£ ? ê° stateì˜ valueë¥¼ êµ¬í•˜ëŠ” ê±°ë‹ˆê¹Œ

    actionì´ (1/4)ë¡œ ìƒí•˜ì¢Œìš°ì´ê³  r = -1 on all transitions í•œë²ˆ ì›€ì§ì¼ë•Œë§ˆë‹¤ ë³´ìƒì´ -1

  - Undiscounted episodic MDP
  - Nonterminal states 1, ... , 14
  - One terminal state (shown twice as shaded squares)
  - Actions leading out of the grid leave state unchanged
  - Reward is -1 until the terminal state is reached
  - Agent follows uniform random policy

### 4. How to Improve a Policy

  - Given a policy ğœ‹

    - Evaluate the policy ğœ‹

      v_ğœ‹(s) = E[R_(t+1) + ğ›¾ * R_(t+2) + ... | S_t = s]

    - Improve the policy by acting greedily with respect to v_ğœ‹

      ğœ‹' = greedy(v_ğœ‹)

  - In small gridworld improved policy was optimal, ğœ‹' = ğœ‹*
  - In general, need more iterations of improvement / evaluation
  - But this process of policy iteration always converges to ğœ‹*

  - This improves the value from any state s over one step,

    q_ğœ‹(s, ğœ‹'(s)) = {a} max q_ğœ‹(s,a) >= q_ğœ‹(s, ğœ‹(s)) = v_ğœ‹(s)
    => q_ğœ‹(s, ğœ‹'(s)) >= v_ğœ‹(s)

  - It therefore improves the value function, v_ğœ‹'(s) >= v_ğœ‹(s)

    q_ğœ‹(s, ğœ‹'(s)) = E_ğœ‹'[R_(t+1) + ğ›¾ * v_ğœ‹(S_(t+1)) | S_t = s]
                 <= E_ğœ‹'[R_(t+1) + ğ›¾ * q_ğœ‹(S_(t+1), ğœ‹'(S_(t+1)) | S_t = s]
                 <= E_ğœ‹'[R_(t+1) + ğ›¾ * R_(t+2) + ... | S_t = s] = v_ğœ‹'(s)

  - If improvements stop,

    q_ğœ‹(s,ğœ‹'(s)) = {a} max q_ğœ‹(s,a) = q_ğœ‹(s, ğœ‹(s)) = v_ğœ‹(s)

  - Then the Bellman optimality equation has been satisfied

    v_ğœ‹(s) = {a} max q_ğœ‹(s,a)

  - Therefore v_ğœ‹(s) = v_*(s) for all s
  - so ğœ‹ is an optimal policy

### 5. Principle of Optimality

  ëª¨ë“  optimal policyëŠ” ë‘ê°œì˜ componentë“¤ë¡œ ë‚˜ë‰œë‹¤.
  - An optimal first action A*
  - Followed by an optimal policy from successor state S'

   > Teorem (Principle of Optimality)

   A policy ğœ‹(a|s) achieves the optimal value from state s, v_ğœ‹(s) = v_*(s), if and only if

    - For any state s' reachable from s
    - ğœ‹ achieves the optimal value from state s', v_ğœ‹(s') = v_*(s')

    ë§Œì•½ së¡œë¶€í„° ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ëª¨ë“  s'ì— ëŒ€í•´ ğœ‹ê°€ v_ğœ‹(s') = v_*(s')ë¥¼ ë§Œì¡±í•˜ê²Œ ëœë‹¤ë©´ policy ğœ‹(a|s)ëŠ” state sì— ëŒ€í•´ v_ğœ‹(s) = v_âˆ—(s)ë¥¼ ì´ë£° ìˆ˜ ìˆë‹¤.

### 6. Deterministic Value Iteration

  - ìš°ë¦¬ê°€ subproblemì¸ v*(s')ì„ ì•Œê²Œ ëœë‹¤ë©´

  - solution v*(s)ëŠ” ë‹¤ìŒê³¼ one-step look aheadë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆë‹¤.
    v*(s) <== {a} max R^a_s + ğ›¾ * {s} Î£ P^a_(ss') * vâˆ—(s')

  - The idea of value iteration is to apply these updates iteratively
  - Intuition : start with final rewards and work backwards
  - Still works with loopy, stochastic MDPs

### 7. Value Iteration

  - Problem  : find optimal policy ğœ‹
  - Solution : iterative application of Bellman optimality backup
  - v1 -> v2 -> ... -> v*
  - Using synchronous backups
      - At each iteration k+1
      - For all states s
      - Update v_(k+1)(s) from v_k(s')

  - Convergence to v_* will be proven later
  - Unlike policy iteration, there is no explicit policy
  - Intermediate value functions may not correspond to any policy










ã…‡
