# RL-study-2019

Policy Gradient

### Policy- Based Reinforcement Learning

  In the last lecture we approximated the value or action-value function using parameters theta,
    Vğœƒ(s)   â‰ˆ V^ğœ‹(s)
    Qğœƒ(s,a) â‰ˆ Q^ğœ‹(s,a)

  A policy was generated directly from the value function
    e.g. using e-greedy

  In this lecture we will directly parameterise the policy
    ğœ‹ğœƒ(s,a) = P[a|s,ğœƒ]

  We will focus again on model-free reinforcement learning

  => ì €ë²ˆ ì¥ì—ì„œ, Function approximationì„ í†µí•´ value function í˜¹ì€ action value function Qê°’ì„ ê·¼ì‚¬ í•´ ë³´ì•˜ë‹¤.
  => ë˜í•œ policyëŠ” value functionì„ í†µí•´ ì •í•´ì§€ëŠ”ê²Œ ë³´í†µì´ì—ˆë‹¤. í•˜ì§€ë§Œ, ì´ë²ˆ ì¥ì—ì„œëŠ” policyë¥¼ ì§ì ‘ì ìœ¼ë¡œ parameteriseí•  ê²ƒì´ë‹¤. ì–´ë–»ê²Œ? P[a|s,ğœƒ]ë¼ëŠ” ì‹ì„ í†µí•´ì„œã…‡ã…‡
  => ì´ ë°©ë²•ì€ model-free reinforcement learningì´ë‹¤.

#### Value-Based and Policy-Based RL
















d
