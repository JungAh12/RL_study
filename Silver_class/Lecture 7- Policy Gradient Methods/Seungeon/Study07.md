# RL-study-2019

Policy Gradient

### Policy- Based Reinforcement Learning

  In the last lecture we approximated the value or action-value function using parameters theta,
    Vğœƒ(s)   â‰ˆ V^ğœ‹(s)
    Qğœƒ(s,a) â‰ˆ Q^ğœ‹(s,a)

  A policy was generated directly from the value function
    e.g. using e-greedy

  In this lecture we will directly parameterise the policy
    ğœ‹ğœƒ(s,a) = P[a|s,ğœƒ]

  We will focus again on model-free reinforcement learning

  => ì €ë²ˆ ì¥ì—ì„œ, Function approximationì„ í†µí•´ value function í˜¹ì€ action value function Qê°’ì„ ê·¼ì‚¬ í•´ ë³´ì•˜ë‹¤.
  => ë˜í•œ policyëŠ” value functionì„ í†µí•´ ì •í•´ì§€ëŠ”ê²Œ ë³´í†µì´ì—ˆë‹¤. í•˜ì§€ë§Œ, ì´ë²ˆ ì¥ì—ì„œëŠ” policyë¥¼ ì§ì ‘ì ìœ¼ë¡œ parameteriseí•  ê²ƒì´ë‹¤. ì–´ë–»ê²Œ? P[a|s,ğœƒ]ë¼ëŠ” ì‹ì„ í†µí•´ì„œã…‡ã…‡
  => ì´ ë°©ë²•ì€ model-free reinforcement learningì´ë‹¤.

#### Value-Based and Policy-Based RL

  Value based RL
    Value functionì„ í•™ìŠµ
    Optimal valueë¥¼ ì•Œë©´ Optimal policyë¥¼ ì•„ëŠ” ê²ƒê³¼ ê°™ìœ¼ë¯€ë¡œ optimal valueë¥¼ êµ¬í•¨
    implicit policy (e.g. e-greedy)

  Policy based RL
    Value functionì„ ë‹¤ë£¨ì§€ ì•ŠìŒ
    Policyë¥¼ ì§ì ‘ í•™ìŠµ

  Actor-Critic
    Value functionì„ í•™ìŠµ
    Policy ë˜í•œ í•™ìŠµ
      Actor  : í–‰ë™í•˜ëŠ” ë†ˆ (Policy)
      Critic : í‰ê°€í•˜ëŠ” ë†ˆ (Value function)

#### Advantages of Policy-Based RL

  Advantages:
    - ìˆ˜ë ´ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤.
    - high-dimensional or continuous action spaceë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ì— íš¨ìœ¨ì ì´ë‹¤.
    - stochasticí•œ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
      ê°€ìœ„ë°”ìœ„ë³´ ê°™ì€ ê²½ìš° optimal policyê°€ 33% 33% 33%ì¸ë° ê·¸ëŸ°ê±¸ í•™ìŠµ ê°€ëŠ¥.

  Disadvantages:
    - ì¼ë°˜ì ìœ¼ë¡œ global optimumë³´ë‹¤ local optimumì— ìˆ˜ë ´í•œë‹¤.
    - Policyì˜ í‰ê°€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¹„íš¨ìœ¨ì ì´ê³  high varaianceì´ë‹¤.

  Example1 : ê°€ìœ„ë°”ìœ„ë³´
    Two-player game of ê°€ìœ„ë°”ìœ„ë³´
      - ê°€ìœ„ > ë³´
      - ë°”ìœ„ > ê°€ìœ„
      - ë³´ > ë°”ìœ„

    Consider policies for iterated ê°€ìœ„ë°”ìœ„ë³´
      - A deterministic policy is easily exploited
      - A uniform random policy is optimal

  Example2 : Aliased Gridworld
    The agent cannot differentiate the grey states
    Consider features of the following form
      ğœ™(s,a) = I(wall to N, a = move E)
      Featureê°€ ì™„ë²½í•˜ì§€ ì•Šì•„ì„œ POMDP ë¬¸ì œì¸ ê²ƒ
      íšŒìƒ‰ state 2ê°œì—ì„œ ë˜‘ê°™ì€ featurë¥¼ ì£¼ê¸° ë•Œë¬¸ì—, ì™¼ìª½ íšŒìƒ‰ë²½ì—ì„œëŠ” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°€ì•¼í•˜ê³  ì˜¤ë¥¸ìª½ íšŒìƒ‰ë²½ì—ì„œëŠ” ì™¼ìª½ì—ì„œ ê°€ì•¼í•˜ëŠ” ë¬¸ì œì¸ ê²ƒì´ë‹¤.

    Compare value-based RL, using an approximate value function
      Qğœƒ(s,a) = f(ğœ™(s,a,),ğœƒ)

    To policy-based RL, suing parametrised policy
      ğœ‹ğœƒ(s,a) = g(ğœ™(s,a,),ğœƒ)

    An optimal stochastic policy will randomly move E or W in grey states
      ğœ‹ğœƒ(wall to N and S, move E) = 0.5
      ğœ‹ğœƒ(wall to N and S, move W) = 0.5

    It will reach the goal state in a few steps with high probability
    Policy-based RL can learn the optimal stochastic policy

    ì´ëŸ¬í•œ ë¬¸ì œì—ì„œëŠ” íšŒìƒ‰ë²½ì¼ë•Œ ë°˜ë°˜ìœ¼ë¡œ ì›€ì§ì—¬ì£¼ëŠ” ì •ì±…ì´ ì¢‹ë‹¤.
    Value based RLì˜ ê²½ìš° deterministic policyë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ì´ê²Œ ë¶ˆê°€ëŠ¥

    ê·¸ëŸ°ë°, deterministicí•œ policyë¥¼ ì™œ ëª» ì“¸ê¹Œ? ê·¸ ì´ìœ ëŠ” Markov propertyë¥¼ ë§Œì¡±í•˜ëŠ” fully observable MDPì—ì„œëŠ” ìµœì ì˜ deterministic policyê°€ ë¶„ëª…íˆ ì¡´ì¬í•˜ì§€ë§Œ, ì´ ë¬¸ì œê°™ì€ Partially observable MDPë¬¸ì œì˜ ê²½ìš°ëŠ” ê·¸ë ‡ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

#### Policy Objective Functions

  Goal : given policy ğœ‹ğœƒ(s,a) with parameters ğœƒ, find best ğœƒ
  But how do we measure the quality of a policy ğœ‹ğœƒ?

  In episodic environments we can use the start value
    J1(ğœƒ) = V^ğœ‹ğœƒ(s1) = E_ğœ‹ğœƒ[v1]
    (ì²« ë²ˆì§¸ stateì—ì„œ value functionì„ policyì˜ ëª©ì í•¨ìˆ˜ë¡œ ì¡ì!)

  In continuing environments we can use the average value
    JavV(ğœƒ) = Î£ d^ğœ‹ğœƒ(s) * V^ğœ‹ğœƒ(s)

  Or the average reward per time-step
    JavR(ğœƒ) = Î£ d^ğœ‹ğœƒ(s) Î£ ğœ‹ğœƒ(s,a) * R(s,a)

  d^ğœ‹ğœƒ(s)ëŠ” Markov chain ğœ‹ì˜ stationary distributionì´ë‹¤.
  Policy ğœ‹ë¥¼ ë”°ë¼ ê³„ì† í–‰ë™í•˜ë‹¤ë³´ë©´ì€ ê° ìƒíƒœì— ë¨¸ë¬´ë¥´ëŠ” í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.
  ê·¸ê²ƒì´ d^ğœ‹ğœƒ(s)ì´ë‹¤.

#### Policy Optimisation

  Policy based reinforcement learning is an optimisation problem
  Find ğœƒ that maximises J(ğœƒ)
  Some approaches do not use gradient
    Hill climbing
    Simplex / amoeba / Nelder Mead
    Genetic algorithms

  Greater efficiency often possible using gradient
    Gradient descent
    Conjugate gradient
    Quasi-newton

  We focus on graidnet descent, many extensions possible
  And on methods that exploit sequential structure

#### Policy Gradient
  Let J(ğœƒ) be any policy objective function
  Policy gradient algorithms search for a local maximum in J(ğœƒ) by ascending the gradient of the policy, w.r.t. parameters ğœƒ
    âˆ†ğœƒ = ğ›¼âˆ‡ğœƒ J(ğœƒ)

  Where âˆ‡ğœƒ J(ğœƒ) is the policy gradient
    âˆ‡ğœƒ J(ğœƒ) = (ğœ•J(ğœƒ)/ğœ•ğœƒ1 , ğœ•J(ğœƒ)/ğœ•ğœƒ2, ... , ğœ•J(ğœƒ)/ğœ•ğœƒn)'

  and ğ›¼ is a step-size parameter

  Jì— ëŒ€í•œ gradientë¡œ ğœƒë¥¼ ì—…ë°ì´íŠ¸!

#### Computing gradients by finite difference

  To evaluate policy gradient of ğœ‹ğœƒ(s,a)
  For each dimension k in [1,n]
    Estimating kth partial derivative of objective function w.r.t ğœƒ
    By perturbing ğœƒ by small amount e in kth dimension
      ğœ•J(ğœƒ) / ğœ•ğœƒk â‰ˆ {J(ğœƒ+euk) - J(ğœƒ)} / e
      where uk is unit vector with 1 in kth component, 0 elsewhere

  Uses n evaluations to compute policy gradient in n dimensions
  Simple, noisy, inefficient - but sometimes effective
  Works for arbitrary policies, even if policy is not differentiable

  ğœƒ1, ğœƒ2, ... , ğœƒnì— ëŒ€í•´ finite differenceë¥¼ í†µí•´ J ì˜ gradientë¥¼ êµ¬í•˜ëŠ” ê²ƒ.

  ì‹¤ì œ ì‚¬ìš©ì˜ˆ : Training AIBO to walk by finite difference policy gradient

#### Score Function

  We now compute the policy gradient analytically
  Assume policy ğœ‹ğœƒ is differentiable whenever it is non-zero
  and we know the gradient âˆ‡ğœƒ ğœ‹ğœƒ(s,a)
  Likelihood ratios exploit the following identity
    âˆ‡ğœƒ ğœ‹ğœƒ(s,a) = ğœ‹ğœƒ(s,a) * âˆ‡ğœƒ ğœ‹ğœƒ(s,a) / ğœ‹ğœƒ(s,a)
               = ğœ‹ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a))

  The score function is âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a))

  ì´ëŸ° ê²ƒì„ Likelihood ratio trickì´ë¼ê³  í•œë‹¤. ì´ê±¸ ì™œ í•˜ëŠ”ì§€ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤.

#### Softmax Policy

  We will use a softmax policy as a running example
  Weight actions using linear combination of features ğœ™(s,a)^Tğœƒ
  Probability of action is proportional to exponentiated weight
    ğœ‹ğœƒ(s,a) âˆ e^ğœ™(s,a)^T * ğœƒ

  The score function is
    âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) = ğœ™(s,a) - E_ğœ‹ğœƒ[ğœ™(s,.)]

  ê³„ì‚°ì‹ì´ ìƒëµë˜ì—ˆê¸´ í•˜ì§€ë§Œ, softmaxë¡œ policyë¥¼ ì •í•´ì£¼ë©´ score functionì„ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ

#### Gaussian Policy
  In continuous action spaces, a Gaussian policy is natural
  Mean is a linear combination of state features mu(s) = ğœ™(s,a)^T * ğœƒ
  Variance may be fixed ğœ^2, or can also parameterised
  Policy is Gaussian, a ~ N(mu(s) , ğœ^2)

  The score function is
    âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) = (a-mu(s)) * ğœ™(s) / ğœ^2

  ê³„ì‚°ì‹ì´ ìƒëµë˜ì—ˆê¸´ í•˜ì§€ë§Œ, Gaussian policyë¥¼ ì‚¬ìš©í•˜ë©´ ìœ„ì™€ ê°™ì€ score functionì„ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ

#### One-Step MDP

  Consider a simple class of one-step MDPs
    Starting in states s~d(s)
    Terminating after one time-step with reward r = R(s,a)

  Use likelihood ratios to compute the policy gradient
    J(ğœƒ) = E_ğœ‹ğœƒ[r]
         = Î£ d^ğœ‹ğœƒ(s) Î£ ğœ‹ğœƒ(s,a) * R(s,a)

    âˆ‡ğœƒ J(ğœƒ) = Î£ d^ğœ‹ğœƒ(s) Î£ ğœ‹ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * R(s,a)
            E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * r]

  => ì•„ì£¼ ì¤‘ìš”!! J(ğœƒ)ì˜ gradientë¥¼ ê¸°ëŒ“ê°’ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤!
  => Samplingì„ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ (Likelihood ratio trickì¢‹ì§€?)

#### Policy Gradient Theorem

  The policy gradient theorem generalises the likelihood ratio appraoch to multi-step MDPs
  Replaces instantaneous reward r with long-term value Q^ğœ‹(s,a)
  Policy gradient theorem applies to start state objective, average reward and average value objective

  Theorem
    For any differentiable policy ğœ‹ğœƒ(s,a), for any of the policy objective functions J=J1, J_avR, or 1/(1-ğ›¾)JavV, the policy gradient is
      âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * Q^ğœ‹ğœƒ(s,a)]

#### Monte-Carlo Policy Gradient (REINFORCE)
  Update parameters by stochastic gradient ascent
  Using policy gradient theorem
  Using return vt as an unbiased sample of Q^ğœ‹ğœƒ(st,at)
    âˆ†ğœƒt = ğ›¼ * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * vt

  Pseudo code

  function REINFORCE
    Initialise ğœƒ arbitrarily
    for each episode {s1, a1, r2, ... , s(T-1), a(T-1), rT} ~ ğœ‹ğœƒ do
      for t = 1 to T-1 do
       ğœƒ <- ğœƒ + ğ›¼ * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * vt
      end for
    end for
    return ğœƒ
  end function

  Monte-Cralo ê¸°ë²•ì„ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— Q ê°’ ëŒ€ì‹  returenì„ ì“°ê³  ìˆë‹¤.

  Puck World Example
    Continuous actions exert small force on puck
    Puck is rewarded for getting close to target
    Target location is reset every 30 secodnds
    Policy is trained using variant of Monte-Carlo policy gradient

  => ì£¼ì˜í•´ì„œ ë´ì•¼í•  ì 1 : í•™ìŠµ ê³¡ì„ ì´ ì§€ê·¸ì¬ê·¸ê°€ ì•„ë‹ˆë¼ ë§¤ë„ëŸ½ê²Œ ì˜¬ë¼ê°„ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤ ì¦‰, stable í•˜ë‹¤.

  => ì£¼ì˜í•´ì„œ ë´ì•¼í•  ì 2 : varianceê°€ ë„ˆë¬´ ì»¤ì„œ, iterationì´ 9*10^7 ì •ë„ ë˜ì•¼ ìˆ˜ë ´ì´ ëœë‹¤. ì¦‰, í•™ìŠµì´ ëŠë¦¬ë‹¤.

### Actor - Critic Policy Gradient









d
