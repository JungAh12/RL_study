# RL-study-2019

Policy Gradient

### Policy- Based Reinforcement Learning

  In the last lecture we approximated the value or action-value function using parameters theta,
    Vğœƒ(s)   â‰ˆ V^ğœ‹(s)
    Qğœƒ(s,a) â‰ˆ Q^ğœ‹(s,a)

  A policy was generated directly from the value function
    e.g. using e-greedy

  In this lecture we will directly parameterise the policy
    ğœ‹ğœƒ(s,a) = P[a|s,ğœƒ]

  We will focus again on model-free reinforcement learning

  => ì €ë²ˆ ì¥ì—ì„œ, Function approximationì„ í†µí•´ value function í˜¹ì€ action value function Qê°’ì„ ê·¼ì‚¬ í•´ ë³´ì•˜ë‹¤.
  => ë˜í•œ policyëŠ” value functionì„ í†µí•´ ì •í•´ì§€ëŠ”ê²Œ ë³´í†µì´ì—ˆë‹¤. í•˜ì§€ë§Œ, ì´ë²ˆ ì¥ì—ì„œëŠ” policyë¥¼ ì§ì ‘ì ìœ¼ë¡œ parameteriseí•  ê²ƒì´ë‹¤. ì–´ë–»ê²Œ? P[a|s,ğœƒ]ë¼ëŠ” ì‹ì„ í†µí•´ì„œã…‡ã…‡
  => ì´ ë°©ë²•ì€ model-free reinforcement learningì´ë‹¤.

#### Value-Based and Policy-Based RL

  Value based RL
    Value functionì„ í•™ìŠµ
    Optimal valueë¥¼ ì•Œë©´ Optimal policyë¥¼ ì•„ëŠ” ê²ƒê³¼ ê°™ìœ¼ë¯€ë¡œ optimal valueë¥¼ êµ¬í•¨
    implicit policy (e.g. e-greedy)

  Policy based RL
    Value functionì„ ë‹¤ë£¨ì§€ ì•ŠìŒ
    Policyë¥¼ ì§ì ‘ í•™ìŠµ

  Actor-Critic
    Value functionì„ í•™ìŠµ
    Policy ë˜í•œ í•™ìŠµ
      Actor  : í–‰ë™í•˜ëŠ” ë†ˆ (Policy)
      Critic : í‰ê°€í•˜ëŠ” ë†ˆ (Value function)

#### Advantages of Policy-Based RL

  Advantages:
    - ìˆ˜ë ´ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤.
    - high-dimensional or continuous action spaceë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ì— íš¨ìœ¨ì ì´ë‹¤.
    - stochasticí•œ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
      ê°€ìœ„ë°”ìœ„ë³´ ê°™ì€ ê²½ìš° optimal policyê°€ 33% 33% 33%ì¸ë° ê·¸ëŸ°ê±¸ í•™ìŠµ ê°€ëŠ¥.

  Disadvantages:
    - ì¼ë°˜ì ìœ¼ë¡œ global optimumë³´ë‹¤ local optimumì— ìˆ˜ë ´í•œë‹¤.
    - Policyì˜ í‰ê°€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¹„íš¨ìœ¨ì ì´ê³  high varaianceì´ë‹¤.

  Example1 : ê°€ìœ„ë°”ìœ„ë³´
    Two-player game of ê°€ìœ„ë°”ìœ„ë³´
      - ê°€ìœ„ > ë³´
      - ë°”ìœ„ > ê°€ìœ„
      - ë³´ > ë°”ìœ„

    Consider policies for iterated ê°€ìœ„ë°”ìœ„ë³´
      - A deterministic policy is easily exploited
      - A uniform random policy is optimal

  Example2 : Aliased Gridworld
    The agent cannot differentiate the grey states
    Consider features of the following form
      ğœ™(s,a) = I(wall to N, a = move E)
      Featureê°€ ì™„ë²½í•˜ì§€ ì•Šì•„ì„œ POMDP ë¬¸ì œì¸ ê²ƒ
      íšŒìƒ‰ state 2ê°œì—ì„œ ë˜‘ê°™ì€ featurë¥¼ ì£¼ê¸° ë•Œë¬¸ì—, ì™¼ìª½ íšŒìƒ‰ë²½ì—ì„œëŠ” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°€ì•¼í•˜ê³  ì˜¤ë¥¸ìª½ íšŒìƒ‰ë²½ì—ì„œëŠ” ì™¼ìª½ì—ì„œ ê°€ì•¼í•˜ëŠ” ë¬¸ì œì¸ ê²ƒì´ë‹¤.

    Compare value-based RL, using an approximate value function
      Qğœƒ(s,a) = f(ğœ™(s,a,),ğœƒ)

    To policy-based RL, suing parametrised policy
      ğœ‹ğœƒ(s,a) = g(ğœ™(s,a,),ğœƒ)

    An optimal stochastic policy will randomly move E or W in grey states
      ğœ‹ğœƒ(wall to N and S, move E) = 0.5
      ğœ‹ğœƒ(wall to N and S, move W) = 0.5

    It will reach the goal state in a few steps with high probability
    Policy-based RL can learn the optimal stochastic policy

    ì´ëŸ¬í•œ ë¬¸ì œì—ì„œëŠ” íšŒìƒ‰ë²½ì¼ë•Œ ë°˜ë°˜ìœ¼ë¡œ ì›€ì§ì—¬ì£¼ëŠ” ì •ì±…ì´ ì¢‹ë‹¤.
    Value based RLì˜ ê²½ìš° deterministic policyë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ì´ê²Œ ë¶ˆê°€ëŠ¥

    ê·¸ëŸ°ë°, deterministicí•œ policyë¥¼ ì™œ ëª» ì“¸ê¹Œ? ê·¸ ì´ìœ ëŠ” Markov propertyë¥¼ ë§Œì¡±í•˜ëŠ” fully observable MDPì—ì„œëŠ” ìµœì ì˜ deterministic policyê°€ ë¶„ëª…íˆ ì¡´ì¬í•˜ì§€ë§Œ, ì´ ë¬¸ì œê°™ì€ Partially observable MDPë¬¸ì œì˜ ê²½ìš°ëŠ” ê·¸ë ‡ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

#### Policy Objective Functions

  Goal : given policy ğœ‹ğœƒ(s,a) with parameters ğœƒ, find best ğœƒ
  But how do we measure the quality of a policy ğœ‹ğœƒ?
  In episodic environments we can use the start value
    J1(ğœƒ) = V^ğœ‹ğœƒ(s1) = E_ğœ‹ğœƒ[v1]

  In continuing environments we can use the average value
    JavV(ğœƒ) =
















d
