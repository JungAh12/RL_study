# RL-study-2019

Policy Gradient

### Policy- Based Reinforcement Learning

  In the last lecture we approximated the value or action-value function using parameters theta,
    Vğœƒ(s)   â‰ˆ V^ğœ‹(s)
    Qğœƒ(s,a) â‰ˆ Q^ğœ‹(s,a)

  A policy was generated directly from the value function
    e.g. using e-greedy

  In this lecture we will directly parameterise the policy
    ğœ‹ğœƒ(s,a) = P[a|s,ğœƒ]

  We will focus again on model-free reinforcement learning

  => ì €ë²ˆ ì¥ì—ì„œ, Function approximationì„ í†µí•´ value function í˜¹ì€ action value function Qê°’ì„ ê·¼ì‚¬ í•´ ë³´ì•˜ë‹¤.
  => ë˜í•œ policyëŠ” value functionì„ í†µí•´ ì •í•´ì§€ëŠ”ê²Œ ë³´í†µì´ì—ˆë‹¤. í•˜ì§€ë§Œ, ì´ë²ˆ ì¥ì—ì„œëŠ” policyë¥¼ ì§ì ‘ì ìœ¼ë¡œ parameteriseí•  ê²ƒì´ë‹¤. ì–´ë–»ê²Œ? P[a|s,ğœƒ]ë¼ëŠ” ì‹ì„ í†µí•´ì„œã…‡ã…‡
  => ì´ ë°©ë²•ì€ model-free reinforcement learningì´ë‹¤.

#### Value-Based and Policy-Based RL

  Value based RL
    Value functionì„ í•™ìŠµ
    Optimal valueë¥¼ ì•Œë©´ Optimal policyë¥¼ ì•„ëŠ” ê²ƒê³¼ ê°™ìœ¼ë¯€ë¡œ optimal valueë¥¼ êµ¬í•¨
    implicit policy (e.g. e-greedy)

  Policy based RL
    Value functionì„ ë‹¤ë£¨ì§€ ì•ŠìŒ
    Policyë¥¼ ì§ì ‘ í•™ìŠµ

  Actor-Critic
    Value functionì„ í•™ìŠµ
    Policy ë˜í•œ í•™ìŠµ
      Actor  : í–‰ë™í•˜ëŠ” ë†ˆ (Policy)
      Critic : í‰ê°€í•˜ëŠ” ë†ˆ (Value function)

#### Advantages of Policy-Based RL

  Advantages:
    - ìˆ˜ë ´ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤.
    - high-dimensional or continuous action spaceë¬¸ì œë¥¼ í•´ê²°í•˜ê¸°ì— íš¨ìœ¨ì ì´ë‹¤.
    - stochasticí•œ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
      ê°€ìœ„ë°”ìœ„ë³´ ê°™ì€ ê²½ìš° optimal policyê°€ 33% 33% 33%ì¸ë° ê·¸ëŸ°ê±¸ í•™ìŠµ ê°€ëŠ¥.

  Disadvantages:
    - ì¼ë°˜ì ìœ¼ë¡œ global optimumë³´ë‹¤ local optimumì— ìˆ˜ë ´í•œë‹¤.
    - Policyì˜ í‰ê°€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¹„íš¨ìœ¨ì ì´ê³  high varaianceì´ë‹¤.

  Example1 : ê°€ìœ„ë°”ìœ„ë³´
    Two-player game of ê°€ìœ„ë°”ìœ„ë³´
      - ê°€ìœ„ > ë³´
      - ë°”ìœ„ > ê°€ìœ„
      - ë³´ > ë°”ìœ„

    Consider policies for iterated ê°€ìœ„ë°”ìœ„ë³´
      - A deterministic policy is easily exploited
      - A uniform random policy is optimal

  Example2 : Aliased Gridworld
    The agent cannot differentiate the grey states
    Consider features of the following form
      ğœ™(s,a) = I(wall to N, a = move E)
      Featureê°€ ì™„ë²½í•˜ì§€ ì•Šì•„ì„œ POMDP ë¬¸ì œì¸ ê²ƒ
      íšŒìƒ‰ state 2ê°œì—ì„œ ë˜‘ê°™ì€ featurë¥¼ ì£¼ê¸° ë•Œë¬¸ì—, ì™¼ìª½ íšŒìƒ‰ë²½ì—ì„œëŠ” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°€ì•¼í•˜ê³  ì˜¤ë¥¸ìª½ íšŒìƒ‰ë²½ì—ì„œëŠ” ì™¼ìª½ì—ì„œ ê°€ì•¼í•˜ëŠ” ë¬¸ì œì¸ ê²ƒì´ë‹¤.

    Compare value-based RL, using an approximate value function
      Qğœƒ(s,a) = f(ğœ™(s,a,),ğœƒ)

    To policy-based RL, suing parametrised policy
      ğœ‹ğœƒ(s,a) = g(ğœ™(s,a,),ğœƒ)

    An optimal stochastic policy will randomly move E or W in grey states
      ğœ‹ğœƒ(wall to N and S, move E) = 0.5
      ğœ‹ğœƒ(wall to N and S, move W) = 0.5

    It will reach the goal state in a few steps with high probability
    Policy-based RL can learn the optimal stochastic policy

    ì´ëŸ¬í•œ ë¬¸ì œì—ì„œëŠ” íšŒìƒ‰ë²½ì¼ë•Œ ë°˜ë°˜ìœ¼ë¡œ ì›€ì§ì—¬ì£¼ëŠ” ì •ì±…ì´ ì¢‹ë‹¤.
    Value based RLì˜ ê²½ìš° deterministic policyë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ì´ê²Œ ë¶ˆê°€ëŠ¥

    ê·¸ëŸ°ë°, deterministicí•œ policyë¥¼ ì™œ ëª» ì“¸ê¹Œ? ê·¸ ì´ìœ ëŠ” Markov propertyë¥¼ ë§Œì¡±í•˜ëŠ” fully observable MDPì—ì„œëŠ” ìµœì ì˜ deterministic policyê°€ ë¶„ëª…íˆ ì¡´ì¬í•˜ì§€ë§Œ, ì´ ë¬¸ì œê°™ì€ Partially observable MDPë¬¸ì œì˜ ê²½ìš°ëŠ” ê·¸ë ‡ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

#### Policy Objective Functions

  Goal : given policy ğœ‹ğœƒ(s,a) with parameters ğœƒ, find best ğœƒ
  But how do we measure the quality of a policy ğœ‹ğœƒ?

  In episodic environments we can use the start value
    J1(ğœƒ) = V^ğœ‹ğœƒ(s1) = E_ğœ‹ğœƒ[v1]
    (ì²« ë²ˆì§¸ stateì—ì„œ value functionì„ policyì˜ ëª©ì í•¨ìˆ˜ë¡œ ì¡ì!)

  In continuing environments we can use the average value
    JavV(ğœƒ) = Î£ d^ğœ‹ğœƒ(s) * V^ğœ‹ğœƒ(s)

  Or the average reward per time-step
    JavR(ğœƒ) = Î£ d^ğœ‹ğœƒ(s) Î£ ğœ‹ğœƒ(s,a) * R(s,a)

  d^ğœ‹ğœƒ(s)ëŠ” Markov chain ğœ‹ì˜ stationary distributionì´ë‹¤.
  Policy ğœ‹ë¥¼ ë”°ë¼ ê³„ì† í–‰ë™í•˜ë‹¤ë³´ë©´ì€ ê° ìƒíƒœì— ë¨¸ë¬´ë¥´ëŠ” í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.
  ê·¸ê²ƒì´ d^ğœ‹ğœƒ(s)ì´ë‹¤.

#### Policy Optimisation

  Policy based reinforcement learning is an optimisation problem
  Find ğœƒ that maximises J(ğœƒ)
  Some approaches do not use gradient
    Hill climbing
    Simplex / amoeba / Nelder Mead
    Genetic algorithms

  Greater efficiency often possible using gradient
    Gradient descent
    Conjugate gradient
    Quasi-newton

  We focus on graidnet descent, many extensions possible
  And on methods that exploit sequential structure

#### Policy Gradient
  Let J(ğœƒ) be any policy objective function
  Policy gradient algorithms search for a local maximum in J(ğœƒ) by ascending the gradient of the policy, w.r.t. parameters ğœƒ
    âˆ†ğœƒ = ğ›¼âˆ‡ğœƒ J(ğœƒ)

  Where âˆ‡ğœƒ J(ğœƒ) is the policy gradient
    âˆ‡ğœƒ J(ğœƒ) = (ğœ•J(ğœƒ)/ğœ•ğœƒ1 , ğœ•J(ğœƒ)/ğœ•ğœƒ2, ... , ğœ•J(ğœƒ)/ğœ•ğœƒn)'

  and ğ›¼ is a step-size parameter

  Jì— ëŒ€í•œ gradientë¡œ ğœƒë¥¼ ì—…ë°ì´íŠ¸!

#### Computing gradients by finite difference

  To evaluate policy gradient of ğœ‹ğœƒ(s,a)
  For each dimension k in [1,n]
    Estimating kth partial derivative of objective function w.r.t ğœƒ
    By perturbing ğœƒ by small amount e in kth dimension
      ğœ•J(ğœƒ) / ğœ•ğœƒk â‰ˆ {J(ğœƒ+euk) - J(ğœƒ)} / e
      where uk is unit vector with 1 in kth component, 0 elsewhere

  Uses n evaluations to compute policy gradient in n dimensions
  Simple, noisy, inefficient - but sometimes effective
  Works for arbitrary policies, even if policy is not differentiable

  ğœƒ1, ğœƒ2, ... , ğœƒnì— ëŒ€í•´ finite differenceë¥¼ í†µí•´ J ì˜ gradientë¥¼ êµ¬í•˜ëŠ” ê²ƒ.

  ì‹¤ì œ ì‚¬ìš©ì˜ˆ : Training AIBO to walk by finite difference policy gradient

#### Score Function

  We now compute the policy gradient analytically
  Assume policy ğœ‹ğœƒ is differentiable whenever it is non-zero
  and we know the gradient âˆ‡ğœƒ ğœ‹ğœƒ(s,a)
  Likelihood ratios exploit the following identity
    âˆ‡ğœƒ ğœ‹ğœƒ(s,a) = ğœ‹ğœƒ(s,a) * âˆ‡ğœƒ ğœ‹ğœƒ(s,a) / ğœ‹ğœƒ(s,a)
               = ğœ‹ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a))

  The score function is âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a))

  ì´ëŸ° ê²ƒì„ Likelihood ratio trickì´ë¼ê³  í•œë‹¤. ì´ê±¸ ì™œ í•˜ëŠ”ì§€ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤.

#### Softmax Policy

  We will use a softmax policy as a running example
  Weight actions using linear combination of features ğœ™(s,a)^Tğœƒ
  Probability of action is proportional to exponentiated weight
    ğœ‹ğœƒ(s,a) âˆ e^ğœ™(s,a)^T * ğœƒ

  The score function is
    âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) = ğœ™(s,a) - E_ğœ‹ğœƒ[ğœ™(s,.)]

  ê³„ì‚°ì‹ì´ ìƒëµë˜ì—ˆê¸´ í•˜ì§€ë§Œ, softmaxë¡œ policyë¥¼ ì •í•´ì£¼ë©´ score functionì„ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ

#### Gaussian Policy
  In continuous action spaces, a Gaussian policy is natural
  Mean is a linear combination of state features mu(s) = ğœ™(s,a)^T * ğœƒ
  Variance may be fixed ğœ^2, or can also parameterised
  Policy is Gaussian, a ~ N(mu(s) , ğœ^2)

  The score function is
    âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) = (a-mu(s)) * ğœ™(s) / ğœ^2

  ê³„ì‚°ì‹ì´ ìƒëµë˜ì—ˆê¸´ í•˜ì§€ë§Œ, Gaussian policyë¥¼ ì‚¬ìš©í•˜ë©´ ìœ„ì™€ ê°™ì€ score functionì„ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ

#### One-Step MDP

  Consider a simple class of one-step MDPs
    Starting in states s~d(s)
    Terminating after one time-step with reward r = R(s,a)

  Use likelihood ratios to compute the policy gradient
    J(ğœƒ) = E_ğœ‹ğœƒ[r]
         = Î£ d^ğœ‹ğœƒ(s) Î£ ğœ‹ğœƒ(s,a) * R(s,a)

    âˆ‡ğœƒ J(ğœƒ) = Î£ d^ğœ‹ğœƒ(s) Î£ ğœ‹ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * R(s,a)
            E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * r]

  => ì•„ì£¼ ì¤‘ìš”!! J(ğœƒ)ì˜ gradientë¥¼ ê¸°ëŒ“ê°’ í˜•íƒœë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤!
  => Samplingì„ í†µí•´ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ (Likelihood ratio trickì¢‹ì§€?)

#### Policy Gradient Theorem

  The policy gradient theorem generalises the likelihood ratio appraoch to multi-step MDPs
  Replaces instantaneous reward r with long-term value Q^ğœ‹(s,a)
  Policy gradient theorem applies to start state objective, average reward and average value objective

  Theorem
    For any differentiable policy ğœ‹ğœƒ(s,a), for any of the policy objective functions J=J1, J_avR, or 1/(1-ğ›¾)JavV, the policy gradient is
      âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * Q^ğœ‹ğœƒ(s,a)]

#### Monte-Carlo Policy Gradient (REINFORCE)
  Update parameters by stochastic gradient ascent
  Using policy gradient theorem
  Using return vt as an unbiased sample of Q^ğœ‹ğœƒ(st,at)
    âˆ†ğœƒt = ğ›¼ * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * vt

  Pseudo code

  function REINFORCE
    Initialise ğœƒ arbitrarily
    for each episode {s1, a1, r2, ... , s(T-1), a(T-1), rT} ~ ğœ‹ğœƒ do
      for t = 1 to T-1 do
       ğœƒ <- ğœƒ + ğ›¼ * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * vt
      end for
    end for
    return ğœƒ
  end function

  Monte-Cralo ê¸°ë²•ì„ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— Q ê°’ ëŒ€ì‹  returenì„ ì“°ê³  ìˆë‹¤.

  Puck World Example
    Continuous actions exert small force on puck
    Puck is rewarded for getting close to target
    Target location is reset every 30 secodnds
    Policy is trained using variant of Monte-Carlo policy gradient

  => ì£¼ì˜í•´ì„œ ë´ì•¼í•  ì 1 : í•™ìŠµ ê³¡ì„ ì´ ì§€ê·¸ì¬ê·¸ê°€ ì•„ë‹ˆë¼ ë§¤ë„ëŸ½ê²Œ ì˜¬ë¼ê°„ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤ ì¦‰, stable í•˜ë‹¤.

  => ì£¼ì˜í•´ì„œ ë´ì•¼í•  ì 2 : varianceê°€ ë„ˆë¬´ ì»¤ì„œ, iterationì´ 9*10^7 ì •ë„ ë˜ì•¼ ìˆ˜ë ´ì´ ëœë‹¤. ì¦‰, í•™ìŠµì´ ëŠë¦¬ë‹¤.

### Actor - Critic Policy Gradient

#### Reducing Variance Using a Critic

  Monte-Carlo policy gradient (REINFORCE) still has high variance
  We use a critic to estimate the action-value function,
    Qw(s,a) â‰ˆ Q^ğœ‹ğœƒ(s,a)

  Actor-Critic algorithms maintain two sets of parameters
    Critic : Updates action-value function parameters w
    Actor  : Updates policy parameters ğœƒ, in direction suggested by critic

  Actor-Critic algorithms follow an approximate policy gradient
    âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * Q^w(s,a)]
         âˆ†ğœƒ = = ğ›¼ * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * Q^w(s,a)

  => ìš°ë¦¬ ì§€ê¸ˆê¹Œì§€ Që¥¼ ê·¼ì‚¬í•´ì„œ êµ¬í–ˆì–ì•„ ê·¸ê±° ì“°ë©´ ì•ˆë¼?
  => ì˜ ë˜ë”ë¼. ê·¸ë˜ì„œ Q ê°’ì„ criticìœ¼ë¡œ í•™ìŠµí•œë‹¤. QëŠ” wë¼ëŠ” parameterë¡œ í‘œí˜„ ë° í•™ìŠµí•œë‹¤.
  => PolicyëŠ” REINFORCEì²˜ëŸ¼ ğœƒë¼ëŠ” parameterë¡œ í•™ìŠµí•œë‹¤.

  => Policy iterationì˜ ëŠë‚Œì´ ìˆì§€? Criticì´ í‰ê°€í•˜ê³  Actorê°€ í–‰ë™í•˜ê³ ã…ã…ã… ì¬ë°Œêµ°

#### Estimating the Action-Value Function

  The critic is solving a familiar problem : policy evaluation
  How good is policy ğœ‹ğœƒ for current parameters ğœƒ?
  This problem was explored in previous two lectures, e.g.
    Monte-Carlo policy evaluation
    Temporal - Difference learning
    TD(ğœ†)

  Could also use e.g. least-squares policy evaluation

#### Action-Value Actor-Critic

  Simple actor-critic algorithm based on action-value critic
  Using linear value function approximation. Qw(s,a) = ğœ™(s,a)^T w
    Critic Updates w by linear TD(0)
    Actor  Updates ğœƒ by policy gradient

  Pseudo code

  function QAC
    Initialise s, ğœƒ
    Sample a ~ ğœ‹ğœƒ
    for each step do
      Sample reward r = R(s,a)
      Sample transition s' ~ P(s,a)
      Sample action a' ~ ğœ‹ğœƒ(s',a')

      ğ›¿ = r + ğ›¾ âˆ— Qw(s',a') - Qw(s,a)
      ğœƒ <- ğœƒ + ğ›¼ * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * Qw(s,a)
      (ğœƒ <- ğœƒ + ğ›¼ * âˆ‡ğœƒ J(ğœƒ)ì™€ ê°™ì€ ë§)
      w <- w + Î² * ğ›¿ * ğœ™(s,a)
      a <- a', s <- s'
    end for
  end function

#### Bias in Actor-Critic algorithm

  Approximating the policy gradient introduces bias
    => Policy gradientë¥¼ ê·¼ì‚¬í•˜ê²Œ ë˜ë©´ biasê°€ ë°œìƒí•œë‹¤.

  A biased policy gradient may not find the right solution
    e.g. if Qw(s,a) uses aliased features, can we solve gridworld example?

  Luckily, if we choose value function approximation carefully
  Then we can avoid introducing any bias
  i.e. We can still follow the exact policy gradient

#### Compatible function approximation Theorem

  If the following two conditions are satisfied:
   1. Value function approximator is compatible to the policy
      âˆ‡w Qw(s,a) = âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a))

   2. Value function parameters w minimise the mean-squared error
      e = E_ğœ‹ğœƒ[(Q^ğœ‹ğœƒ(s,a) - Qw(s,a))^2]

  Then the policy gradient is exact,
      âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * Q^w(s,a)]

#### Proof of compatible function approximation theorem

  If w is chosen to minimise mean-squared error, gradient of e w.r.t. w must be zero,

  âˆ‡w e = 0
    E_ğœ‹ğœƒ[(Q^ğœ‹ğœƒ(s,a) - Qw(s,a)) * âˆ‡w Qw(s,a)] = 0

  ìœ„ì˜ ì¡°ê±´ 1ì— ì˜í•´,
    E_ğœ‹ğœƒ[(Q^ğœ‹ğœƒ(s,a) - Qw(s,a)) * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a))] = 0

  ì¦‰,
    E_ğœ‹ğœƒ[Q^ğœ‹ğœƒ(s,a) * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)] = E_ğœ‹ğœƒ[Qw(s,a) * âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)]

  ë”°ë¼ì„œ, Qw(s,a)ëŠ” Q^ğœ‹ğœƒ(s,a)ë¥¼ ëŒ€ì‹ í•´ì„œ policy gradientì— ì§ì ‘ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
    âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * Q^w(s,a)]

#### Reducing Variance Using a Baseline

  We subtract a baseline function B(s) from the policy gradient
  This can reduce variance, without changing expectation
    E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * B(s)] = {s} Î£ d^ğœ‹ğœƒ(s) {a} Î£ âˆ‡ğœƒ ğœ‹ğœƒ(s,a) * B(s)
                                 = {s} Î£ d^ğœ‹ğœƒ(s) * B(s) âˆ‡ğœƒ {a} Î£ ğœ‹ğœƒ(s,a)
                                   ({a}Î£ ğœ‹ğœƒ(s,a) = 1 => âˆ‡ğœƒ {a} Î£ ğœ‹ğœƒ(s,a) = 0)
                                 = 0!!!

    E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a))*Qw(s,a)] = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a))*(Qw(s,a)-B(s)]

  A good baseline is the state value function B(s) = V^ğœ‹ğœƒ(s)
  So we can rewrite the policy gradient using the advantage function A^ğœ‹ğœƒ(s,a)
    A^ğœ‹ğœƒ(s,a) = Q^ğœ‹ğœƒ(s,a) - V^ğœ‹ğœƒ(s)
    âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * A^ğœ‹ğœƒ(s,a)]

  Baselineì„ ì“°ëŠ” ì´ìœ ëŠ”, Gradientê°€ 100ë§Œ, 99ë§Œ ë§‰ ì´ëŸ´ë•Œ ê·¸ ê°’ë“¤ë¡œ í•™ìŠµì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼!
  100ë§Œ-95ë§Œ = 5ë§Œ, 99ë§Œ-95ë§Œ = 4ë§Œ ì´ëŸ° ê°’ë“¤ë¡œ í•™ìŠµì„ í•˜ì—¬ varianceë¥¼ ì¤„ì´ê³  ì‹¶ë‹¤ëŠ” ê²ƒ..??
  ê·¸ëŸ¬ë‹ˆê¹Œ ìƒëŒ€ì ì¸ ì°¨ì´ë¥¼ ê°€ì§€ê³  í•™ìŠµì„ í•˜ê³  ì‹¶ë‹¤ëŠ” ê±°ì§€ã…‡ã…‡
  Control variate ëŠë‚Œì¸ë° ë­”ê°€ control variateë³´ë‹¤ ì‚´ì§ ë¶€ì¡±í•œ ëŠë‚Œì´ë„¤

#### Estimating the Advantage Function

  The advantage function can significantly reduce variance of policy gradient
  So the critic should really estimate the advantage function
  For example, by estimating both V^ğœ‹ğœƒ and Q^ğœ‹ğœƒ(s,a)
  Using two function approximators and two parameter vectors,
    Vv(s)   â‰ˆ V^ğœ‹ğœƒ
    Qw(s,a) â‰ˆ Q^ğœ‹ğœƒ(s,a)
    A(s,a)  = Qw(s,a) - Vv(s)

  And updating both value functions by e.g. TD learning

  => Advantage functionì„ ì´ìš©í•˜ë©´ Varianceê°€ ì—„ì²­ë‚˜ê²Œ reductionëœë‹¤. í•˜ì§€ë§Œ, Vì™€ Q ëª¨ë‘ estimationí•´ì•¼ í•œë‹¤.
  => ê·¸ë˜ì„œ VëŠ” vë¼ëŠ” parameterë¥¼ ê°€ì§€ê³  TD learningë“±ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³ 
  => Qì™€ policyëŠ” ìœ„ì— ì–¸ê¸‰í•œ ê²ƒ ì²˜ëŸ¼ ì§„í–‰í•˜ë©´ ëœë‹¤.
  => í•˜ì§€ë§Œ ê·¸ëŸ¬ë©´ parameterê°€ 3ê°œì§€?? ê·¸ë˜ì„œ ì²œì¬ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì´ í•´ê²°í–ˆë‹¤.

  For the true value function V^ğœ‹ğœƒ, the TD error ğ›¿^ğœ‹ğœƒ
    ğ›¿^ğœ‹ğœƒ = r + ğ›¾ * V^ğœ‹ğœƒ(s') - V^ğœ‹ğœƒ(s)

  is an unbiased estimate of the advantage function (ìœ„ì˜ ì‹ì— expectationí•´ë³´ë©´)
    E_ğœ‹ğœƒ[ğ›¿^ğœ‹ğœƒ|s, a] = E_ğœ‹ğœƒ[r + ğ›¾ * V^ğœ‹ğœƒ(s')|s, a] - V^ğœ‹ğœƒ(s)
                    = Q^ğœ‹ğœƒ(s,a) - V^ğœ‹ğœƒ(s)
                    = A^ğœ‹ğœƒ(s,a)

  => State = s, Action = a ì¼ë•Œ r + ğ›¾ * V^ğœ‹ğœƒ(s')ì˜ ê¸°ëŒ“ê°’ì€ Q(s,a)ì´ë‹¤.
  => ì¦‰, s,aì— ëŒ€í•œ TD errorì˜ ê¸°ëŒ“ê°’ì€ Advantage functionì´ ëœë‹¤ëŠ” ê²ƒì´ë‹¤.

  So we can use the TD error to compute the policy gradient
    âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * A^ğœ‹ğœƒ(s,a)] ?
    âˆ‡ğœƒ J(ğœƒ) = E_ğœ‹ğœƒ[âˆ‡ğœƒ log(ğœ‹ğœƒ(s,a)) * ğ›¿^ğœ‹ğœƒ] !

  =>ê·¸ë ‡ê¸° ë•Œë¬¸ì— policy gradientë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ  advantage functionì˜ ìë¦¬ì— TD errorë¥¼ ì‚¬ìš©í•´ë„ ëœë‹¤.

  In practice we can use an approximate TD error
    ğ›¿_v = r + ğ›¾ * Vv(s') - Vv(s)

  =>í•˜ì§€ë§Œ, ì‹¤ì œ true valueëŠ” ì•Œê¸° ì‰½ì§€ ì•Šê³ , vë¼ëŠ” parameterë¥¼ ì´ìš©í•´ ê·¼ì‚¬í•œ Vv(s)ë¥¼ ì‚¬ìš©í•´ë„ ëœë‹¤.

  This approach only requires one set of critic parameters v !!

  => Qê°’ì„ ê·¼ì‚¬í•  í•„ìš”ê°€ ì—†ë‹¤ëŠ” í° ì¥ì ì´ ìˆë‹¤.

#### Critics at Different Time-scales

  Critic can estimate value function Vğœƒ(s) from many targets at different time-scales from last lectur...
    For MC,

    For TD(0),

    For forward-veiw TD(),

    For backward-view TD(),

#### Actors at Different Time-scales

  The policy gradient can also be estimated at many time-scales

#### Alternator ~~~

#### Natural ~~~

#### Summary of Policy Gradient Algorithms

  The policy gradient has many equivalent forms

  REINFORCE : return vtë¥¼ ì‚¬ìš©í–ˆì—ˆë‹¤.
  Q Actor-Critic : returnì€ varianceê°€ ë„ˆë¬´ ë†’ì•„ì„œ Që¥¼ ì‚¬ìš©í–ˆì—ˆë‹¤.
  Advantage Actor-Critic : ê·¸ë˜ë„ varianceê°€ ë†’ì•„ì„œ Advantage functionì„ ì‚¬ìš©í–ˆì—ˆë‹¤.
  TD Actor-Critic : TD errorì˜ expectationì´ Advantage functionì´ê¸° ë•Œë¬¸ì— TD errorë¥¼ ì‚¬ìš©
  TD(lambda) Actor-Critic : í•œ stepë§Œ ë³´ëŠ”ê²Œ ì•„ë‹ˆë¼ ì—¬ëŸ¬ stepì„ ë³´ëŠ” TD(lambda)ë¥¼ ì‚¬ìš©

  Each leads a stochastic gradient ascent algorithm
  Critic uses policy evaluation (e.g. MC or TD learning)
  to estimate Q(s,a), A(s,a) or V(s)
