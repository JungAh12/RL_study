# RL-study-2019
8ì¥ ê¹Œì§€ëŠ” MDPë¥¼ ì•Œ ë•Œ dynamic programmingì„ í†µí•´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ë„ ë°°ì› ê³ , Model - freeë¡œ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œë„ ë°°ì› ì—ˆë‹¤.

ê·¸ë˜ì„œ Predictionë¬¸ì œì™€ Control ë¬¸ì œë¥¼ í‘¸ëŠ” ë²•ì— ëŒ€í•´ì„œ ë‹¤ë£¨ì–´ ë³´ì•˜ë‹¤.

ë˜í•œ 6ê°•ì—ì„œ Function approximationì„ í†µí•´ ë¬¸ì œë¥¼ scale-upí•˜ê¸°ë„ í•´ë´¤ë‹¤ã…ã…

8ì¥ì€ ëª¨ë¸ì„ ë§Œë“¤ê³ , ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°•í™”í•™ìŠµì„ í•˜ëŠ” ë¬¸ì œ! modelì„ ì•Œ ë•Œ í‘¸ëŠ” ê²ƒì€ planningì´ê³  modelì„ ëª¨ë¥¼ ë•Œ í‘¸ëŠ” ê²ƒì€ learningì´ë¼ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì‹œê³ ~~

ì´ë²ˆ ì¥ì—ì„œëŠ” planningê³¼ learningì„ ì„ëŠ” ë²•ì— ëŒ€í•´ì„œë„ ì¡°ê¸ˆ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤.

### Integrating Learning and Planning
ë°”ë¡œ ì§€ë‚œ ê°•ì˜ì—ì„œëŠ” ê²½í—˜ìœ¼ë¡œë¶€í„° policyë¥¼ ì§ì ‘ í•™ìŠµí–ˆë‹¤.
ê·¸ ì´ì „ì˜ ê°•ì˜ì—ì„œëŠ” value functionì„ ê²½í—˜ìœ¼ë¡œ ë¶€í„° ì§ì ‘ í•™ìŠµ í–ˆë‹¤.
ì´! ë²ˆ! ì—”! ê²½í—˜ë“¤ë¡œ ë¶€í„° modelì„ ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì— ëŒ€í•´ì„œ ë‹¤ë£¬ë‹¤.

ê·¸ í›„ì— ëª¨ë¸ì„ í†µí•´ì„œ value functionê³¼ policyë¥¼ constructingí•œë‹¤.
ë˜í•œ, learningê³¼ planningì„ single architectureì— integrateí•˜ëŠ” ê²ƒì„ ë°°ìš´ë‹¤.

#### Model-Free RL and Model-Based RL
Model - Free RL
  Modelì´ ì—†ë‹¤! ë˜í•œ ê²½í—˜ìœ¼ë¡œ ë¶€í„° Value functionì„ learningí•œë‹¤!

Model - Based RL
  ê²½í—˜ìœ¼ë¡œ ë¶€í„° modelì„ ë°°ìš´ë‹¤
  modelë¡œ ë¶€í„° value function í˜¹ì€ policy functionì„ planningí•œë‹¤!

#### Model-Based RL Reinforcement Learning
1. Model - based RLì´ë€ policyê°€ actionì„ í–ˆì„ ë•Œ ìƒê¸°ëŠ” ê²½í—˜ì„ í†µí•´ì„œ
2. modelì„ í•™ìŠµí•˜ê³ 
3. Planningì„ í†µí•´ì„œ valueì™€ policyë¥¼(MDPë¥¼) learní•˜ê³  solve í•˜ëŠ” ê²ƒì´ë‹¤.

Advantages:
  Can efficiently learn model by supervised learning methods
  Can reason about model uncertainty
  1. spuervised learningì„ í†µí•´ì„œ modelì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¯¸ë¦¬ í•™ìŠµ ì‹œí‚¬ ìˆ˜ ìˆë‹¤.
  2. ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë‹¤ë£° ìˆ˜ ìˆë‹¤.

Disadvantages:
  First learn a model, then construct a value function
    => two suorces of approximation eror
    => í‹€ë¦´ ìˆ˜ ìˆëŠ” ê³³ì´ ë‘ êµ°ë°ë‚˜ ìƒê¸´ë‹¤. ëª¨ë¸ì„ ì˜ ëª» í•™ìŠµí•˜ë˜ê°€ valueë¥¼ ì˜ëª» í•™ìŠµí•˜ë˜ê°€ ê·¸ëŸ´ ìˆ˜ ìˆë‹¤.

What is model?
  A model M is a representation of an MDP <S, A, P, R>, parameterized by n
  We will assume state space S and action space A are known
  So a model M = <Pn, Rn> represents state transitions Pn â‰ˆ P and rewards Rn â‰ˆ R
    S_(t+1) â‰ˆ Pn(S_(t+1) | S_t, A_t)
    R_(t+1) = Rn(R_(t+1) | S_t, A_t)

  Typically assume conditional independence between state transitions and rewards
    P[S_(t+1), R_(t+1) | S_t, A_t] = P[S_(t+1) | S_t, A_t] * P[R_(t+1) | S_t, A_t]

Model Learning
  Goal : estimate model Mn from experience {S1, A1, R2, ... , ST}
  This is a supervised learning problem
        S1, A1 => R2, S2
        S2, A2 => R3, S3
                .
                .
                .
  S_(T-1), A_(T-1) => RT, ST

  Learning s, a -> r  is a regression problem
  Learning s, a -> s' is a density estimation problem
  Pick loss function, e.g. mean-suqared error, KL divergence and so on
  Find parameters n that minimise empirical loss

Examples of Models
  Table Lookup Model
  Linear Expectation Model
  Linear Gaussian Model
  Gaussian Process Model
  Deep Belief Network Model

Table Lookup Model
  Model is an explicit MDP, P_hat, R_hat
  ê° state action pairë¥¼ ë°©ë¬¸í–ˆì„ ë•Œ ë§ˆë‹¤ N(s,a)ë¥¼ êµ¬í•˜ê³ 
  P_hat(a,ss')ê³¼ R_hat(s,a)ë¥¼ í‰ê· ì„ í†µí•´ ì •í•´ì£¼ëŠ” ê²ƒ

  Alternatively
    ê° time-step tì—ì„œ experience tupleì„ ê¸°ë¡ <St, At, R_(t+1), S_(t+1)>
    modelì„ sampleí•˜ê¸° ìœ„í•´ tupleì„ randomí•˜ê²Œ ì„ íƒ

#### Planning with a Model
  Given a model Mn = <Pn, Rn>
  Solve the MDP <S, A, Pn, Rn>
  Using favourite planning algorithm
    Value iteration
    Policy iteration
    Tree search
    ....

  ê°€ìƒì˜ MDPë¥¼ ì•„ëŠ” ìƒí™©ì´ë¯€ë¡œ VI, PIë¥¼ ì ìš© ê°€ëŠ¥

Sample - Based Planning
  A simple but powerful approach to planning
  Use the model only to generate samples
  Sample experience from model
    S_(t+1) ~ Pn(S_(t+1) | S_t, A_t)
    R_(t+1) = Rn(R_(t+1) | S_t, A_t)

  Apply model-free RL to samples e.g. :
    Monte-Carlo control
    Sarsa
    Q-learning

  Sample-based planning methods are often more efficient

  => Sampleë“¤ì„ modelì„ ê¸°ë°˜ìœ¼ë¡œ ë§ˆêµ¬ ë§Œë“¤ì–´ë‚¸ ë‹¤ìŒ model-free RL ê¸°ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ
  => DPëŠ” ìˆœì§„í•˜ê²Œ ì „ë¶€ ë‹¤ back-upí•˜ëŠ” ë°˜ë©´, Sample based planningì€ ìì£¼ ì¼ì–´ë‚˜ëŠ”
  ì‚¬ê±´ì— ë” ì§‘ì¤‘ì ìœ¼ë¡œ planningì„ í•  ìˆ˜ ìˆê³ , curse of dimensionalityë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

=> Model ì„ ì•Œ ê²½ìš°, Modelì„ í†µí•´ì„œ ë¬´í•œí•œ ê²½í—˜ë“¤ì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤. ê·¸ë¥¼ í†µí•´
Model - free RLì„ ì§„í–‰í•˜ë©´ ì•„ì£¼ íš¨ê³¼ê°€ ì¢‹ì•„ ë²„ë¦°ë‹¤.

Planning with an inaccurate model
  Given an imperfect model <Pn, Rn> â‰  <P, R>
  Performance of model-based RL is limited to optimal policy for approximate MDP <S,A,Pn,Rn>
  i.e. Model-based RL is only as good as estimated model
  When the model is inaccurate, planning process will compute a suboptimal policy
  Solution 1
    When model is wrong, use model-free RL
  Solution 2
    reason explicitly about model uncertainty

  => modelì´ ì¢‹ì€ ë§Œ í¼ model-based RLì˜ íš¨ê³¼ê°€ ì¢‹ì„ ê²ƒì´ë‹¤.
  => modelì´ í‹€ë¦´ ê²½ìš°, modelì„ ì•ˆì“°ê³  model-free RLì„ ì“¸ ìˆ˜ ìˆì„ê±°ê³ 
  => modelì˜ uncertaintyë¥¼ í‘œí˜„í•´ ì¤Œìœ¼ë¡œì¨ modelì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
    ì˜ˆë¥¼ ë“¤ì–´ modelì´ 30ì„ ì¶œë ¥í•œë‹¤ ë¼ëŠ” ê²ƒì„ modelì´ 20~40ì‚¬ì´ì—ì„œ ì¶œë ¥í•œë‹¤. ë¼ëŠ” ì‹

***************

### Integrated Architecture

Real and Simulated Experience
  we consider two sources of experience

  Real experience - Sampled from environment (true MDP)
    S' ~ P^a_ss'
    R  = R^a_s

  Simulated experience - Sampled from model (approximate MDP)
    S' ~ Pn(S'| S,A)
    R  = Rn(R | S,A)

Integrating Learning and Planning
  Model - Free RL
    No model
    Learn value function (and/or policy) from real experience

  Model - Based RL
    Learn a model from real experience
    Plan value function (and/or policy) from simulated experience

  Dyna
    Learn a model from experience
    Learn and plan value function (and/or policy) from real and simulated experience

Dyna-Q Algorithm
  Initialize Q(s,a) and Model(s,a) for all s and a
  Do forever:
    (a) S <- current (nonterminal) state
    (b) A <- e-greedy(S,Q)
    (c) Execute action A; observe resultant reward, R, and state, S'
    (d) Q(S,A) <- Q(S,A) + ğ›¼ * [R + gamma * max{Q(S',A)} - Q(S,A)]
    (e) Model(S,A) <- R, S' (assuming deterministic environment)
    (f) Repeat n times :
        S <- random previously observed state
        A <- random action previously taken in S
        R, S' <- Model(S,A)
        Q(S,A) <- Q(S,A) + ğ›¼ * [R + gamma * max{Q(S',A)} - Q(S,A)]
















ã…‡
