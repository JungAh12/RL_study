# RL-study-2019
8ì¥ ê¹Œì§€ëŠ” MDPë¥¼ ì•Œ ë•Œ dynamic programmingì„ í†µí•´ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ë„ ë°°ì› ê³ , Model - freeë¡œ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œë„ ë°°ì› ì—ˆë‹¤.

ê·¸ë˜ì„œ Predictionë¬¸ì œì™€ Control ë¬¸ì œë¥¼ í‘¸ëŠ” ë²•ì— ëŒ€í•´ì„œ ë‹¤ë£¨ì–´ ë³´ì•˜ë‹¤.

ë˜í•œ 6ê°•ì—ì„œ Function approximationì„ í†µí•´ ë¬¸ì œë¥¼ scale-upí•˜ê¸°ë„ í•´ë´¤ë‹¤ã…ã…

8ì¥ì€ ëª¨ë¸ì„ ë§Œë“¤ê³ , ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°•í™”í•™ìŠµì„ í•˜ëŠ” ë¬¸ì œ! modelì„ ì•Œ ë•Œ í‘¸ëŠ” ê²ƒì€ planningì´ê³  modelì„ ëª¨ë¥¼ ë•Œ í‘¸ëŠ” ê²ƒì€ learningì´ë¼ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì‹œê³ ~~

ì´ë²ˆ ì¥ì—ì„œëŠ” planningê³¼ learningì„ ì„ëŠ” ë²•ì— ëŒ€í•´ì„œë„ ì¡°ê¸ˆ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤.

### Integrating Learning and Planning
ë°”ë¡œ ì§€ë‚œ ê°•ì˜ì—ì„œëŠ” ê²½í—˜ìœ¼ë¡œë¶€í„° policyë¥¼ ì§ì ‘ í•™ìŠµí–ˆë‹¤.
ê·¸ ì´ì „ì˜ ê°•ì˜ì—ì„œëŠ” value functionì„ ê²½í—˜ìœ¼ë¡œ ë¶€í„° ì§ì ‘ í•™ìŠµ í–ˆë‹¤.
ì´! ë²ˆ! ì—”! ê²½í—˜ë“¤ë¡œ ë¶€í„° modelì„ ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì— ëŒ€í•´ì„œ ë‹¤ë£¬ë‹¤.

ê·¸ í›„ì— ëª¨ë¸ì„ í†µí•´ì„œ value functionê³¼ policyë¥¼ constructingí•œë‹¤.
ë˜í•œ, learningê³¼ planningì„ single architectureì— integrateí•˜ëŠ” ê²ƒì„ ë°°ìš´ë‹¤.

#### Model-Free RL and Model-Based RL
Model - Free RL
  Modelì´ ì—†ë‹¤! ë˜í•œ ê²½í—˜ìœ¼ë¡œ ë¶€í„° Value functionì„ learningí•œë‹¤!

Model - Based RL
  ê²½í—˜ìœ¼ë¡œ ë¶€í„° modelì„ ë°°ìš´ë‹¤
  modelë¡œ ë¶€í„° value function í˜¹ì€ policy functionì„ planningí•œë‹¤!

#### Model-Based RL Reinforcement Learning
1. Model - based RLì´ë€ policyê°€ actionì„ í–ˆì„ ë•Œ ìƒê¸°ëŠ” ê²½í—˜ì„ í†µí•´ì„œ
2. modelì„ í•™ìŠµí•˜ê³ 
3. Planningì„ í†µí•´ì„œ valueì™€ policyë¥¼(MDPë¥¼) learní•˜ê³  solve í•˜ëŠ” ê²ƒì´ë‹¤.

Advantages:
  Can efficiently learn model by supervised learning methods
  Can reason about model uncertainty
  1. spuervised learningì„ í†µí•´ì„œ modelì„ íš¨ìœ¨ì ìœ¼ë¡œ ë¯¸ë¦¬ í•™ìŠµ ì‹œí‚¬ ìˆ˜ ìˆë‹¤.
  2. ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë‹¤ë£° ìˆ˜ ìˆë‹¤.

Disadvantages:
  First learn a model, then construct a value function
    => two suorces of approximation eror
    => í‹€ë¦´ ìˆ˜ ìˆëŠ” ê³³ì´ ë‘ êµ°ë°ë‚˜ ìƒê¸´ë‹¤. ëª¨ë¸ì„ ì˜ ëª» í•™ìŠµí•˜ë˜ê°€ valueë¥¼ ì˜ëª» í•™ìŠµí•˜ë˜ê°€ ê·¸ëŸ´ ìˆ˜ ìˆë‹¤.

What is model?
  A model M is a representation of an MDP <S, A, P, R>, parameterized by n
  We will assume state space S and action space A are known
  So a model M = <Pn, Rn> represents state transitions Pn â‰ˆ P and rewards Rn â‰ˆ R
    S_(t+1) â‰ˆ Pn(S_(t+1) | S_t, A_t)
    R_(t+1) = Rn(R_(t+1) | S_t, A_t)

  Typically assume conditional independence between state transitions and rewards
    P[S_(t+1), R_(t+1) | S_t, A_t] = P[S_(t+1) | S_t, A_t] * P[R_(t+1) | S_t, A_t]

Model Learning
  Goal : estimate model Mn from experience {S1, A1, R2, ... , ST}
  This is a supervised learning problem
        S1, A1 => R2, S2
        S2, A2 => R3, S3
                .
                .
                .
  S_(T-1), A_(T-1) => RT, ST

  Learning s, a -> r  is a regression problem
  Learning s, a -> s' is a density estimation problem
  Pick loss function, e.g. mean-suqared error, KL divergence and so on
  Find parameters n that minimise empirical loss

Examples of Models
  Table Lookup Model
  Linear Expectation Model
  Linear Gaussian Model
  Gaussian Process Model
  Deep Belief Network Model

Table Lookup Model
  Model is an explicit MDP, P_hat, R_hat
  ê° state action pairë¥¼ ë°©ë¬¸í–ˆì„ ë•Œ ë§ˆë‹¤ N(s,a)ë¥¼ êµ¬í•˜ê³ 
  P_hat(a,ss')ê³¼ R_hat(s,a)ë¥¼ í‰ê· ì„ í†µí•´ ì •í•´ì£¼ëŠ” ê²ƒ

  Alternatively
    ê° time-step tì—ì„œ experience tupleì„ ê¸°ë¡ <St, At, R_(t+1), S_(t+1)>
    modelì„ sampleí•˜ê¸° ìœ„í•´ tupleì„ randomí•˜ê²Œ ì„ íƒ

#### Planning with a Model
  Given a model Mn = <Pn, Rn>
  Solve the MDP <S, A, Pn, Rn>
  Using favourite planning algorithm
    Value iteration
    Policy iteration
    Tree search
    ....

  ê°€ìƒì˜ MDPë¥¼ ì•„ëŠ” ìƒí™©ì´ë¯€ë¡œ VI, PIë¥¼ ì ìš© ê°€ëŠ¥

Sample - Based Planning
  A simple but powerful approach to planning
  Use the model only to generate samples
  Sample experience from model
    S_(t+1) ~ Pn(S_(t+1) | S_t, A_t)
    R_(t+1) = Rn(R_(t+1) | S_t, A_t)

  Apply model-free RL to samples e.g. :
    Monte-Carlo control
    Sarsa
    Q-learning

  Sample-based planning methods are often more efficient

  => Sampleë“¤ì„ modelì„ ê¸°ë°˜ìœ¼ë¡œ ë§ˆêµ¬ ë§Œë“¤ì–´ë‚¸ ë‹¤ìŒ model-free RL ê¸°ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ
  => DPëŠ” ìˆœì§„í•˜ê²Œ ì „ë¶€ ë‹¤ back-upí•˜ëŠ” ë°˜ë©´, Sample based planningì€ ìì£¼ ì¼ì–´ë‚˜ëŠ”
  ì‚¬ê±´ì— ë” ì§‘ì¤‘ì ìœ¼ë¡œ planningì„ í•  ìˆ˜ ìˆê³ , curse of dimensionalityë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

=> Model ì„ ì•Œ ê²½ìš°, Modelì„ í†µí•´ì„œ ë¬´í•œí•œ ê²½í—˜ë“¤ì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆë‹¤. ê·¸ë¥¼ í†µí•´
Model - free RLì„ ì§„í–‰í•˜ë©´ ì•„ì£¼ íš¨ê³¼ê°€ ì¢‹ì•„ ë²„ë¦°ë‹¤.

Planning with an inaccurate model
  Given an imperfect model <Pn, Rn> â‰  <P, R>
  Performance of model-based RL is limited to optimal policy for approximate MDP <S,A,Pn,Rn>
  i.e. Model-based RL is only as good as estimated model
  When the model is inaccurate, planning process will compute a suboptimal policy
  Solution 1
    When model is wrong, use model-free RL
  Solution 2
    reason explicitly about model uncertainty

  => modelì´ ì¢‹ì€ ë§Œ í¼ model-based RLì˜ íš¨ê³¼ê°€ ì¢‹ì„ ê²ƒì´ë‹¤.
  => modelì´ í‹€ë¦´ ê²½ìš°, modelì„ ì•ˆì“°ê³  model-free RLì„ ì“¸ ìˆ˜ ìˆì„ê±°ê³ 
  => modelì˜ uncertaintyë¥¼ í‘œí˜„í•´ ì¤Œìœ¼ë¡œì¨ modelì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
    ì˜ˆë¥¼ ë“¤ì–´ modelì´ 30ì„ ì¶œë ¥í•œë‹¤ ë¼ëŠ” ê²ƒì„ modelì´ 20~40ì‚¬ì´ì—ì„œ ì¶œë ¥í•œë‹¤. ë¼ëŠ” ì‹

***************
### Integrated Architecture

Real and Simulated Experience
  we consider two sources of experience

  Real experience - Sampled from environment (true MDP)
    S' ~ P^a_ss'
    R  = R^a_s

  Simulated experience - Sampled from model (approximate MDP)
    S' ~ Pn(S'| S,A)
    R  = Rn(R | S,A)

Integrating Learning and Planning
  Model - Free RL
    No model
    Learn value function (and/or policy) from real experience

  Model - Based RL
    Learn a model from real experience
    Plan value function (and/or policy) from simulated experience

  Dyna
    Learn a model from experience
    Learn and plan value function (and/or policy) from real and simulated experience

#### Dyna-Q Algorithm
Dyna-Q Algorithm pseudo code
  Initialize Q(s,a) and Model(s,a) for all s and a
  Do forever:
    (a) S <- current (nonterminal) state
    (b) A <- e-greedy(S,Q)
    (c) Execute action A; observe resultant reward, R, and state, S'
    (d) Q(S,A) <- Q(S,A) + ğ›¼ * [R + ğ›¾ * max{Q(S',A)} - Q(S,A)]
    (e) Model(S,A) <- R, S' (assuming deterministic environment)
    (f) Repeat n times :
        S <- random previously observed state
        A <- random action previously taken in S
        R, S' <- Model(S,A)
        Q(S,A) <- Q(S,A) + ğ›¼ * [R + ğ›¾ * max{Q(S',A)} - Q(S,A)]

    => (e)ê³¼ì •ì€ Model learningì´ê³  (f)ê³¼ì •ì€ modelì„ í†µí•œ planning!

Dyna-Q on a Simple Maze example
  ì•„ì£¼ ì¡°ê¸ˆì˜ ê²½í—˜ì„ ê°€ì§€ê³  ê·¸ê±¸ ì¥ì–´ ì§œë‚´ì„œ dataë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë¼ê³ 
  ì‹¤ë²„ êµìˆ˜ë‹˜ì´ ì–¸ê¸‰ í–ˆë‹¤ê³  í•¨.

***************
### Simulation-Based Search
  ì–´ë–»ê²Œ Planningì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§„í–‰ í•  ê±´ì§€ì— ëŒ€í•œ ê°•ì˜

#### Forward Search
  Forward search algorithms select the best action by lookahead
  They build a search tree with the current state s_t at the root
  Using a model of the MDP to look ahead
  No need to solve whole MDP, just sub-MDP starting from now

=> í˜„ì¬ ìƒíƒœëŠ” particularly importantí•˜ë‹¤. ì§€ê¸ˆìœ¼ë¡œ ë¶€í„°ì˜ ë¯¸ë˜ë§Œ ë³´ê² ë‹¤ ë¼ëŠ” ì² í•™
ì§€ê¸ˆë¶€í„° ì´ì–´ì§€ëŠ” sub-MDPë§Œ í’€ì–´ë‚¸ë‹¤ëŠ” ë§ˆì¸ë“œ

#### Simulation-Based Search
ë¯¸ë˜ì˜ ìƒí™©ë“¤ì„ sample based planningì„ í†µí•´ì„œ í‘¸ëŠ” ê²ƒ
  Forward search paradigm using sample-based planning
  Simulate episodes of experience from now with the model
  Apply model-free RL to simulated episodes

=> Stë¡œë¶€í„° ìƒì„±ë˜ëŠ” ë§ì€ episodeë“¤ì„ simulationí•˜ëŠ” ê²ƒ
=> ê·¸ë ‡ê²Œ ìƒì„±ëœ simulated episodeë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ model - free RLì„ ì ìš©í•˜ëŠ” ê²ƒ.

  Simulate episodes of experience from now with the model
    {k=1 -> K}{s^k_t, A^k_t, R^k_(t+1), ... , S^k_T} ~ Mv

  Apply model-free RL to simulated episodes
    Monte-Carlo control -> Monte-Carlo search
    Sarsa -> TD search

#### Simple Monte-Carlo Search
  Given a model Mv and a simulation policy ğœ‹
  For each action a
    Simulate K episodes from current (real) state s_t
      {k=1 -> K}{s_t, a, R^k_(t+1), S^k_(t+1), S^k_(t+1), ... , S^k_T} ~ Mv,ğœ‹

    Evaluate actions by mearn return (Monte-Carlo evaluation)
      Q(s_t, a) = 1/K sum(G_t) -> qğœ‹(st,a)

  Select current (real) action with maximum value
    at = {a} argmax Q(st,a)

#### Monte - Carlo Tree Search (Evaluation)
  Given a model Mv
  Simulate K episodes from current state s_t using current simulation policy ğœ‹
    {k=1 -> K}{s_t, a, R^k_(t+1), S^k_(t+1), S^k_(t+1), ... , S^k_T} ~ Mv,ğœ‹

  Build a search tree containing visited states and actions
  Evaluate states Q(s,a) by mean return of episodes from s, a
    Q(s, a) = 1/N(s,a) {k=1->K} Î£ {u=t->T} Î£ II(Su, Au = s,a) Gu -> qğœ‹(s,a)

  After search is finished, select current (real) action with maximum value in search tree
    at = {a} argmax Q(st,a)

ëª¨ë“  actionì— ëŒ€í•´ ë­˜ í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ í˜„ì¬ policyì— ëŒ€í•´ì„œ simulationì„ í•˜ëŠ” ê²ƒ.

In MCTS, the simulation policy ğœ‹ improves!!
=> Monte-Carlo searchì˜ ê²½ìš°ëŠ” policyê°€ ê³ ì • ë˜ì–´ ìˆì—ˆë‹¤. ê·¼ë° MCTSëŠ” ğœ‹ë¥¼ ê°œì„ ì‹œí‚´

Each simulation consists of two phase (in-tree, out-of-tree)
  Tree policy (improves): pick actions to maximise Q(S,A)
  => Që¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” actionì„ ì„ íƒí•˜ëŠ” policy (in-tree ì¼ ê²½ìš° ë‚´ê°€ Q ê°’ì„ ì•„ë‹ˆê¹)
  Default policy (fixed): pick actions randomly
  => ëœë¤í•œ actionì„ ì„ íƒí•˜ëŠ” policy (out-of-tree ì¼ ê²½ìš° ë‚´ê°€ Q ê°’ì„ ëª¨ë¥´ë‹ˆê¹)

Repeat (each simulation)
  Evaluate states Q(S,A) by Monte-Carlo evaluation
  Improve tree policy, e.g. by e-greedy(Q)

Monte-Carlo control applied to simulated experience
Converges on the optimal search tree, Q(S,A) -> q*(S,A)

Advantages of MC Tree Search
  Highly selective best-first search
  Evaluates states dynamically (unlike e.g. DP)
    => DPì™€ ë‹¤ë¥´ê²Œ stateë¥¼ ë™ì ìœ¼ë¡œ evaluationí•œë‹¤. ì´ ì´ìœ ëŠ” ëª¨ë“  stateë¥¼ í•œë²ˆì— back up í•˜ì§€ ì•Šì•„ì„œ.

  Uses sampling to break curse of dimensionality
    => Samplingì„ í†µí•´ì„œ ì°¨ì›ì˜ ì €ì£¼ë¥¼ ë¶€ìˆœë‹¤.

  Works for "black-box" models (only requires samples)
    => modelì´ black-boxì—¬ë„ ì§„í–‰ì´ ëœë‹¤.

  Computationally efficient, anytime, parallelisable
    => ê³„ì‚° íš¨ìœ¨ì ì´ê³ , ë³‘ë ¬ì ì´ë¼ëŠ” ì¥ì ì´ ìˆë‹¤.

#### Temporal - Difference Search
  Simulation-based search
  Using TD instead of MC (bootstrapping)
  MC tree search applies MC control to sub-MDP from now
  TD search applies Sarsa to sub-MDP from now

MC vs TD search
  For model-free reinforcement learning, bootstrapping is helpful
    TD learning reduces variance but increases bias
    TD learning is usually more efficient than MC
    TD(ğœ†) can be much more efficient than MC

  For simulation-based search, bootstrapping is also helpful
    TD search reduces variance but increases bias
    TD search is usually more eficient than MC search
    TD(ğœ†) can be much more efficient than MC search

TD Search
  Simulate episodes from the current (real) state st
  Estimate action-value function Q(s,a)
  For each step of simulation, update action-values by Sarsa
    âˆ†Q(S, A) = ğ›¼ * (R + ğ›¾ * Q(S', A') - Q(S,A))

  Select actions based on action-values Q(s,a)
    e.g. e-greedy

  May also use function approximation for Q

#### Dyna-2 ì•ì—ì„œ í–ˆë˜ Dynaì˜ ë‹¤ë¥¸ ë²„ì ¼
In Dyna-2, the agent stores two sets of feature weights
  Long-term memory
  Short-term (working) memory
  agentê°€ ë‘ê°œì˜ featureë¥¼ ì¤€ë¹„í•´ì„œ Long-term memoryì— ê´€í•œ ê²ƒì€ ì‹¤ì œ ê²½í—˜ìœ¼ë¡œ í•™ìŠµ
  Short-term memoryì˜ ê²½ìš° simulated experienceë¥¼ í†µí•´ í•™ìŠµ

Long-term memory is updated from real experience using TD learning
  General domain knowledge that applies to any episode

Short-term memory is updated from simulated experience using TD search
  Specific local knowledge about the current situation

Over value function is sum of long and short-term memories

***

#### ì •ë¦¬
ëª¨ë¸ì„ í•™ìŠµ í•˜ëŠ”ë°©ë²•ê³¼, ê·¸ë¥¼ ì´ìš©í•˜ëŠ” Model-Based RLì„ ë°°ì› ë‹¤.
ê·¸ ë‹¤ìŒ ê·¸ Modelì„ ì•„ë‹ˆê¹Œ Dynamic programmingì„ í•˜ëŠ” ë°©ë²•ë„ ë°°ì›Œë´¤ê³ 

ëª¨ë¸ì„ ëª¨ë¥´ëŠ” ì²™ í•˜ê³  ëª¨ë¸ì—ì„œ sampleì„ simulationì„ í†µí•´ ì–»ì–´ì„œ model-free RLì„ í•´ë³´ê¸°ë„ í–ˆë‹¤.

ë˜í•œ, real experienceë¡œ direct RLì„ í•˜ê³  modelì„ ë§Œë“œëŠ” ê³¼ì •ê³¼
modelì´ ë§Œë“  simulated experienceë¡œ planningí•˜ëŠ” ê³¼ì •ì´ í†µí•©ëœ Dynaì— ëŒ€í•´ì„œë„ ë°°ì› ë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ Forward searchì— ëŒ€í•´ì„œë„ ë°°ì› ë‹¤. í˜„ì¬ ìƒí™©ì— íŠ¹í™”í•´ì„œ searchí•˜ëŠ” ë¬¸ì œì˜€ìŒ.
ê·¸ ì¤‘ì—ì„œë„ Simulation-based searchë¥¼ ë°°ì› ë‹¤. simulationìœ¼ë¡œ í˜„ì¬ ìƒí™©ì—ì„œì˜ experienceë¥¼ ë§‰ ë§Œë“¤ì–´ ë‚¸ ë‹¤ìŒ searchë¥¼ í•˜ëŠ” ê²ƒ.

ë§Œë“¤ì–´ë‚¸ experienceë¥¼ Monte-Carlo methodë¡œ í•™ìŠµí•´ searchí•˜ëŠ” Monte-Carlo searchë„í–ˆê³ ,

ì£¼ë³€ì˜ ë…¸ë“œë„ ë‹¤ ê¸°ì–µí•˜ëŠ” Monte-Carlo Tree searchë„ ë°°ì› ì—ˆë‹¤!

ë§ˆì§€ë§‰ìœ¼ë¡œ TD Searchì— ëŒ€í•´ì„œë„ ë°°ì› ìŒã…ã…
