# RL-study-2019

Introduction to Reinforcement Learning

1. ê°•í™”í•™ìŠµì´ë€ ê¸°ê³„í•™ìŠµì˜ ì¼ì¢…ìœ¼ë¡œ, ì§€ë„í•™ìŠµ / ë¹„ì§€ë„ í•™ìŠµê³¼ëŠ” ê·¸ ê¶¤ë¥¼ ë‹¬ë¦¬í•˜ëŠ” ë°©ë²•ë¡ ì´ë‹¤.

  - ê°•í™”í•™ìŠµì—ëŠ” Labeled dataë“±ì˜ supervisor ì—†ê³ , reward signalì„ í†µí•´ í•™ìŠµí•œë‹¤.
  - ì´ë•Œì˜ rewardë€, objective functionìœ¼ë¡œë§Œ ê²°ì •ë˜ë©°, êµ¬ì²´ì ì¸ ì§€ì¹¨(ì •ë‹µ)ì´ ì—†ë‹¤.
  - ë™ë¬¼, ì‚¬ëŒì´ í•™ìŠµí•˜ë“¯ì´ Try & error => reward ë¥¼ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” ê²ƒ.
  - Feedbackì´ ì§€ì—°ë  ìˆ˜ ìˆë‹¤. ì–´ë–¤ actionì„ í–ˆì„ ë•Œ, ì¦‰ê°ì ì¸ ë°˜ì‘ì„ ì–»ì„ ìˆ˜ ì—†ì„ ìˆ˜ë„ ìˆë‹¤.
  - Time really matters : actionì˜ ìˆœì„œ(ìš°íšŒì „ í›„ ì¢ŒíšŒì „ or ì¢ŒíšŒì „ í›„ ìš°íšŒì „)ê°€ ì—„ì²­ë‚œ ì˜í–¥ì„ ë¼ì¹  ìˆ˜ ìˆë‹¤.
  - agentì—ì„œ ì–´ë–¤ actionì„ ì·¨í•˜ê²Œ ë˜ë©´, ê·¸ ì´í›„ì˜ stateê°€ ë‹¤ ë°”ë€ë‹¤. (ê°•í™”í•™ìŠµì˜ íŠ¹ì§•)


2. Reward R_t : ë³´ìƒ Scalar feedback signal
  - R_të€ step tì—ì„œ agentê°€ ì–¼ë§ˆë‚˜ ì˜ í–ˆëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ëŠ” ì§€í‘œ(indicator)
  - ê°•í™”í•™ìŠµì€ cumulative rewardë¥¼ maximizeí•˜ëŠ” ê²ƒì´ ëª©ì 
  - ê°•í™”í•™ìŠµì€ reward hypothesisë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¤„ì§„ë‹¤.
    > reward hypothesisëŠ” ë¬¸ì œì— ë”°ë¼ ë‹¬ë¼ì ¸ì•¼ í•œë‹¤. ìµœì í™”ì—ì„œì˜ objective function
  - Sequential Decision making
    > sequentialí•œ actionì„ decision í•˜ëŠ” ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.


3. Agent & Environment
  - AgentëŠ” Controller ê°™ì€ ë…€ì„ í˜„ì¬ ê´€ì¸¡ì¹˜ O_tì™€ ë³´ìƒ R_të¥¼ í†µí•´ action A_t+1ì„ ì·¨í•œë‹¤.
  - Action A_t+1ì€ environmentì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¼ì¹˜ê³  Agentë¡œ O_t+1ê³¼ R_t+1ì„ emití•œë‹¤.


4. History and State
  - H_t = O1,R1,At, ... , A_t-1,O_t,R_t
  - Historyë€ sequentialí•œ ê´€ì¸¡ì¹˜, í–‰ë™, ë³´ìƒì˜ ê¸°ë¡ì´ë‹¤.
  - agentëŠ” historyë¥¼ ë³´ê³  actionì„ ì •í•˜ê³ , environmentëŠ” observationê³¼ rewardë¥¼ ë°©ì¶œí•œë‹¤.

  - StateëŠ” ë‹¤ìŒì— ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚ ì§€ë¥¼ ê²°ì •í•˜ëŠ” 'ì •ë³´'ì´ë‹¤!
  - ê³µì‹ì ìœ¼ë¡œ, state is a function of the history
    > ê³¼ê±°ì˜ ëª¨ë“  ê±¸ ë³¼ìˆ˜ë„ ìˆê³ , ì¼ë¶€ë§Œ ë³¼ ìˆ˜ë„ ìˆëŠ” ê·¸ëŸ° ëŠë‚Œì¸ ê²ƒ


5. Environmentì˜ state
  - í™˜ê²½ì´ observationê³¼ rewardë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì“°ëŠ” ì •ë³´ë“¤ì´ ë°”ë¡œ stateì´ë‹¤.
    > ìë™ì°¨ë¡œ ì¹˜ë©´, ìŠ¤í‹°ì–´ë§ ì•µê¸€, ì—‘ì…€ í˜ë‹¬ í¬ì§€ì…˜ ë“± agentê°€ ì•Œì•„ë´¤ì ì“¸ëª¨ ì—†ëŠ” ê²ƒë“¤ ì´ì§€ë§Œ, environmentê°€ actionì„ ë°›ì•„ì„œ ë‹¤ìŒ ìƒíƒœê°€ ë˜ê¸°ê¹Œì§€ì—ëŠ” ê¼­ í•„ìš”í•œ ì •ë³´


6. Agentì˜ state
  - ë‹¤ìŒ actionì„ í•´ì•¼ í•  ë•Œ, í•„ìš”í•œ ì •ë³´ë“¤
    > ìë™ì°¨ë¡œ ì¹˜ë©´, í˜„ì¬ ê¸°ì–´ ë‹¨ ìˆ˜, í˜„ì¬ ì†ë„, 100m ì•ì˜ ì‹ í˜¸ë“± ìƒ‰ ë“±
  - ì˜ˆì „ì— ìˆì—ˆë˜ reward, observation, actionë“±ì˜ ì •ë³´ë¥¼ ë°”ë¡œ ì“¸ ìˆ˜ë„ ìˆê³ , ê°€ê³µí•´ì„œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤.


7. Markov State
  - Stateê°€ Markoví•˜ë‹¤ëŠ” ê²ƒì€, ì–´ë–¤ ìƒíƒœê°€ í˜„ì¬ ìƒíƒœì—ë§Œ ì˜í–¥ì„ ë°›ì•„ ë‹¤ìŒ ìƒíƒœë¡œ ê°„ë‹¤ëŠ” ê²ƒ


8. Fully Observable Environments
  - agent directly observes environment state
    > O(t) = S(t,a)
  - ìë™ì°¨ë¡œ ì¹˜ë©´, ë¼ì´ë‹¤ê°€ í•œ ë°”í€´ ëì—ë”°ë¼, partially observableí•˜ê²Œ í™˜ê²½ì„ ì•Œ ìˆ˜ ìˆë‹¤.
  - POMDP (Partially Observable MDP)
    > Complete history : S(t,a) = H(t)
    > Beliefs of environment state : S(t,a) = (P[S(t,e)=s1], ... , P[S(t,e)=sn])


9. Major Components of RL
  - Policy
    > agentì˜ actionì„ ì •í•´ì£¼ëŠ” ì¹œêµ¬, stateì™€ actionì„ mapping í•´ì¤€ë‹¤.
    > Deterministic policy: a = ğœ‹(s), Stochastic policy: ğœ‹(a|s) = P[A_t = a | S_t = s]ê°€ ìˆë‹¤.

  - Value function
    > state, actionì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜

  - Model
    > A model predics what the environment will do next~!
    > P predicts the next state => P(ss',a) = P[S(t+1) = s' | S(t) = s, A(t) = a]
    > R predicts the next (immediate) reward => P(ss',a) = E[R(t+1) | S(t) = s, A(t) = a]
    > modelì˜ ì‚¬ìš© ìœ ë¬´ë¡œ RLì„ model - based RL // model - free RL ë¡œ ë‚˜ëˆˆë‹¤.


10. Categorizing RL agent (1)
  - Value based agent
    > Value functionë§Œ ìˆì–´ë„ agentì˜ ì—­í• ì„ í•  ìˆ˜ ìˆìŒ <= ?? ë§ì´ ì´ìƒí•œë“¯

  - Policy based agent
    > Policy ë§Œ ìˆì–´ë„ agentì˜ ì—­í• ì„ í•  ìˆ˜ ìˆìŒ <= ?? ë§ì´ ì´ìƒí•œë“¯

  - Actor - Critic
    > Policyì™€ Value functionì„ í•™ìŠµí•˜ëŠ” agent


10. Categorizing RL agent (2)
  - Model-Free
    > Environmentì˜ modelì„ ì•Œì§€ ëª» í•˜ë”ë¼ë„, Policyì™€ Valueë§Œ ê°€ì§€ê³  í•™ìŠµì„ í•˜ëŠ” ê²ƒ??

  - Model-Based
    > Environmentì˜ modelì„ ì˜ˆì¸¡í•˜ì—¬ ë§Œë“¤ì–´ì„œ, ê·¸ê²ƒì— ê·¼ê±°í•´ì„œ í•™ìŠµì„ í•˜ëŠ” ê²ƒ??


11. RLë¡œ í’€ ìˆ˜ ìˆëŠ” ëŒ€í‘œì ì¸ ë‘ ê°€ì§€ ë¬¸ì œ: Learning and Planning
  - Lenarning ë¬¸ì œëŠ” í™˜ê²½ì— ëŒ€í•´ ëª¨ë¥´ì§€ë§Œ, í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš© í•˜ë©´ì„œ Policyë¥¼ ê°œì„ í•´ë‚˜ê°€ëŠ” ë¬¸ì œ

  - Planning ë¬¸ì œëŠ” í™˜ê²½ì˜ ëª¨ë¸ì„ ì•„ëŠ” ê²½ìš°ì—(Rê³¼ Pê°€ ì£¼ì–´ì§„ ê²½ìš°) ì‹¤ì œë¡œ environmentë¥¼ ì•„ë‹ˆê¹Œ, actionì„ ì·¨í•˜ì§€ ì•Šê³ ë„, ë‚´ë¶€ì ìœ¼ë¡œ computationì„ í†µí•´ ëª¨ë“  ìƒí™©ì„ ì•Œ ìˆ˜ ìˆë‹¤.
    > ë‚´ë¶€ì ì¸ ê³„ì‚°ì„ í†µí•´ Policyë¥¼ ê°œì„ í•´ë‚˜ê°€ëŠ” ë¬¸ì œ
    > agentì˜ ë‡Œì— perfect modelì´ ìˆë‹¤ê³  í‘œí˜„í–ˆìŒ.


12. Exploration and Exploitation
  - Reinforcement learningì€ Trial and Errorë¥¼ í†µí•´ í•™ìŠµí•˜ëŠ” í•™ìŠµ ë°©ë²•ì´ë‹¤.
  - Environmentì˜ ê²½í—˜ìœ¼ë¡œë¶€í„° ì¢‹ì€ Policyë¥¼ discoverí•˜ëŠ” ê²ƒì´ ëª©ì 

  - Exploration
    > Environmentë¥¼ íƒí—˜í•´ì„œ ì •ë³´ë¥¼ ëª¨ìœ¼ëŠ” ê³¼ì •

  - Exploitation
    > ì§€ê¸ˆê¹Œì§€ì˜ ì •ë³´ë¥¼ ê°€ì§€ê³  ìµœì„ ì˜ ì„ íƒì„ ë‚´ë¦¬ëŠ” ê³¼ì •

  - Exploration, exploitation ë‘˜ ë‹¤ ì¤‘ìš”í•˜ì§€ë§Œ, ì´ ë‘˜ì´ Trade - off ê´€ê³„ì— ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•´ë¼.

13. Prediction and Control
  - Prediction : ë¯¸ë˜ë¥¼ í‰ê°€í•˜ëŠ” term
    > Given a policy
    > Value functionì„ í•™ìŠµì‹œí‚¤ëŠ” ë¬¸ì œ

  - Control : ë¯¸ë˜ë¥¼ ìµœì í™” í•˜ëŠ” term
    > Find the best policy
