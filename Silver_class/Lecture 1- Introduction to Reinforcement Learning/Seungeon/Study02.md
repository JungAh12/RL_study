# RL-study-2019

Marcov Decision Process

1. Introduction to MDPs

  - ê°•í™”í•™ìŠµ ë¬¸ì œì—ì„œ í™˜ê²½ì„ formally describeí•œ ê²ƒ
  - ëŒ€ë¶€ë¶„ì˜ ê°•í™”í•™ìŠµ ë¬¸ì œëŠ” MDPë¡œ formailiseí•  ìˆ˜ ìˆë‹¤.
    - MDPë¥¼ í†µí•´ ìµœì ì œì–´ ë¬¸ì œë¥¼ ë‹¤ë£° ìˆ˜ ìˆë‹¤.
    - Partially observable ë¬¸ì œ ë˜í•œ MDPë¡œ convertí•  ìˆ˜ ìˆë‹¤.

2. Markov State

  - Stateê°€ Markoví•˜ë‹¤ëŠ” ê²ƒì€, ì–´ë–¤ ìƒíƒœê°€ í˜„ì¬ ìƒíƒœì—ë§Œ ì˜í–¥ì„ ë°›ì•„ ë‹¤ìŒ ìƒíƒœë¡œ ê°„ë‹¤ëŠ” ê²ƒ

3. State Transition Matrix

  - For a Markov state s and successor state s', the state transition probability is defined by
      P_ss' = P [S_(t+1) = s' | S_t = s]

  - State transition matrix P defines transition probabilities from all states s to all successor states s'

4. A Markov Process

  A Markov process is a memoryless random process, i.e. a sequence of random states S1, S2, ... with the Markov property.

    > Definition

      A Markov Process (or Markov Chain) is a tuple <S, P>
      S is a (finite) set of states
      P is a state transition probability matrix,
        P_ss' = P[S_(t+1) = s' | S_t = s]

5. Markov Reward Process
  A Markov reward process is a Markov chain with values.

    > Definition
      A Markov Reward Process is a tuple <S, P, R, ğ›¾>
      - S is a finite set of states
      - P is a state transition probability matrix,
          P_ss' = P[S_(t+1) = s' | S_t = s]
      - R is a reward function, R_s = E[R_(t+1) | S_t = s]
      - ğ›¾ is a discount factor, ğ›¾ âˆˆ [0, 1]

6. Return

    > Definition
      The return G_t is the total discounted reward from time-step t.
        G_t = R_(t+1) + ğ›¾ * R_(t+2) + ... = {k = 0 to âˆ} Î£ ğ›¾^k * R_(t+k+1)

      - The discount ğ›¾ âˆˆ [0, 1] is present value of future rewards
      - The value of receiving reward R after k+1 time-steps is ğ›¾^k * R
      - This values immediate reward above delayed reward.
          ğ›¾ close to 0 leads to "myopic" evaluation
          ğ›¾ close to 1 leads to "far-sighted" evaluation

7. Discount factorê°€ í•„ìš”í•œ ì´ìœ 
  Most Markov reward and decision processes are discounted. Why?

  - Mathematically convenient to discount rewards
  - Avoids infinite returns in cyclic Markov processes
  - Uncertainty about the future may not be fully represented
  - If the reward is financial, immediate rewards may earn more interset than delayed rewards
  - Animal / human behaviour shows preference for immediate reward
  - It is sometimes possible to use undiscounted Markov reward processes (i.e. ğ›¾ = 1), e.g. if all sequences terminate.

8. State-value function
  The value function v(s) gives the long-term value of state s

    > Definition
      The stete value function v(s) of an MRP is the expected return starting from state s
        v(s) = E[G_t | S_t = s]

9. Bellman Equation for MRPs
  The value function can be decomposed into two parts :
    immediate reward R_(t+1)
    discounted value of successor state ğ›¾ * v(S_(t+1))
      v(s) = E[G_t | S_t = s]
           = E[R_(t+1) + ğ›¾ * R_(t+2) + ğ›¾^2 * R_(t+2) + ... | S_t = s]
           ...
           = E[R_(t+1) + ğ›¾ * G_(t+1) | S_t = s]
           = E[R_(t+1) + ğ›¾ * v(S_(t+1)) | S_t = s]

      v(s) = R_s + ğ›¾ * {s' âˆˆ S} Î£ ğ‘ƒ_ğ‘ ğ‘ â€² âˆ— ğ‘£(ğ‘ â€²)

    Vectorì‹ìœ¼ë¡œ í‘œí˜„ì„ í•˜ê²Œ ë˜ë©´
      v = R + ğ›¾Pv, where v is a column vector with one entry per state
      [v(1), ... , v(n)]' = [R(1), ... , R(n)]' + ğ›¾ * [P_11 ... P_1n ; P_21, ... P_2n ; ... P_nn] * [v(1), ... , v(n)]'

10. Solving the Bellman Equation in MRPs

   - The bellman equation is a linear euqation
   - It can be solved directly
              v = R + ğ›¾Pv
      (I - ğ›¾P)v = R
              v = (I - ğ›¾P)^-1 * R
   - Computational complexity is O(n^3) for n states
   - Direct solution only possible for small MPRs
   - There are many iterative methods for large MRPs, e.g.
     Dynamic programming
     Monte-Carlo evaluation
     Temporal-Difference learning

11. Markov Decision Process
  A Markov decision process (MDP) is a Markov reward process with decisions.
  It is an environment in which all states are Markov.

    > Definition
      A Markov Decision Process is a tuple <S, A, P, R, ğ›¾>
      - S is a finite set of states
      - A is a finite set of actions
      - P is a state transition probability matrix,
        P^a_ss' = P[S_(t+1) = s' | S_t = s, A_t = a]
      - R is a reward function, R^a_s = E[R_(t+1) | S_t = s, A_t = a]
      - ğ›¾ is a discount factor ğ›¾ âˆˆ [0, 1].

12. Policies

    > Definition
      A policy ğœ‹ is a distribution over actions given states,
        ğœ‹(a|s) = P[A_t = a | S_t = s]

      - A policy fully defines the behaviour of an agent
      - MDP policies depend on the current state (not the history)
      - i.e. Policies are stationary (time-independent),
        A_t ~ ğœ‹(âˆ™|S_t), âˆ€t > 0

  - Given an MDP M = <S, A, P, R, ğ›¾> and a policy ğœ‹
  - The state sequence S_1, S_2, ... is a Markov process <S, P^ğœ‹>
  - The state and reward sequence S_1, R_2, S_2, ... is a Markov reward process <S, P^ğœ‹, R^ğœ‹, ğ›¾>
  - where
    P^ğœ‹_ss' = {a âˆˆ A} ğœ‹(a|s) * P^a_ss'
    R^ğœ‹_s   = {a âˆˆ A} ğœ‹(a|s) * R^a_s

13. Value function

    > State-value function v(s) Definition
      The state-value function v_ğœ‹(s) of an MDP is the expected return starting from state s, and then following polic ğœ‹
        v_ğœ‹(s) = E_ğœ‹[G_t | S_t = s]

    > Action-value function q(s,a) Definition
      The action-value function q_ğœ‹(s,a) is the expected return starting from state s, taking action a, and then following policy ğœ‹
        q_ğœ‹(s,a) = E_ğœ‹[G_t | S_t = s, A_t = a]

14. Bellman Expectaion Equation
  The state-value function can again be decomposed into immediate reward plus discounted value of successor state,

    v_ğœ‹(s) = E_ğœ‹[R_(t+1) + ğ›¾ * v_ğœ‹(S_(t+1)) | S_t = s]

  The action-value function can similarly be decomposed.

    q_ğœ‹(s,a) = E_ğœ‹[R_(t+1) + ğ›¾ * q_ğœ‹(S_(t+1), A_(t+1)) | S_t = s, A_t = a]

15. Bellman Expectation Equation (Matrix Form)
  The Bellman expectation equation can be expressed concisely using the induced MRP,

            v_ğœ‹ = R^ğœ‹ + ğ›¾ * P^ğœ‹ * v_ğœ‹
    with direct solution
            v_ğœ‹ = (I - ğ›¾ * P^ğœ‹)^-1 * R^ğœ‹

16. Optimal Value function

  - The optimal value function specifies the best possible performance in the MDP
  - An MDP is "solved" when we know the optimal value function

    > Definition
      The optimal state-value function v_*(s) is the maximum value function over all policies
        v_*(s) = {ğœ‹} max (v_ğœ‹(s))

      The optimal action-value function q_*(s,a) is the maximum action-value function over all policies
        q_*(s,a) = {ğœ‹} max (q_ğœ‹(s,a))

17. Optimal policy

  Define a partial ordering over policies

    ğœ‹ >= ğœ‹' if v_ğœ‹(s) >= v_ğœ‹'(s), âˆ€s

    > Theorem
      For any Markov Decision Process

      - There exists an optimal policy ğœ‹_* that is better than or equal to all other policies,
        ğœ‹_* >= âˆ€ğœ‹

      - All optimal policies achieve the optimal value function
        v_ğœ‹**(s) = v_**(s)

      - All optimal policies achieve the optimal action-value function,
        q_ğœ‹**(s,a) = q_**(s,a)

18. Finding an Optimal Policy
  An optimal policy can be found by maximising over q_*(s,a)
  q_*ë¥¼ ì•Œë©´, íŠ¹ì • ìƒíƒœì—ì„œ optimal actionì„ ì•„ëŠ” ê²ƒì´ë¯€ë¡œ optimal policyë¥¼ ì•ˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.

                1    if a = {a âˆˆ A} argmax (q_*(s,a))
    ğœ‹_*(a|s) =
                0    otherwise

  - There is always a deterministic optimal policy for any MDP
  - if we know q_*(s,a), we immediately have the optimal policy

19. Bellman Optimality Equation
  The optimal value function are recursively related by the Bellman optimality equations :
    v_**(s)   = {a} max(q_**(s,a))

    q_**(s,a) = R^a_s + ğ›¾ * {s' âˆˆ S} Î£ P^a_ss' v_**(s')

    v_**(s)   = {a} max (R^a_s) + ğ›¾ * {s' âˆˆ S} Î£ P^a_ss' v_**(s')

    q_**(s,a) = R^a_s + ğ›¾ * {s' âˆˆ S} Î£ P^a_ss' q_**(s', a')

20. Solving the Bellman Optimality Equation
  - Bellman Optimality Equation is non-linear
  - No closed form solution (in general)
  - Many iterative solution methods
    - Value iteration
    - Policy iteration
    - Q-learning
    - Sarsa

21. Extensions to MDPs
  - Infinite and continous MDPs
  - Partially observable MDPs
  - Undiscounted, average reward MDPs

22. Infinite MDPs
  The following extensions are all possible:
    - Countably infinite state and/or action spaces
      - Straightforward

    - Continuous state and/or action spaces
      - Closed form for linear quadratic model (LQR)

    - Continuous time
      - Requires partial differential equations
      - Hamilton-Jacobi-Bellman (HJB) equation
      - Limiting case of Bellman equation as time-step -> 0

23. POMDPs
  A partially Observable Markov Decision Process is an MDP with hidden states.
  It is a hidden Markov model with actions.

    > Definition
      A POMDP is a tuple <S, A, O, P, R, Z, ğ›¾>
        - S is a finite set of states
        - A is a finite set of actions
        - O is a finite set of observations
        - P is a state transition probability matrix
          - P^a_(ss') = P[S_(t+1) = s' | S_t = s, A_t = a]
        - R is a reward function, R^a_s = E[R_(t+1) | S_t = s, A_t = a]
        - Z is an observation function,
          - Z^a_(s', o) = P[O_(t+1) = o | S_t = s', A_t = a]
        - ğ›¾ is a discount factor ğ›¾ âˆˆ [0, 1]

24. Belief States

    > Definition
      A history H_t is a sequence of actions, observations and rewards,
        H_t = A_0, O_1, R_1, ... , A_(t-1), O_t, R_t

    > Defienition
      A belief state b(h) is a probability distribution over states, conditioned on the history h
        b(h) = (P[S_t = s^1 | H_t = h], ..., P[S_t = s^n | H_t = h])

25. Reductions of POMDPs
  - The history H_t satisfies the Markov property
  - The belief state b(H_t) satisfies the Markov property

  - A POMDP can be reduced to an (infinite) history tree
  - A POMDP can be reduced to an (infinite) belief state tree

26. Ergodic Markov Process
  An ergodic Markov process is
  - Recurrent: each state is visited an infinite number of times
  - Aperiodic: each state is visited without any systematic period

    > Theorem
      An ergodic Markov process has a limiting stationary distribution d^ğœ‹(s) with the property
        d^ğœ‹(s) = {s' âˆˆ S} Î£ d^ğœ‹(s') * P_(s's)

27. Ergodic MDP
  For any policy ğœ‹, an ergodic MDP has an average reward per time-step ğœŒ^ğœ‹ that is independent of start state
    ğœŒ^ğœ‹ = {T -> âˆ} lim (1/T) * E[{t=1 to T} Î£ R_t]

28. Average Reward Value function
  The value function of an undiscounted, ergodic MDP can be wxpressed in terms of average reward.
  vtild_ğœ‹(s) is the extra reward due to starting from state s,
    vtild_ğœ‹(s) = E_ğœ‹[ {k=1 -> âˆ} Î£ (R_(t+k) - ğœŒ^ğœ‹) | S_t = s]

  There is a corresponding average reward Bellman equation,
    vtild_ğœ‹(s) = E_ğœ‹[ (R_(t+1) - ğœŒ^ğœ‹) + {k=1 -> âˆ} Î£ (R_(t+k+1) - ğœŒ^ğœ‹) | S_t = s]
               = E_ğœ‹[ (R_(t+1) - ğœŒ^ğœ‹) + vtild_ğœ‹(S_(t+1)) | S_t = s]

















d
