# RL-study-2019

Marcov Decision Process

1. Introduction to MDPs

  - Í∞ïÌôîÌïôÏäµ Î¨∏Ï†úÏóêÏÑú ÌôòÍ≤ΩÏùÑ formally describeÌïú Í≤É
  - ÎåÄÎ∂ÄÎ∂ÑÏùò Í∞ïÌôîÌïôÏäµ Î¨∏Ï†úÎäî MDPÎ°ú formailiseÌï† Ïàò ÏûàÎã§.
    - MDPÎ•º ÌÜµÌï¥ ÏµúÏ†ÅÏ†úÏñ¥ Î¨∏Ï†úÎ•º Îã§Î£∞ Ïàò ÏûàÎã§.
    - Partially observable Î¨∏Ï†ú ÎòêÌïú MDPÎ°ú convertÌï† Ïàò ÏûàÎã§.

2. Markov State

  - StateÍ∞Ä MarkovÌïòÎã§Îäî Í≤ÉÏùÄ, Ïñ¥Îñ§ ÏÉÅÌÉúÍ∞Ä ÌòÑÏû¨ ÏÉÅÌÉúÏóêÎßå ÏòÅÌñ•ÏùÑ Î∞õÏïÑ Îã§Ïùå ÏÉÅÌÉúÎ°ú Í∞ÑÎã§Îäî Í≤É

3. State Transition Matrix

  - For a Markov state s and successor satte s', the state transition probability is defined by
      P_ss' = P [S_(t+1) = s' | S_t = s]

  - State transition matrix P defines transition probabilities from all states s to all successor states s'

4. A Markov Process
  A Markov process is a memoryless random process, i.e. a sequence of random states S1, S2, ... with the Markov property.

    > Definition
      A Markov Process (or Markov Chain) is a tuple <S, P>
      - S is a (finite) set of states
      - P is a state transition probability matrix,
        P_ss' = P[S_(t+1) = s' | S_t = s]

5. Markov Reward Process
  A Markov reward process is a Markov chain with values.

    > Definition
      A Markov Reward Process is a tuple <S, P, R, ùõæ>
      - S is a finite set of states
      - P is a state transition probability matrix,
          P_ss' = P[S_(t+1) = s' | S_t = s]
      - R is a reward function, R_s = E[R_(t+1) | S_t = s]
      - ùõæ is a discount factor, ùõæ ‚àà [0, 1]

6. Return

    > Definition
      The return G_t is the total discounted reward from time-step t.
        G_t = R_(t+1) + ùõæ * R_(t+2) + ... = {k = 0 to ‚àû} Œ£ ùõæ^k * R_(t+k+1)

      - The discount ùõæ ‚àà [0, 1] is present value of future rewards
      - The value of receiving reward R after k+1 time-steps is ùõæ^k * R
      - This values immediate reward above delayed reward.
          ùõæ close to 0 leads to "myopic" evaluation
          ùõæ close to 1 leads to "far-sighted" evaluation

7. Discount factorÍ∞Ä ÌïÑÏöîÌïú Ïù¥Ïú†
  Most Markov reward and decision processes are discounted. Why?

  - Mathematically convenient to discount rewards
  - Avoids infinite returns in cyclic Markov processes
  - Uncertainty about the future may not be fully represented
  - If the reward is financial, immediate rewards may earn more interset than delayed rewards
  - Animal / human behaviour shows preference for immediate reward
  - It is sometimes possible to use undiscounted Markov reward processes (i.e. ùõæ = 1), e.g. if all sequences terminate.

8. State-value function
  The value function v(s) gives the long-term value of state s

    > Definition
      The stete value function v(s) of an MRP is the expected return starting from state s
        v(s) = E[G_t | S_t = s]

9. Bellman Equation for MRPs
  The value function can be decomposed into two parts :
    immediate reward R_(t+1)
    discounted value of successor state ùõæ * v(S_(t+1))
      v(s) = E[G_t | S_t = s]
           = E[R_(t+1) + ùõæ * R_(t+2) + ùõæ^2 * R_(t+2) + ... | S_t = s]
           ...
           = E[R_(t+1) + ùõæ * G_(t+1) | S_t = s]
           = E[R_(t+1) + ùõæ * v(S_(t+1)) | S_t = s]

      v(s) = R_s + ùõæ * {s' ‚àà S} Œ£ ùëÉ_ùë†ùë†‚Ä≤ ‚àó ùë£(ùë†‚Ä≤)

    VectorÏãùÏúºÎ°ú ÌëúÌòÑÏùÑ ÌïòÍ≤å ÎêòÎ©¥
      v = R + ùõæPv, where v is a column vector with one entry per state
      [v(1), ... , v(n)]' = [R(1), ... , R(n)]' + ùõæ * [P_11 ... P_1n ; P_21, ... P_2n ; ... P_nn] * [v(1), ... , v(n)]'

10. Solving the Bellman Equation in MRPs

   - The bellman equation is a linear euqation
   - It can be solved directly
              v = R + ùõæPv
      (I - ùõæP)v = R
              v = (I - ùõæP)^-1 * R
   - Computational complexity is O(n^3) for n states
   - Direct solution only possible for small MPRs
   - There are many iterative methods for large MRPs, e.g.
     Dynamic programming
     Monte-Carlo evaluation
     Temporal-Difference learning

11. Markov Decision Process
  A Markov decision process (MDP) is a Markov reward process with decisions.
  It is an environment in which all states are Markov.

    > Definition
      A Markov Decision Process is a tuple <S, A, P, R, ùõæ>
      - S is a finite set of states
      - A is a finite set of actions
      - P is a state transition probability matrix,
        P^a_ss' = P[S_(t+1) = s' | S_t = s, A_t = a]
      - R is a reward function, R^a_s = E[R_(t+1) | S_t = s, A_t = a]
      - ùõæ is a discount factor ùõæ ‚àà [0, 1].

12. Policies

    > Definition
      A policy ùúã is a distribution over actions given states,
        ùúã(a|s) = P[A_t = a | S_t = s]

      - A policy fully defines the behaviour of an agent
      - MDP policies depend on the current state (not the history)
      - i.e. Policies are stationary (time-independent),
        A_t ~ ùúã(‚àô|S_t), ‚àÄt > 0

  - Given an MDP M = <S, A, P, R, ùõæ> and a policy ùúã
  - The state sequence S_1, S_2, ... is a Markov process <S, P^ùúã>
  - The state and reward sequence S_1, R_2, S_2, ... is a Markov reward process <S, P^ùúã, R^ùúã, ùõæ>
  - where
    P^ùúã_ss' = {a ‚àà A} ùúã(a|s) * P^a_ss'
    R^ùúã_s   = {a ‚àà A} ùúã(a|s) * R^a_s

13. Value function

    > State-value function v(s) Definition
      The state-value function v_ùúã(s) of an MDP is the expected return starting from state s, and then following polic ùúã
        v_ùúã(s) = E_ùúã[G_t | S_t = s]

    > Action-value function q(s,a) Definition
      The action-value function q_ùúã(s,a) is the expected return starting from state s, taking action a, and then following policy ùúã
        q_ùúã(s,a) = E_ùúã[G_t | S_t = s, A_t = a]

14. Bellman Expectaion Equation
  The state-value function can again be decomposed into immediate reward plus discounted value of successor state,

    v_ùúã(s) = E_ùúã[R_(t+1) + ùõæ * v_ùúã(S_(t+1)) | S_t = s]

  The action-value function can similarly be decomposed.

    q_ùúã(s,a) = E_ùúã[R_(t+1) + ùõæ * q_ùúã(S_(t+1), A_(t+1)) | S_t = s, A_t = a]

15. Bellman Expectation Equation (Matrix Form)
  The Bellman expectation equation can be expressed concisely using the induced MRP,

            v_ùúã = R^ùúã + ùõæ * P^ùúã * v_ùúã
    with direct solution
            v_ùúã = (I - ùõæ * P^ùúã)^-1 * R^ùúã

16. Optimal Value function

  - The optimal value function specifies the best possible performance in the MDP
  - An MDP is "solved" when we know the optimal value function

    > Definition
      The optimal state-value function v_*(s) is the maximum value function over all policies
        v_*(s) = {ùúã} max (v_ùúã(s))

      The optimal action-value function q_*(s,a) is the maximum action-value function over all policies
        q_*(s,a) = {ùúã} max (q_ùúã(s,a))

17. Optimal policy

  Define a partial ordering over policies

    ùúã >= ùúã' if v_ùúã(s) >= v_ùúã'(s), ‚àÄs

    > Theorem
      For any Markov Decision Process

      - There exists an optimal policy ùúã_* that is better than or equal to all other policies,
        ùúã_* >= ‚àÄùúã

      - All optimal policies achieve the optimal value function

        > Eq
          v_ùúã*(s) = v_*(s)

      - All optimal policies achieve the optimal action-value function,

        > Eq
          q_ùúã*(s,a) = q_*(s,a)

18. Finding an Optimal Policy
  An optimal policy can be found by maximising over q_*(s,a)
  q_*Î•º ÏïåÎ©¥, ÌäπÏ†ï ÏÉÅÌÉúÏóêÏÑú optimal actionÏùÑ ÏïÑÎäî Í≤ÉÏù¥ÎØÄÎ°ú optimal policyÎ•º ÏïàÎã§Í≥† Ìï† Ïàò ÏûàÎã§.

                1    if a = {a ‚àà A} argmax (q_*(s,a))
    ùúã_*(a|s) =
                0    otherwise

  - There is always a deterministic optimal policy for any MDP
  - if we know q_*(s,a), we immediately have the optimal policy

19. Bellman Optimality Equation for v_*
  The optimal value function are recursively related by the Bellman optimality equations :

    > Eq
      v_*(s) = {a} max(q_*(s,a))

















asdf
