# RL-study-2019

Marcov Decision Process

1. Introduction to MDPs

  - ê°•í™”í•™ìŠµ ë¬¸ì œì—ì„œ í™˜ê²½ì„ formally describeí•œ ê²ƒ
  - ëŒ€ë¶€ë¶„ì˜ ê°•í™”í•™ìŠµ ë¬¸ì œëŠ” MDPë¡œ formailiseí•  ìˆ˜ ìˆë‹¤.
    - MDPë¥¼ í†µí•´ ìµœì ì œì–´ ë¬¸ì œë¥¼ ë‹¤ë£° ìˆ˜ ìˆë‹¤.
    - Partially observable ë¬¸ì œ ë˜í•œ MDPë¡œ convertí•  ìˆ˜ ìˆë‹¤.

2. Markov State

  - Stateê°€ Markoví•˜ë‹¤ëŠ” ê²ƒì€, ì–´ë–¤ ìƒíƒœê°€ í˜„ì¬ ìƒíƒœì—ë§Œ ì˜í–¥ì„ ë°›ì•„ ë‹¤ìŒ ìƒíƒœë¡œ ê°„ë‹¤ëŠ” ê²ƒ

3. State Transition Matrix

  - For a Markov state s and successor satte s', the state transition probability is defined by
      P_ss' = P [S_(t+1) = s' | S_t = s]

  - State transition matrix P defines transition probabilities from all states s to all successor states s'

4. A Markov Process
  A Markov process is a memoryless random process, i.e. a sequence of random states S1, S2, ... with the Markov property.

    > Definition
      A Markov Process (or Markov Chain) is a tuple <S, P>
      - S is a (finite) set of states
      - P is a state transition probability matrix,
        P_ss' = P[S_(t+1) = s' | S_t = s]

5. Markov Reward Process
  A Markov reward process is a Markov chain with values.

    > Definition
      A Markov Reward Process is a tuple <S, P, R, ğ›¾>
      - S is a finite set of states
      - P is a state transition probability matrix,
          P_ss' = P[S_(t+1) = s' | S_t = s]
      - R is a reward function, R_s = E[R_(t+1) | S_t = s]
      - ğ›¾ is a discount factor, ğ›¾ âˆˆ [0, 1]

6. Return

    > Definition
      The return G_t is the total discounted reward from time-step t.
        G_t = R_(t+1) + ğ›¾ * R_(t+2) + ... = {k = 0 to âˆ} Î£ ğ›¾^k * R_(t+k+1)

      - The discount ğ›¾ âˆˆ [0, 1] is present value of future rewards
      - The value of receiving reward R after k+1 time-steps is ğ›¾^k * R
      - This values immediate reward above delayed reward.
          ğ›¾ close to 0 leads to "myopic" evaluation
          ğ›¾ close to 1 leads to "far-sighted" evaluation

7. Discount factorê°€ í•„ìš”í•œ ì´ìœ 
  Most Markov reward and decision processes are discounted. Why?

  - Mathematically convenient to discount rewards
  - Avoids infinite returns in cyclic Markov processes
  - Uncertainty about the future may not be fully represented
  - If the reward is financial, immediate rewards may earn more interset than delayed rewards
  - Animal / human behaviour shows preference for immediate reward
  - It is sometimes possible to use undiscounted Markov reward processes (i.e. ğ›¾ = 1), e.g. if all sequences terminate.

8. asdf
  sdf










































asdf
