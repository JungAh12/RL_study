# RL-study-2019

Model-Free Prediction

### 1. Model-Free Reinforcement Learning

  Last lecture :
    - Planning by dynamic programming
    - Solve a known MDP
      => MDPë¥¼ ì•ˆë‹¤ëŠ” ê²ƒì€ transitionê³¼ rewardsë¥¼ ì•„ëŠ” ê²ƒ

  This lecture :
    - Model-free prediction
    - Estimate the value function of an unknown MDP
      => MDPë¥¼ ëª¨ë¥¼ë•Œ value fucntionì„ í•™ìŠµ
      => Policyì— ë”°ë¥¸ true valueë¥¼ ì¶”ì • í•˜ëŠ” ë¬¸ì œ
      => Bellman expectation eqë¥¼ ê°€ì§€ê³  í‘¸ëŠ” ê²ƒ

  Next lecture :
    - Model-free control
    - Optimise the value function of an unknown MDP
    => Bellman optimality eqë¥¼ ê°€ì§€ê³  í‘¸ëŠ” ê²ƒ

### 2. Monte-Carlo Reinforcement Learning

  => Monte-Carlo ê¸°ë²•ì€ ì‰½ê²Œ ë§í•´ ê²½í—˜ì„ í†µí•´ ì•Œê²Œ ë˜ëŠ” ë°©ë²•

  - MC methods learn directly from episodes of experience
    => ê²½í—˜ì„ í†µí•´ ì§ì ‘ ë°°ìš´ë‹¤
  - MC is model-free : no knowledge of MDP transition / rewards
  - MC learns from complete episodes : no bootstrapping
    => episodeê°€ ëë‚˜ì•¼ returnì´ ì •í•´ì§€ê³ , ê° stateì—ì„œì˜ valueë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.
  - MC uses the simplest possible idea : value = mean return
  - Caveat : can only apply MC to episodic MPDs
    - All episodes must terminate
      => ëì´ ìˆëŠ” MDPì—¬ì•¼ í•œë‹¤.

#### (1) Monte-Carlo Policy Evalutaion

  Goal : learn v_ğœ‹ from episodes of experience under policy ğœ‹
    S_1, A_1, R_2, ..., S_k ~ ğœ‹

  Recall that the return is the total discounted reward:
    G_t = R_(t+1) + ğ›¾ * R_(t+2) + ... + ğ›¾^(T-1) * R_T

  Recall that the value function is the expected return :
    v_ğœ‹(s) = E_ğœ‹[G_t | S_t = s]

  Monte-Carlos policy evalutation uses empirical mean return instead of expected return
  => expected return ëŒ€ì‹ ì— ê²½í—˜ì ì¸ meanì„ í†µí•´ policy evaluationì„ í•˜ëŠ” ê²ƒ

#### (2) First-Visit Monte-Carlo Policy Evaluation

  - To evaluate state s
  - The first time-setp t that state s is visited in an episode,
  - Increment counter N(s) <- N(s) + 1
  - Incrment total return S(s) <- S(s) + G_t
  - Value is estimated by mean return V(s) = S(s)/N(s)
  - By law of large numbers, V(s) -> v_ğœ‹(s) as N(s) -> âˆ
    => sì— ê°€ì¥ ì²˜ìŒ ë°©ë¬¸í–ˆì„ ë•Œ counterë¥¼ ì˜¬ë¦¬ê³  returnì„ ë”í•œë‹¤.
    => V(s)ëŠ” mean returnì¸ S(s)/N(s)ë¡œ ì¶”ì •í•œë‹¤. í° ìˆ˜ì˜ ë²•ì¹™ì— ë”°ë¼ ì‹¤ì œ ê°’ì— ìˆ˜ë ´í•œë‹¤.

#### (3) Every-Visit Monte-Carlo Policy Evaluation

  - To evaluate state s
  - Every time-step t that state s is visited in an episode,
  - Increment counter N(s) <- N(s) + 1
  - Increment total return S(s) <- S(s) + G_t
  - Value is estimated by mean return V(s) = S(s)/N(s)
  - Again, V(s) -> v_ğœ‹(s) as N(s) -> âˆ
    => ëª¨ë“  ë°©ë¬¸ì— ëŒ€í•˜ì—¬ N(s), S(s)ë¥¼ ì¶”ì •ì— í¬í•¨í•˜ëŠ” ê²ƒ

#### (4) Incremental Mean

  The mean ğœ‡_1, ğœ‡_2, ... of a sequence x_1, x_2, ... can be computed incrementally,

    ğœ‡_k = (1/k) * {j=1 => k} Î£ x_j
        = (1/k) * (x_k + {j=1 => k-1} Î£ x_j )
        = (1/k) * (x_k + (k-1) * ğœ‡_(k-1))
        = ğœ‡_(k-1) + (1/k) * (x_k - ğœ‡_(k-1))

#### (5) Incremental Monte-Calro Updates

  Update V(s) incrementally after episode S_1, A_1, R_2, ... , S_T
  For each state S_t with return G_t

    N(S_t) <- N(S_t) + 1
    V(S_t) <- V(S_t) + (1/N(S_t)) * (G_t - V(S_t))

  In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes.

    V(S_t) <- V(S_t) + ğ›¼ * (G_t - V(S_t))

### 3. Temporal - Difference Learning

  - TD methods learn directly from episodes of experience
  - TD is model-free : no knowledge of MDP transition / rewards
  - TD learns from incomplete episodes, by bootstrapping
  - TD updates a guess towards a guess

#### (1) MC and TD

  - Goal : learn v_ğœ‹ online from experience under policy ğœ‹
  - Incremental every-visit Monte-Carlo
    - Update value V(S_t) toward actual return G_t
      V(S_t) <- V(S_t) + ğ›¼ * (G_t - V(S_t))

  - Simplest temporal-difference learning algorithm: TD(0)
    - Update value V(S_t) toward estimated return R_(t+1) + ğ›¾ * V(S_(t+1))
      V(S_t) <- V(S_t) + ğ›¼ * (R_(t+1) + ğ›¾ * V(S_(t+1)) - V(S_t))

    - R_(t+1) + ğ›¾ * V(S_(t+1)) is called the TD target
    - ğ›¿_t = R_(t+1) + ğ›¾ * V(S_(t+1)) - V_(S_t) is called the TD error

#### (2) Advantages and Disadvantages of MC vs TD

  - TD can learn before knowing the final outcome
    - TD can learn online after every step
    - MC must wait until end of episode before return is known

  - TD can learn without the final outcome
    - TD can learn from incomplete sequences
    - MC can only learn from complete sequences
    - TD works in continuing (non-terminating) environments
    - MC only works for episodic (terminating) environments

#### (3) Bias / Variance Trad - Off

  - Return G_t = R_(t+1) + ğ›¾ * R_(t+2) + ... + ğ›¾^(T-1) * R_t is unbiased estimate of v_ğœ‹(S_t)
  - True TD Target R_(t+1) + ğ›¾ * v_ğœ‹(S_(t+1)) is unbiased estimate of v_ğœ‹(S_t)
  - TD target is much lower variance than the return :
    - Return depends on many random actions, transitions, rewards
    - TD target depends on one random action, transition, reward

#### (4) Advantages and Disadvantages of MC vs TD (2)

  - MC has high variance, zero bias
    - Good convergence properties
    - (even with function approximation)
    - Not very sensitive to initial value
    - Very simple to understand and use

  - TD has low variance, some bias
    - Usually more efficient than MC
    - TD(0) converges to v_ğœ‹(s)
    - (but not always with function approximation)
    - More sensitve to initial value

### 3. Batch Monte-Carlo and Temporal Difference

  - MC and TD converge : V(s) -> v_ğœ‹(s) as experience -> âˆ
  - But what about batch solution for finite experience?
      s^1_1, a^1_1, r^1_2, ... , S^1_T1
                  ...
      s^K_1, a^K_1, r^K_2, ... , S^K_Tk

    - e.g. Repeatedly sample episode k
    - Apply MC or TD(0) to episode k

#### (1) Certainty Equivalence















  d
