# RL-study-2019

9ì¥ì€ Explorationê³¼ Exploitationì— ì´ˆì ì„ ë§ì¶°ì„œ ê·¸ì™€ ê´€ë ¨ëœ ì–˜ê¸°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

### Exploration vs Explitation Deilemma
Online ì˜ì‚¬ ê²°ì •(Decision-making)ì„ í•¨ì— ìˆì–´ ê·¼ë³¸ì ì¸ ë¬¸ì œê°€ ìˆë‹¤.
  Exploitation : ì£¼ì–´ì§„ í˜„ì¬ ì •ë³´ë¥¼ í†µí•´ ìµœì„ ì˜ ê²°ì •ì„ í•˜ëŠ” ê²ƒ
  Exploration  : ë§ì€ ì •ë³´ë¥¼ ëª¨ìœ¼ëŠ” ê²ƒ

The best long-term strategy may involve short-term sacrifices
Gather enough information to make the best overall decisions

agentê°€ ì´ ë‘˜ ì‚¬ì´ì˜ ì¤„íƒ€ê¸°ë¥¼ ì—„ì²­ í•´ì•¼ í•˜ëŠ” ê²ƒ... êµ‰ì¥íˆ ì¤‘ìš”í•˜ë‹¤.

Principles
  Naive Exploration
    Add noise to greedy policy (e.g. e-greedy)
    => noiseë¼ê³  í•˜ëŠ” randomì„±ì„ ì¶”ê°€í•˜ëŠ” ê²ƒ ì¡°ê¸ˆ ìˆœì§„í•œ íƒí—˜

  Optimistic Initialisation
    Assume the best until proven otherwise

  Optimism in the Face of Uncertainty
    Prefer actions with uncertain values
    => ë¶ˆí™•ì‹¤ ì„±ì„ ê¸ì •ì ìœ¼ë¡œ ë³´ëŠ” ë°©ë²•ë¡  ë’¤ì—ì„œ ë‹¤ë£°ê²ƒ

  Probability Matching
    Select actions according to probability they are best

  Information State Search
    Lookahead search incorporating value of information
    ì •ë³´ì˜ ê°€ì¹˜ë¥¼ í¬í•¨í•œ

#### The Multi-Armed Bandit
ë©€í‹°ì•”ë“œ ë°´ë”§ ë¬¸ì œ : ì•„ì£¼ ë‹¨ìˆœí•œ ë¬¸ì œì´ë©°, tuple <A, R>ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì œ
=> One-setp MDPë¼ê³ ë„ ë¶ˆë¦°ë‹¤.

A is a known set of m actions (or "arms")
R^a(r) = P[r|a] is an unknown probability distribution over rewards
At each step t the agent selects an action at
The environment generates a reward rt ~ R^at
The goal is to maximise cumulative reward {tau=1 ->t} Sigma r_tau

Regret
  The action-value is the mean reward for action a,
    Q(a) = E[r|a] (ì—¬ê¸°ì„œëŠ” returnì´ ê³§ rewarì´ë‹¤. one-step MDPì—¬ì„œ)

  The optimal value V* is
    V* = max Q(a)

  The regret is the opportunity loss for one step
    l_t = E[V* - Q(a_t)] (ì´ë¡ ì ì¸ ê°œë…ì„. V*ëŠ” ì‚¬ì‹¤ ì•„ë¬´ë„ ëª¨ë¥´ëŠ”ê±°)

  The total regret is the total opportunity loss
    L_t = E[{tau=1 ->t} Sigma{V* - Q(a_tau)}]

  Maximise cumulative reward â‰¡ minimise total regret

Counting Regret
  The count Nt(a) is expected number of selections for action âˆ†a
  The gap a is the difference in value between action a and optimal action a*,
    âˆ†a = V* - Q(a)

  The regret is a function of gaps and the counts
    L_t = E[{tau=1 ->t} Sigma{V* - Q(a_tau)}]
        = {a} Sigma{ E[Nt(a)] * (V* - Q(a))}
        = {a} Sigma{ E[Nt(a)] * âˆ†a}

  A good algorithm ensures small counts for large gaps
  Problem: gaps are not known!

Linear or Sublinear Regret
  ë§Œì•½ alrgorithmì´ ì˜ì›íˆ íƒí—˜í•œë‹¤ë©´, total regretì´ linarí•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

  ë§Œì•½ algorithmì´ íƒí—˜ì„ ì ˆëŒ€ ì•ˆí•´ë„, total regretì´ linearí•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.

  ê·¸ë ‡ë‹¤ë©´, sublinearí•œ total regretì„ ì–»ëŠ” ê²ƒì€ ê°€ëŠ¥í•œê°€?? => ê·¸ëŸ´ ìˆ˜ ìˆë‹¤!

Greedy Algorithm
  We consider algorithms that estimate Q_hat_t(a) â‰ˆ Q(a)
  Estimate the value of each action by Monte-Carlo evaluation
  greedy algorithmì€ ê°€ì¥ ë†’ì€ value ê°’ì„ ì§€ë‹Œ actionì„ ì„ íƒ!
  greedy algirhtmì€ suboptimal actionì„ ì˜ì›íˆ í•œë‹¤... => total regretì´ linearì´ë‹¤!

e-Greedy Algorithm
  e-greedy algorithmì€ íƒí—˜ì„ ì˜ì›íˆ ì§€ì†í•œë‹¤.
  Constant e ensures minimum regert
    l_t >= e/A * Sigma{âˆ†a}
  e-greedy algorithmë˜í•œ linear total regretì„ ì§€ë‹Œë‹¤.

#### Optimistic Initialisation
  Sample and practical idea : Initialise Q(a) to high value!
  Update action value by incremental Monte-Carlo evaluation
  Starting with N(a) > 0
    Q_hat_t(a_t) = Q_hat_(t-1) + 1/Nt(at) * (r_t - Q_hat_(t-1))

  Encourages systematic exploration early on
  => ì´ˆê¸°ì— Qê°€ ë‹¤ ë†’ìœ¼ë‹ˆê¹Œ íƒí—˜ì„ systematicí•˜ê²Œ ì§„í–‰í•  ê²ƒì´ë‹¤.

  But can still lock onto suboptimal action
  => greedy + Optimistic Initialisation ë˜í•œ ì„ í˜•ì˜ total regretì„ ì§€ë‹˜
  => e-greedy + Optimistic Initialisation ë˜í•œ ì„ í˜•ì˜ total regretì„ ì§€ë‹˜

=> Q valueë¥¼ ì´ˆê¸°í™”ë¥¼ 0ì´ ì•„ë‹ˆë¼, êµ‰ì¥íˆ ë‚™ê´€ì ìœ¼ë¡œ í•´ë†“ëŠ” ê²ƒ!
=> ê·¸ ë‹¤ìŒë¶€í„° greedy í˜¹ì€ e-greedyë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒ.

#### Decaying et-Greedy Algorithm
  pick a decay schedule for e1, e2, ...
  Consider the following schedule
    c  > 0
    d  = min âˆ†i
    et = min{1, c|A|/d^2*t}

  => scheduling ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ì´ë‹¤.
     dëŠ” 1ë“± ë¨¸ì‹ ê³¼ 2ë“± ë¨¸ì‹ ì˜ regretì˜ ì°¨ì´ë‹¤. ë¯¸ì„¸í•˜ê²Œ ì°¨ì´ë‚˜ë©´ eê°€ ì»¤ì§€ê³  í¬ê²Œ ì°¨ì´ë‚˜ë©´ eê°€ ì¤„ì–´ë“¤ì–´ì„œ íƒí—˜ê³¼ ì°©ì·¨ë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ë‹¤.

  Decaying et-greedy has logarithmic asymptotic total regret!
  Unfortunately, schedule requires advance knowledge of gaps
  => ë¶ˆí–‰í•˜ê²Œë„, gapì„ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ”ë°, ì´ gapì€ ì‹¤ì œ ë¬¸ì œì—ì„  ì•Œê¸° í˜ë“  ì •ë³´ì´ë‹¤.

  Goal : find an algorithm with sublinear regret for any multi-armed bandit
  (without knowledge of R)

#### Lower Bound
  The performance of any algorithm is determined by similarity between optimal arm and other arms
  Hard problem have similar-looking arms with different means
  This is described formally by the gap âˆ†a and the similiarity in distriblution KL(Ra||Ra*)

  ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ë„, Lower boundëŠ” ë„˜ì–´ê°ˆ ìˆ˜ ì—†ë‹¤! ì•„ë¬´ë¦¬ ì•Œê³ ë¦¬ì¦˜ì´ ì¢‹ë‹¤ê³  í•˜ë”ë¼ë„!
  ë‹¤ë¥¸ armë“¤ë¼ë¦¬ì˜ ìœ ì‚¬ì„±ì— ë”°ë¼ ì•Œê³ ë¦¬ì¦˜ì˜ í•œê³„ê°€ ë‹¬ë¼ì§„ë‹¤.
  => ë‹¤ë¥¸ armë“¤ë¼ë¦¬ ìœ ì‚¬í•  ìˆ˜ë¡ lower boundê°€ ë†’ì•„ì§€ëŠ” ëŠë‚Œ

  Teorem (Lai and Robbins)
  Asymptotic total regret is at least logarithmic in number of steps
    ~~~~ Gapê³¼ KL divergenceë“±ìœ¼ë¡œ Lower boundê°€ ì •ì˜ê°€ ëœë‹¤.
    Gapì´ í´ìˆ˜ë¡, KL divergenceê°€ ì‘ì„ ìˆ˜ë¡, Lower boundê°€ ì»¤ì§€ëŠ” ê²ƒ.

#### Optimism in the Face of Uncertainty
  Which action should we pick?
  The more uncertian we are about an action-value
  The more important it is to explore that action
  It could turn out to be the best action

  => a1, a2, a3ì— ëŒ€í•œ Qì˜ ë¶„í¬ë¥¼ ê·¸ë ¤ ë³¸ ê·¸ë˜í”„ë¥¼ ì„¤ëª…í•´ë³´ë©´
  íŒŒë€ìƒ‰ì˜ ê²½ìš° 95%ì˜ ì‹ ë¢°êµ¬ê°„ì„ ìƒê°í•´ë³´ë©´ uncertaintyê°€ í¬ê³ , Qê°€ ë†’ì„ ìˆ˜ë„ ìˆë‹¤.
  ê·¸ë˜ì„œ íŒŒë€ìƒ‰ì´ best actionìœ¼ë¡œ ë°”ë€” ìˆ˜ ìˆëŠ” ê±°ë‹ˆê¹Œ íŒŒë€ìƒ‰ actionìœ¼ë¡œ exploreë¥¼ í•´ë³´ìëŠ” idea!

#### Upper confidence Bound
  ê° action valueì— ëŒ€í•´ì„œ U_t(a)ë¥¼ estimation í•´ë³´ì.
  ë˜, ê° action valueì— ëŒ€í•´ì„œ Q(a)ê°€ Q(a) <= Qt(a) + Ut(a)ì¼ í™•ë¥ ì´ êµ‰ì¥íˆ ë†’ë‹¤ê³  í•˜ì.
  This depends on the number of times N(a) has been selected
    Small Nt(a) => large Ut(a) (estimate valueê°€ uncertainí•˜ë‹¤ëŠ” ì˜ë¯¸)
    Large Nt(a) => small Ut(a) (estimate valueê°€ ë‹¤ì†Œ accurate ì˜ë¯¸)

  Slect action maximising Upper Confidence Bound (UCB)
    at = argamx Qt(a) + Ut(a)

#### Hoeffding's Inequality
  Theorem (Hoeffding's inequality)
    Let X1, ... , Xt be i.i.d. random variables in [0,1], and
    let X_t_bar be the sample mean. Then
      P[E[X] > X_t_bar] <= e^-2tu^2
    => Hoeffding ë¶€ë“±ì‹ì˜ ë‹¨ í•˜ë‚˜ì˜ ì¡°ê±´ Xiê°€ 0~1 ì‚¬ì´ì—¬ì•¼ í•œë‹¤ëŠ” ê²ƒ!!
    => ì´ê²ƒ ë°–ì— ì œí•œ ì¡°ê±´ì´ ì—†ê³ , Xê°€ ì–´ë–¤ ë¶„í¬ì¸ì§€ì— ëŒ€í•œ ì œí•œì€ ì—†ë‹¤.
    => ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì¡°ê¸ˆ ì•½í•œ ë¶€ë“±ì‹ì´ì–´ì„œ, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ UCBì˜ ê²½ìš°
    => ì˜ íŠœë‹í•œ e-greedyì™€ ì„±ëŠ¥ ì°¨ì´ê°€ í¬ê²Œ ì•ˆ ë‚œë‹¤ê³  í•œë‹¤.

  We will apply Hoeffding's Inequality to rewards of the Bandit
  Conditioned on selecting action a
    P[Q(a) > Qt(a) + Ut(a)] <= e^-2Nt(a)Ut(a)^2

  => E[x]ê°€ X_t_barë³´ë‹¤ í´ í™•ë¥ ì´ e^-2tu^2 ì´í•˜ë¼ëŠ” ê²ƒì´ Hoeffdingì´ ì¦ëª…í•œ ë¶€ë“±ì‹ì´ë‹¤.
  => ì´ê²ƒ ë§ê³ ë„ ë‹¤ë¥¸ ë§ì€ ì§€í‘œë“¤ì´ UCBë¡œì¨ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.

Calculating Upper Confidence Bounds
  Pick a probability p that true value exceeds UCB
  Now solve for Ut(a)
    e^-2Nt(a)Ut(a)^2 = p
    ì–‘ë³€ì— ln ì”Œìš°ê³  ì •ë¦¬í•˜ë©´ Utë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ!

  Reduce p as we observe more rewards, e.g. p=t^-4
  => pë¥¼ t^-4ë¼ê³  ì •í•´ì£¼ë©´ scheduling íš¨ê³¼ê°€ ìˆë‹¤!
  Ensures we select optimal action as t-> ë¬´

  => ì´ê±¸ ì‚¬ìš©í•œ ê²ƒì´ UCB1 algorithmì´ë‹¤!

ì˜ˆì œë¥¼ í†µí•´ ë¹„êµí•œ ê²°ê³¼ UCBì™€ e-greedyê°€ ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê¸´ í–ˆë‹¤!
ê·¼ë° e-greedyì˜ ê²½ìš° eê°’ì„ ì˜ëª» ì…‹íŒ…í•˜ë©´ ì¬ì•™ì´ ì¼ì–´ë‚  ìˆ˜ ìˆë‹¤.

#### Bayesian Bandits
So fare we have made no assumptions about the reward distribution R
  Except bounds on rewards
  => Bayesian Banditsì˜ ê²½ìš° rewardsì˜ prior knowledgeë¥¼ ì•Œì•„ì•¼ í•œë‹¤!

Bayesian bandits exploit prior knowledge of rewards, p[R]
They compute posterior distribution of rewards p[R|ht]
  where ht = a1, r1, ... , a(t-1), r(t-1) is the history
  => ê·¸ëŸ´ ê²½ìš°, historyë“¤ì„ í†µí•´ì„œ posterior distributionì„ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤!

Use posterior to guide exploration
  Upper confidence bounds (Bayesian UCB)
  Probability matching (Thompson sampling)
  => posterior distributionì„ ì–»ì–´ë‚´ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” UCB, probablity mathcingë“±ì´ ìˆë‹¤.

Better performance if prior knowledge is accurate
  => ì´ ë°©ë²•ì„ ì‚¬ìš©í•  ê²½ìš°, prior knowledgeê°€ ì •í™•í•˜ë©´ ì •í™•í•  ìˆ˜ë¡ ì„±ëŠ¥ì´ ë‚˜ì˜¨ë‹¤.

#### Probability Matching
Probability mathcingì€, aê°€ ì œì¼ ì¢‹ì€ í™•ë¥  ê·¸ëŒ€ë¡œ ë½‘ëŠ” ë°©ë²•ì´ë¼ê³  í•œë‹¤.

Probability matching selects action a according to probability that a is the optimal action
  ğœ‹(a|ht) = P[Q(a) > Q(a'), âˆ€a' â‰  a|ht]

Probablity matching is optimistic in the face of uncertainty
  Uncertain actions have higher probablity of being max

Can be difficult to compute analytically from posterior

#### Tompson Sampling
ê°„ë‹¨í•œ ë°©ë²•ì´ë¼ê³  í•œë‹¤!!

Tompson sampling implements probablity matching
  ğœ‹(a|ht) = P[Q(a) > Q(a'), âˆ€a' â‰  a|ht]
          = E_R|ht[I(a=argmax Q(a))]

Use bayes law to compute posterior distribution p[R|ht]
Sample a reward distribution R from posterior
Compute action-value function Q(a) = E[Ra]
Select action maximising value on sample, at = argmax Q(a)
Thompson sampling achieves Lai and Robbins lower bound!

  => priorë¥¼ ì•Œê³  ìˆìœ¼ë¯€ë¡œ posteriorë¥¼ í†µí•´ Rì„ samplingí•  ìˆ˜ ìˆê³  samplingì„ í†µí•´
  => action value function Që¥¼ ê³„ì‚°í•´ ë‚¼ ìˆ˜ ìˆë‹¤.
  => ê·¸ëŸ¬ë©´, sampleì„ í†µí•´ valueë¥¼ ìµœëŒ€í™” í•´ì£¼ëŠ” action atë¥¼ ì„ íƒí•œë‹¤!
  thompson samplingì€ Laiì™€ Robbinsì˜ lower boundë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤
  => 1ë²ˆì´ ì¢‹ë‹¤ê³ í•´ì„œ 1ë²ˆì„ ëŒ•ê¸°ë©´, ê·¸ ê²°ê³¼ë¥¼ í†µí•´ posterior ë¥¼ ë‹¤ì‹œ ê³„ì‚°í•  ìˆ˜ ìˆê³ 
  => í™•ë¥ ì— ë”°ë¥¸ ëœë¤ì„± ë•Œë¬¸ì— ë‹¤ë¥¸ ê²ƒë„ ë½‘ì„ ìˆ˜ ìˆë‹¤ê³  í•˜ëŠ”ë° ë„ˆë¬´ ì–´ë µì–ì•„ ì´ê±°..?

#### Value of information
  íƒí—˜ì€ ì •ë³´ë¥¼ ì–»ê¸°ì— ìœ ìš©í•˜ë‹¤.
  ìš°ë¦¬ëŠ” ì •ë³´ì˜ ê°€ì¹˜ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ì—†ì„ê¹Œ?
    How much reward a decision-maker would be prepared to pay in order to
    have that information, prior to making a decision

    Long-term reward after getting information - immediate reward

  Information gain is higher in uncertain situation
  => ë¶ˆí™•ì‹¤í•œ ìƒí™©ì— ëŒ€í•´ information gainì´ ë” ë†’ë‹¤. (í¬ë°• ìƒí™©)
  Therefore it makes sense to explore uncertain situations more
  => ì´ëŠ” íƒí—˜ì„ ì¢…ìš©ì‹œí‚´ì„ ìš°ë¦¬ëŠ” ì•Œ ìˆ˜ ìˆë‹¤.
  If we know value of information, we can trade-off exploration and exploitation optimally
  => ì •ë³´ì˜ ì •ëŸ‰í™”ë¥¼ í•  ìˆ˜ ìˆìœ¼ë©´, optimalí•œ decision makingì„ í•  ìˆ˜ ìˆë‹¤ëŠ” idea

Information State Space
  We have viewd bandits as one-step decision-making problems
  Can also view as sequential decision-making problems
  At each step there is an information state s_tild
    s_tild is a static of the history, s_tild_t = f(h_t)
    summarising all information accumulated so far
    => ì§€ê¸ˆê¹Œì§€ ìŒ“ì•„ ë†“ì•˜ë˜ historyë“¤ì„ ìš”ì•½í•´ì£¼ëŠ” ì •ë³´ê°€ s_tild ì´ë‹¤!

  Each action a causes a transition to a new information state s'_tild
  (by adding information), with probability P_tild(a,s_tild,s'_tild)

  This defines MDP M_tild in augmented information state space
    M_tild = <S_tild, A, P_tild, R, ğ›¾>

Example : Bernoulli Bandits
  Consider a Bernolli bandit, such that R^a = B(mu_a)
  e.g. Win or lose a game with probability mu_a
  Want to find which arm has the highest mu_a
  The information state is s_tild = <ğ›¼_a, Î²_a>
    ğ›¼_a counts the pulls of arm a where reward was 0
    Î²_a counts the pulls of arm a where reward was 1











ã…‡
