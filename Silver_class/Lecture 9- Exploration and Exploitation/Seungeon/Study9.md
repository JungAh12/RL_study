# RL-study-2019

9장은 Exploration과 Exploitation에 초점을 맞춰서 그와 관련된 얘기를 다룹니다.

### Exploration vs Explitation Deilemma
Online 의사 결정(Decision-making)을 함에 있어 근본적인 문제가 있다.
  Exploitation : 주어진 현재 정보를 통해 최선의 결정을 하는 것
  Exploration  : 많은 정보를 모으는 것

The best long-term strategy may involve short-term sacrifices
Gather enough information to make the best overall decisions

agent가 이 둘 사이의 줄타기를 엄청 해야 하는 것... 굉장히 중요하다.

Principles
  Naive Exploration
    Add noise to greedy policy (e.g. e-greedy)
    => noise라고 하는 random성을 추가하는 것 조금 순진한 탐험

  Optimistic Initialisation
    Assume the best until proven otherwise

  Optimism in the Face of Uncertainty
    Prefer actions with uncertain values
    => 불확실 성을 긍정적으로 보는 방법론 뒤에서 다룰것

  Probability Matching
    Select actions according to probability they are best

  Information State Search
    Lookahead search incorporating value of information
    정보의 가치를 포함한

#### The Multi-Armed Bandit
멀티암드 밴딧 문제 : 아주 단순한 문제이며, tuple <A, R>로 이루어진 문제
=> One-setp MDP라고도 불린다.

A is a known set of m actions (or "arms")
R^a(r) = P[r|a] is an unknown probability distribution over rewards
At each step t the agent selects an action at
The environment generates a reward rt ~ R^at
The goal is to maximise cumulative reward {tau=1 ->t} Sigma r_tau

Regret
  The action-value is the mean reward for action a,
    Q(a) = E[r|a] (여기서는 return이 곧 rewar이다. one-step MDP여서)

  The optimal value V* is
    V* = max Q(a)

  The regret is the opportunity loss for one step
    l_t = E[V* - Q(a_t)] (이론적인 개념임. V*는 사실 아무도 모르는거)

  The total regret is the total opportunity loss
    L_t = E[{tau=1 ->t} Sigma{V* - Q(a_tau)}]

  Maximise cumulative reward ≡ minimise total regret










ㅇ
