# RL-study-2019

9장은 Exploration과 Exploitation에 초점을 맞춰서 그와 관련된 얘기를 다룹니다.

### Exploration vs Explitation Deilemma
Online 의사 결정(Decision-making)을 함에 있어 근본적인 문제가 있다.
  Exploitation : 주어진 현재 정보를 통해 최선의 결정을 하는 것
  Exploration  : 많은 정보를 모으는 것

The best long-term strategy may involve short-term sacrifices
Gather enough information to make the best overall decisions

agent가 이 둘 사이의 줄타기를 엄청 해야 하는 것... 굉장히 중요하다.

Principles
  Naive Exploration
    Add noise to greedy policy (e.g. e-greedy)
    => noise라고 하는 random성을 추가하는 것 조금 순진한 탐험

  Optimistic Initialisation
    Assume the best until proven otherwise

  Optimism in the Face of Uncertainty
    Prefer actions with uncertain values
    => 불확실 성을 긍정적으로 보는 방법론 뒤에서 다룰것

  Probability Matching
    Select actions according to probability they are best

  Information State Search
    Lookahead search incorporating value of information
    정보의 가치를 포함한

#### The Multi-Armed Bandit
멀티암드 밴딧 문제 : 아주 단순한 문제이며, tuple <A, R>로 이루어진 문제
=> One-setp MDP라고도 불린다.

A is a known set of m actions (or "arms")
R^a(r) = P[r|a] is an unknown probability distribution over rewards
At each step t the agent selects an action at
The environment generates a reward rt ~ R^at
The goal is to maximise cumulative reward {tau=1 ->t} Sigma r_tau

Regret
  The action-value is the mean reward for action a,
    Q(a) = E[r|a] (여기서는 return이 곧 rewar이다. one-step MDP여서)

  The optimal value V* is
    V* = max Q(a)

  The regret is the opportunity loss for one step
    l_t = E[V* - Q(a_t)] (이론적인 개념임. V*는 사실 아무도 모르는거)

  The total regret is the total opportunity loss
    L_t = E[{tau=1 ->t} Sigma{V* - Q(a_tau)}]

  Maximise cumulative reward ≡ minimise total regret

Counting Regret
  The count Nt(a) is expected number of selections for action ∆a
  The gap a is the difference in value between action a and optimal action a*,
    ∆a = V* - Q(a)

  The regret is a function of gaps and the counts
    L_t = E[{tau=1 ->t} Sigma{V* - Q(a_tau)}]
        = {a} Sigma{ E[Nt(a)] * (V* - Q(a))}
        = {a} Sigma{ E[Nt(a)] * ∆a}

  A good algorithm ensures small counts for large gaps
  Problem: gaps are not known!

Linear or Sublinear Regret
  만약 alrgorithm이 영원히 탐험한다면, total regret이 linar한 것을 알 수 있을 것이다.

  만약 algorithm이 탐험을 절대 안해도, total regret이 linear한 것을 알 수 있을 것이다.

  그렇다면, sublinear한 total regret을 얻는 것은 가능한가?? => 그럴 수 있다!

Greedy Algorithm
  We consider algorithms that estimate Q_hat_t(a) ≈ Q(a)
  Estimate the value of each action by Monte-Carlo evaluation
  greedy algorithm은 가장 높은 value 값을 지닌 action을 선택!
  greedy algirhtm은 suboptimal action을 영원히 한다... => total regret이 linear이다!

e-Greedy Algorithm
  e-greedy algorithm은 탐험을 영원히 지속한다.
  Constant e ensures minimum regert
    l_t >= e/A * Sigma{∆a}
  e-greedy algorithm또한 linear total regret을 지닌다.

#### Optimistic Initialisation
  Sample and practical idea : Initialise Q(a) to high value!
  Update action value by incremental Monte-Carlo evaluation
  Starting with N(a) > 0
    Q_hat_t(a_t) = Q_hat_(t-1) + 1/Nt(at) * (r_t - Q_hat_(t-1))

  Encourages systematic exploration early on
  => 초기에 Q가 다 높으니까 탐험을 systematic하게 진행할 것이다.

  But can still lock onto suboptimal action
  => greedy + Optimistic Initialisation 또한 선형의 total regret을 지님
  => e-greedy + Optimistic Initialisation 또한 선형의 total regret을 지님

=> Q value를 초기화를 0이 아니라, 굉장히 낙관적으로 해놓는 것!
=> 그 다음부터 greedy 혹은 e-greedy를 진행하는 것.

#### Decaying et-Greedy Algorithm
  pick a decay schedule for e1, e2, ...
  Consider the following schedule
    c  > 0
    d  = min ∆i
    et = min{1, c|A|/d^2*t}

  => scheduling 방법은 여러가지이다.
     d는 1등 머신과 2등 머신의 regret의 차이다. 미세하게 차이나면 e가 커지고 크게 차이나면 e가 줄어들어서 탐험과 착취를 조절하는 것이다.

  Decaying et-greedy has logarithmic asymptotic total regret!
  Unfortunately, schedule requires advance knowledge of gaps
  => 불행하게도, gap을 우리가 사용하는데, 이 gap은 실제 문제에선 알기 힘든 정보이다.

  Goal : find an algorithm with sublinear regret for any multi-armed bandit
  (without knowledge of R)

#### Lower Bound
  The performance of any algorithm is determined by similarity between optimal arm and other arms
  Hard problem have similar-looking arms with different means
  This is described formally by the gap ∆a and the similiarity in distriblution KL(Ra||Ra*)

  어떤 알고리즘도, Lower bound는 넘어갈 수 없다! 아무리 알고리즘이 좋다고 하더라도!
  다른 arm들끼리의 유사성에 따라 알고리즘의 한계가 달라진다.
  => 다른 arm들끼리 유사할 수록 lower bound가 높아지는 느낌

  Teorem (Lai and Robbins)
  Asymptotic total regret is at least logarithmic in number of steps
    ~~~~ Gap과 KL divergence등으로 Lower bound가 정의가 된다.
    Gap이 클수록, KL divergence가 작을 수록, Lower bound가 커지는 것.

#### Optimism in the Face of Uncertainty
  Which action should we pick?
  The more uncertian we are about an action-value
  The more important it is to explore that action
  It could turn out to be the best action

  => a1, a2, a3에 대한 Q의 분포를 그려 본 그래프를 설명해보면
  파란색의 경우 95%의 신뢰구간을 생각해보면 uncertainty가 크고, Q가 높을 수도 있다.
  그래서 파란색이 best action으로 바뀔 수 있는 거니까 파란색 action으로 explore를 해보자는 idea!

#### Upper confidence Bound
  각 action value에 대해서 U_t(a)를 estimation 해보자.
  또, 각 action value에 대해서 Q(a)가 Q(a) <= Qt(a) + Ut(a)일 확률이 굉장히 높다고 하자.
  This depends on the number of times N(a) has been selected
    Small Nt(a) => large Ut(a) (estimate value가 uncertain하다는 의미)
    Large Nt(a) => small Ut(a) (estimate value가 다소 accurate 의미)

  Slect action maximising Upper Confidence Bound (UCB)
    at = argamx Qt(a) + Ut(a)

#### Hoeffding's Inequality
  Theorem (Hoeffding's inequality)
    Let X1, ... , Xt be i.i.d. random variables in [0,1], and
    let X_t_bar be the sample mean. Then
      P[E[X] > X_t_bar] <= e^-2tu^2
    => Hoeffding 부등식의 단 하나의 조건 Xi가 0~1 사이여야 한다는 것!!
    => 이것 밖에 제한 조건이 없고, X가 어떤 분포인지에 대한 제한은 없다.
    => 그렇기 때문에 조금 약한 부등식이어서, 이를 기반으로 한 UCB의 경우
    => 잘 튜닝한 e-greedy와 성능 차이가 크게 안 난다고 한다.

  We will apply Hoeffding's Inequality to rewards of the Bandit
  Conditioned on selecting action a
    P[Q(a) > Qt(a) + Ut(a)] <= e^-2Nt(a)Ut(a)^2

  => E[x]가 X_t_bar보다 클 확률이 e^-2tu^2 이하라는 것이 Hoeffding이 증명한 부등식이다.
  => 이것 말고도 다른 많은 지표들이 UCB로써 사용될 수 있다.

Calculating Upper Confidence Bounds
  Pick a probability p that true value exceeds UCB
  Now solve for Ut(a)
    e^-2Nt(a)Ut(a)^2 = p
    양변에 ln 씌우고 정리하면 Ut를 구할 수 있음!

  Reduce p as we observe more rewards, e.g. p=t^-4
  => p를 t^-4라고 정해주면 scheduling 효과가 있다!
  Ensures we select optimal action as t-> 무

  => 이걸 사용한 것이 UCB1 algorithm이다!

예제를 통해 비교한 결과 UCB와 e-greedy가 성능이 비슷하긴 했다!
근데 e-greedy의 경우 e값을 잘못 셋팅하면 재앙이 일어날 수 있다.

#### Bayesian Bandits
So fare we have made no assumptions about the reward distribution R
  Except bounds on rewards

Bayesian bandits exploit prior knowledge of rewards, p[R]
They compute posterior distribution of rewards p[R|ht]
  where ht = a1, r1, ... , a(t-1), r(t-1) is the history

Use posterior to guide exploration
  Upper confidence bounds (Bayesian UCB)
  Probability matching (Thompson sampling)

Better performance if prior knowledge is accurate








ㅇ
