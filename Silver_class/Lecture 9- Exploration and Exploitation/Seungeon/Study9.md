# RL-study-2019

9장은 Exploration과 Exploitation에 초점을 맞춰서 그와 관련된 얘기를 다룹니다.

### Exploration vs Explitation Deilemma
Online 의사 결정(Decision-making)을 함에 있어 근본적인 문제가 있다.
  Exploitation : 주어진 현재 정보를 통해 최선의 결정을 하는 것
  Exploration  : 많은 정보를 모으는 것

The best long-term strategy may involve short-term sacrifices
Gather enough information to make the best overall decisions

agent가 이 둘 사이의 줄타기를 엄청 해야 하는 것... 굉장히 중요하다.

Principles
  Naive Exploration
    Add noise to greedy policy (e.g. e-greedy)
    => noise라고 하는 random성을 추가하는 것 조금 순진한 탐험

  Optimistic Initialisation
    Assume the best until proven otherwise

  Optimism in the Face of Uncertainty
    Prefer actions with uncertain values
    => 불확실 성을 긍정적으로 보는 방법론 뒤에서 다룰것

  Probability Matching
    Select actions according to probability they are best

  Information State Search
    Lookahead search incorporating value of information
    정보의 가치를 포함한

#### The Multi-Armed Bandit
멀티암드 밴딧 문제 : 아주 단순한 문제이며, tuple <A, R>로 이루어진 문제
=> One-setp MDP라고도 불린다.

A is a known set of m actions (or "arms")
R^a(r) = P[r|a] is an unknown probability distribution over rewards
At each step t the agent selects an action at
The environment generates a reward rt ~ R^at
The goal is to maximise cumulative reward {tau=1 ->t} Sigma r_tau

Regret
  The action-value is the mean reward for action a,
    Q(a) = E[r|a] (여기서는 return이 곧 rewar이다. one-step MDP여서)

  The optimal value V* is
    V* = max Q(a)

  The regret is the opportunity loss for one step
    l_t = E[V* - Q(a_t)] (이론적인 개념임. V*는 사실 아무도 모르는거)

  The total regret is the total opportunity loss
    L_t = E[{tau=1 ->t} Sigma{V* - Q(a_tau)}]

  Maximise cumulative reward ≡ minimise total regret

Counting Regret
  The count Nt(a) is expected number of selections for action ∆a
  The gap a is the difference in value between action a and optimal action a*,
    ∆a = V* - Q(a)

  The regret is a function of gaps and the counts
    L_t = E[{tau=1 ->t} Sigma{V* - Q(a_tau)}]
        = {a} Sigma{ E[Nt(a)] * (V* - Q(a))}
        = {a} Sigma{ E[Nt(a)] * ∆a}

  A good algorithm ensures small counts for large gaps
  Problem: gaps are not known!

Linear or Sublinear Regret
  만약 alrgorithm이 영원히 탐험한다면, total regret이 linar한 것을 알 수 있을 것이다.

  만약 algorithm이 탐험을 절대 안해도, total regret이 linear한 것을 알 수 있을 것이다.

  그렇다면, sublinear한 total regret을 얻는 것은 가능한가?? => 그럴 수 있다!

Greedy Algorithm
  We consider algorithms that estimate Q_hat_t(a) ≈ Q(a)
  Estimate the value of each action by Monte-Carlo evaluation
  greedy algorithm은 가장 높은 value 값을 지닌 action을 선택!
  greedy algirhtm은 suboptimal action을 영원히 한다... => total regret이 linear이다!

e-Greedy Algorithm
  e-greedy algorithm은 탐험을 영원히 지속한다.
  Constant e ensures minimum regert
    l_t >= e/A * Sigma{∆a}
  e-greedy algorithm또한 linear total regret을 지닌다.

#### Optimistic Initialisation
  Sample and practical idea : Initialise Q(a) to high value!
  Update action value by incremental Monte-Carlo evaluation
  Starting with N(a) > 0
    Q_hat_t(a_t) = Q_hat_(t-1) + 1/Nt(at) * (r_t - Q_hat_(t-1))

  Encourages systematic exploration early on
  => 초기에 Q가 다 높으니까 탐험을 systematic하게 진행할 것이다.

  But can still lock onto suboptimal action
  => greedy + Optimistic Initialisation 또한 선형의 total regret을 지님
  => e-greedy + Optimistic Initialisation 또한 선형의 total regret을 지님

=> Q value를 초기화를 0이 아니라, 굉장히 낙관적으로 해놓는 것!
=> 그 다음부터 greedy 혹은 e-greedy를 진행하는 것.

#### Decaying et-Greedy Algorithm
  pick a decay schedule for e1, e2, ...
  Consider the following schedule
    c  > 0
    d  = min ∆i
    et = min{1, c|A|/d^2*t}

  => scheduling 방법은 여러가지이다.
     d는 1등 머신과 2등 머신의 regret의 차이다. 미세하게 차이나면 e가 커지고 크게 차이나면 e가 줄어들어서 탐험과 착취를 조절하는 것이다.

  Decaying et-greedy has logarithmic asymptotic total regret!
  Unfortunately, schedule requires advance knowledge of gaps
  => 불행하게도, gap을 우리가 사용하는데, 이 gap은 실제 문제에선 알기 힘든 정보이다.

  Goal : find an algorithm with sublinear regret for any multi-armed bandit
  (without knowledge of R)

#### Lower Bound
  The performance of any algorithm is determined by similarity between optimal arm and other arms
  Hard problem have similar-looking arms with different means
  This is described formally by the gap ∆a and the similiarity in distriblution KL(Ra||Ra*)

  어떤 알고리즘도, Lower bound는 넘어갈 수 없다! 아무리 알고리즘이 좋다고 하더라도!
  다른 arm들끼리의 유사성에 따라 알고리즘의 한계가 달라진다.
  => 다른 arm들끼리 유사할 수록 lower bound가 높아지는 느낌

  Teorem (Lai and Robbins)
  Asymptotic total regret is at least logarithmic in number of steps
    ~~~~ Gap과 KL divergence등으로 Lower bound가 정의가 된다.


















ㅇ
